{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "    \n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def countDiffMasks(mask1,mask2):\n",
    "    total_diff=0\n",
    "    for i in range(len(mask1)):\n",
    "        m_1=mask1[i].flatten()\n",
    "        m_2=mask2[i].flatten()\n",
    "        count_same=(m_1 == m_2).sum()\n",
    "        count_different=m_1.flatten().shape[0]-count_same\n",
    "        total_diff+=count_different\n",
    "    return total_diff\n",
    "\n",
    "\n",
    "def get_mask_compression(mask_whole_model):\n",
    "    num_total=0\n",
    "    num_non_zeros=0\n",
    "    for mask_each_layer in mask_whole_model:\n",
    "        num_total+=torch.numel(mask_each_layer)\n",
    "        num_non_zeros+=torch.count_nonzero(mask_each_layer)\n",
    "        \n",
    "    return (num_total-num_non_zeros)/num_total\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def prune_model_get_mask(model,prune_rate):\n",
    "    '''\n",
    "    works purely on the model to get\n",
    "    mask\n",
    "    '''\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.std(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "            threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    " \n",
    "    \n",
    "def get_thresholds_each_layer(model,prune_rate):\n",
    "    thresholds_per_layer=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "            abs_std=torch.std(torch.abs(params.data))\n",
    "            threshold=abs_std*prune_rate\n",
    "            thresholds_per_layer.append(threshold)\n",
    "    return thresholds_per_layer\n",
    "    \n",
    "                \n",
    "def apply_mask_model(model,list_mask_whole_model):\n",
    "    mask_layer_count=0\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=list_mask_whole_model[mask_layer_count]\n",
    "            with torch.no_grad():\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#                 print(\"Devices are \",params.device,mask_layer.device)\n",
    "                mask_layer=mask_layer.to(device)\n",
    "    \n",
    "                params.data=params.data*mask_layer            \n",
    "            mask_layer_count+=1\n",
    "    \n",
    "\n",
    "def store_weights_in_dic(weight_description,model):\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            if nm not in weight_description:\n",
    "                weight_description[nm]={}\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                if index_component not in weight_description[nm]:\n",
    "                    weight_description[nm][index_component]={}\n",
    "                values=params[index_component]\n",
    "                flat_values=values.flatten()\n",
    "                for index_wt in range(flat_values.shape[0]):\n",
    "                    if index_wt not in weight_description[nm][index_component]:\n",
    "                        weight_description[nm][index_component][index_wt]=[]\n",
    "                    weight_description[nm][index_component][index_wt].append(flat_values[index_wt].detach().item())\n",
    "    return weight_description\n",
    "\n",
    "\n",
    "def get_boolean_dict_weight_dict(weight_description,prune_rate,thresholds_per_layer,last_few):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of 1s and 0s to show\n",
    "    how many times weight is more than threshold\n",
    "    per layer\n",
    "    '''\n",
    "    boolean_weight_description={}\n",
    "    count=0\n",
    "    for layer in weight_description.keys():  \n",
    "#         print(\"Count = \",count)\n",
    "        threshold_this_layer=thresholds_per_layer[count]\n",
    "#         print(\"Threshold for layer \",count,layer,\"is \",threshold_this_layer)\n",
    "        if layer not in boolean_weight_description:\n",
    "            boolean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in boolean_weight_description[layer]:\n",
    "                boolean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in boolean_weight_description[layer][index_component]:\n",
    "                    boolean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt][:-last_few]\n",
    "                all_wts_boolean=[]\n",
    "                for wt in all_wts:\n",
    "                    if abs(wt)>threshold_this_layer:\n",
    "                        all_wts_boolean.append(1)\n",
    "                    else:\n",
    "                        all_wts_boolean.append(0)\n",
    "                boolean_weight_description[layer][index_component][index_wt]=all_wts_boolean                    \n",
    "        count+=1\n",
    "        \n",
    "    return boolean_weight_description\n",
    "\n",
    "\n",
    "\n",
    "def get_mean_dict_weight_dict(weight_description,last_few):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of mean of weights simply\n",
    "    for the last few epochs\n",
    "    '''\n",
    "    mean_weight_description={}\n",
    "\n",
    "    for layer in weight_description.keys():  \n",
    "        if layer not in mean_weight_description:\n",
    "            mean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in mean_weight_description[layer]:\n",
    "                mean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in mean_weight_description[layer][index_component]:\n",
    "                    mean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt][-last_few:]                \n",
    "                all_wts_mean=np.mean(all_wts)                \n",
    "                mean_weight_description[layer][index_component][index_wt]=all_wts_mean                    \n",
    "        \n",
    "    return mean_weight_description\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# create mask from boolean weight dictionary\n",
    "def create_mask_from_boolean_wt(model,boolean_wt_dict):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.var(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "#             threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten() \n",
    "                mask_vals=[]\n",
    "                for val_index in range(re_shaped_values.shape[0]):\n",
    "                    boolean_vals=boolean_wt_dict[nm][index_component][val_index]\n",
    "                    m = stats.mode(boolean_vals)\n",
    "#                     print(\"Verdict for this weight is \",m[0][0])\n",
    "                    mask_vals.append(m[0][0])\n",
    "#                 mask_vals = (torch.abs(re_shaped_values)>threshold).float()                \n",
    "                mask_vals=np.asarray(mask_vals)\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=torch.from_numpy(mask_vals)\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "\n",
    "\n",
    "         \n",
    "def create_mask_from_mean_wt(model,mean_weight_description,prune_rate):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)    \n",
    "            # get all mean weights for this layer\n",
    "            mean_wt_layer=mean_weight_description[nm]\n",
    "            wts_this_layer=[]\n",
    "            for neuron_index in list(mean_wt_layer.keys()):\n",
    "                all_wts_this_neu=mean_wt_layer[neuron_index]\n",
    "\n",
    "                for weight_index in list(mean_wt_layer[neuron_index].keys()):\n",
    "                    wts_this_layer.append(mean_wt_layer[neuron_index][weight_index])\n",
    "            # end of all neurons\n",
    "#             print(\"first 5 Mean weights this layer are \",wts_this_layer[:5])\n",
    "            abs_var=torch.std(torch.FloatTensor(wts_this_layer))\n",
    "#             print(\"standard dev is \",abs_var)\n",
    "            threshold=abs_var*prune_rate\n",
    "#             print(\"Threshold is \",threshold)\n",
    "            num_components=params.shape[0]            \n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "            \n",
    "                \n",
    "                \n",
    "import math\n",
    "\n",
    "def get_weighted_mean_dict_weight_dict(weight_description,last_few):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of mean of weights     \n",
    "    for the last few epochs\n",
    "    and gives a weightage to each weight value\n",
    "    depending on the epoch\n",
    "    last epoch highest weight\n",
    "    '''\n",
    "    weighted_mean_weight_description={}\n",
    "\n",
    "    for layer in weight_description.keys():  \n",
    "        if layer not in weighted_mean_weight_description:\n",
    "            weighted_mean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in weighted_mean_weight_description[layer]:\n",
    "                weighted_mean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in weighted_mean_weight_description[layer][index_component]:\n",
    "                    weighted_mean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt][-last_few:]\n",
    "                i_weights=[math.sqrt(i) for i in range(1,last_few+1)]\n",
    "#                 print(all_wts,i_weights)\n",
    "                all_wts_mean=np.average(all_wts,weights=i_weights)                \n",
    "                weighted_mean_weight_description[layer][index_component][index_wt]=all_wts_mean                    \n",
    "        \n",
    "    return weighted_mean_weight_description    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part copied from shrinkbench\n",
    "\n",
    "def nonzero(tensor):\n",
    "    \"\"\"Returns absolute number of values different from 0\n",
    "\n",
    "    Arguments:\n",
    "        tensor {numpy.ndarray} -- Array to compute over\n",
    "\n",
    "    Returns:\n",
    "        int -- Number of nonzero elements\n",
    "    \"\"\"\n",
    "    return np.sum(tensor != 0.0)\n",
    "\n",
    "\n",
    "def model_size(model, as_bits=False):\n",
    "    \"\"\"Returns absolute and nonzero model size\n",
    "\n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Network to compute model size over\n",
    "\n",
    "    Keyword Arguments:\n",
    "        as_bits {bool} -- Whether to account for the size of dtype\n",
    "\n",
    "    Returns:\n",
    "        int -- Total number of weight & bias params\n",
    "        int -- Out total_params exactly how many are nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for tensor in model.parameters():\n",
    "        t = np.prod(tensor.shape)\n",
    "        nz = nonzero(tensor.detach().cpu().numpy())\n",
    "        if as_bits:\n",
    "            bits = dtype2bits[tensor.dtype]\n",
    "            t *= bits\n",
    "            nz *= bits\n",
    "        total_params += t\n",
    "        nonzero_params += nz\n",
    "    return int(total_params), int(nonzero_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))        \n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes how many correct outputs with respect to targets\n",
    "\n",
    "    Does NOT compute accuracy but just a raw amount of correct\n",
    "    outputs given target labels. This is done for each value in\n",
    "    topk. A value is considered correct if target is in the topk\n",
    "    highest values of output.\n",
    "    The values returned are upperbounded by the given batch size\n",
    "\n",
    "    [description]\n",
    "\n",
    "    Arguments:\n",
    "        output {torch.Tensor} -- Output prediction of the model\n",
    "        target {torch.Tensor} -- Target labels from data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "    Returns:\n",
    "        List(int) -- Number of correct values for each topk\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        # Only need to do topk for highest k, reuse for the rest\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(torch.tensor(correct_k.item()))\n",
    "        return res\n",
    "\n",
    "\n",
    "# below copied from shrinkbench, to be used later\n",
    "# def accuracy(model, dataloader, topk=(1,)):\n",
    "#     \"\"\"Compute accuracy of a model over a dataloader for various topk\n",
    "\n",
    "#     Arguments:\n",
    "#         model {torch.nn.Module} -- Network to evaluate\n",
    "#         dataloader {torch.utils.data.DataLoader} -- Data to iterate over\n",
    "\n",
    "#     Keyword Arguments:\n",
    "#         topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "#     Returns:\n",
    "#         List(float) -- List of accuracies for each topk\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Use same device as model\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     accs = np.zeros(len(topk))\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for i, (input, target) in enumerate(dataloader):\n",
    "#             input = input.to(device)\n",
    "#             target = target.to(device)\n",
    "#             output = model(input)\n",
    "\n",
    "#             accs += np.array(correct(output, target, topk))\n",
    "\n",
    "#     # Normalize over data length\n",
    "#     accs /= len(dataloader.dataset)\n",
    "\n",
    "#     return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LENET 300-100 for MNIST and comparison\n",
    "class FashionMnistNet(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)        \n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.fc4.is_classifier = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))        \n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        top_1, top_5 = correct(out, labels,topk=(1,5))\n",
    "#         print(\"Batch is \",batch[1].shape)\n",
    "        \n",
    "        top_1=top_1/batch[1].shape[0]\n",
    "        top_5=top_5/batch[1].shape[0]\n",
    "\n",
    "#         print(\"corr\",top_1,top_5)\n",
    "#         return {'val_loss': loss, 'val_acc': acc}\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'top_1': top_1, 'top_5': top_5}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        \n",
    "        batch_top_1s = [x['top_1'] for x in outputs]\n",
    "#         print(batch_top_1s)\n",
    "        epoch_top_1 = torch.stack(batch_top_1s).mean()      # Combine top_1\n",
    "        \n",
    "        batch_top_5s = [x['top_5'] for x in outputs]\n",
    "        epoch_top_5 = torch.stack(batch_top_5s).mean()      # Combine top_5\n",
    "        \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),\n",
    "               'val_top_1': epoch_top_1.item(), 'val_top_5': epoch_top_5.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_top_1: {:.4f}, val_top_5: {:.4f}\".format(\n",
    "                                epoch, result['val_loss'], result['val_acc'], \n",
    "                                result['val_top_1'], result['val_top_5']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     print(\"outputs are \",outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD,\n",
    "        weight_description=None,mask_whole_model=None):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    print(\"At train\")\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if mask_whole_model:\n",
    "#                 print(\"Applying mask\")\n",
    "                apply_mask_model(model,mask_whole_model)\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "#         print(\"wt desc = \",weight_description)\n",
    "        if weight_description!=None:\n",
    "            print(\"going for weight\")\n",
    "            weight_description=store_weights_in_dic(weight_description,model)\n",
    "    return history, weight_description\n",
    "\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "\n",
    "# val_size = 10000\n",
    "# train_size = len(dataset) - val_size\n",
    "\n",
    "# train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "# print(len(train_ds), len(val_ds))\n",
    "\n",
    "\n",
    "# batch_size=128\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# shape=dataset[0][0].shape\n",
    "# input_size=1\n",
    "# for s in shape:\n",
    "#     input_size*=s\n",
    "# print(input_size)\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print('images.shape:', images.shape)\n",
    "#     inputs = images.reshape(-1, input_size)\n",
    "#     print('inputs.shape:', inputs.shape)\n",
    "#     break\n",
    "    \n",
    "# input_size = inputs.shape[-1]\n",
    "# print(input_size)\n",
    "# hidden_size = 32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program with weighht pruning, dynamic, many ranges\n",
      "Torch cuda  False\n",
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Program with weighht pruning, dynamic, many ranges\")\n",
    "\n",
    "print(\"Torch cuda \",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(\"device \",device)\n",
    "\n",
    "\n",
    "\n",
    "dataset = FashionMNIST(root='data/', download=True, transform=ToTensor())\n",
    "\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = FashionMNIST(root='data/', train=False,transform=ToTensor())\n",
    "\n",
    "val_size = 10000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "shape=dataset[0][0].shape\n",
    "input_size=1\n",
    "for s in shape:\n",
    "    input_size*=s\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/gfxhome/asislam25/.conda/envs/dlproject/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "targets=train_ds.dataset.targets\n",
    "training_data=torch.tensor(train_ds.dataset.data)\n",
    "\n",
    "training_data = training_data.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial result [{'val_loss': 2.305004119873047, 'val_acc': 0.10927734524011612, 'val_top_1': 0.10927734524011612, 'val_top_5': 0.5257812738418579}]\n",
      "At train\n",
      "Epoch [0], val_loss: 2.0115, val_acc: 0.3284, val_top_1: 0.3284, val_top_5: 0.9773\n",
      "wt desc =  {}\n",
      "going for weight\n",
      "Epoch [1], val_loss: 1.1694, val_acc: 0.5684, val_top_1: 0.5684, val_top_5: 0.9891\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], val_loss: 0.9074, val_acc: 0.6356, val_top_1: 0.6356, val_top_5: 0.9920\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], val_loss: 0.7799, val_acc: 0.6829, val_top_1: 0.6829, val_top_5: 0.9931\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], val_loss: 0.7517, val_acc: 0.7116, val_top_1: 0.7116, val_top_5: 0.9940\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], val_loss: 0.6265, val_acc: 0.7686, val_top_1: 0.7686, val_top_5: 0.9940\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], val_loss: 0.5685, val_acc: 0.8002, val_top_1: 0.8002, val_top_5: 0.9953\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], val_loss: 0.5481, val_acc: 0.8048, val_top_1: 0.8048, val_top_5: 0.9951\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], val_loss: 0.5348, val_acc: 0.8126, val_top_1: 0.8126, val_top_5: 0.9950\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], val_loss: 0.5410, val_acc: 0.8122, val_top_1: 0.8122, val_top_5: 0.9948\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], val_loss: 0.5103, val_acc: 0.8215, val_top_1: 0.8215, val_top_5: 0.9955\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], val_loss: 0.4978, val_acc: 0.8244, val_top_1: 0.8244, val_top_5: 0.9954\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], val_loss: 0.4939, val_acc: 0.8248, val_top_1: 0.8248, val_top_5: 0.9953\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], val_loss: 0.4838, val_acc: 0.8321, val_top_1: 0.8321, val_top_5: 0.9955\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], val_loss: 0.4886, val_acc: 0.8251, val_top_1: 0.8251, val_top_5: 0.9955\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], val_loss: 0.4743, val_acc: 0.8318, val_top_1: 0.8318, val_top_5: 0.9955\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22], val_loss: 0.4470, val_acc: 0.8416, val_top_1: 0.8416, val_top_5: 0.9956\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46], val_loss: 0.3768, val_acc: 0.8689, val_top_1: 0.8689, val_top_5: 0.9964\n",
      "wt desc =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=FashionMnistNet()\n",
    "history = [evaluate(model, val_loader)]\n",
    "print(\"initial result\",history)\n",
    "weight_description={}\n",
    "epochs=50\n",
    "lr=0.01\n",
    "history2,weight_description = fit(epochs, lr, model, train_loader, val_loader,weight_description=weight_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case doing it for the first time\n",
    "\n",
    "model_state_path=\"model_state/mod.pt\"\n",
    "torch.save(model.state_dict(), model_state_path)\n",
    "\n",
    "pickle.dump( weight_description, open( \"pickles/weight_description.p\", \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fc1.weight', 'fc2.weight', 'fc3.weight', 'fc4.weight'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression= 0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # list_threshold=get_thresholds_each_layer(model,prune_rate)\n",
    "# # print(list_threshold)\n",
    "# last_few=50\n",
    "# # mean_weight_description=get_mean_dict_weight_dict(weight_description,last_few)\n",
    "# mean_weight_description_weighted=get_weighted_mean_dict_weight_dict(weight_description,last_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask compression =  tensor(0.9608)\n"
     ]
    }
   ],
   "source": [
    "# prune_rate=1.98 # compression of 0.9627\n",
    "# mask_whole_model=create_mask_from_mean_wt(model,mean_weight_description,prune_rate)\n",
    "# # boolean_weight_description=get_boolean_dict_weight_dict(weight_description,\n",
    "# #                                                         prune_rate,list_threshold,last_few)\n",
    "\n",
    "# # mask_whole_model=create_mask_from_boolean_wt(model,boolean_weight_description)\n",
    "# # print(prune_rate,get_mask_compression(mask_whole_model))\n",
    "# print(\"Mask compression = \",get_mask_compression(mask_whole_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result is  {'val_loss': 0.3928965628147125, 'val_acc': 0.860644519329071, 'val_top_1': 0.860644519329071, 'val_top_5': 0.996777355670929}\n",
      "Compression= 0.0\n",
      "Mask compression =  0.1 tensor(0.0602)\n",
      "After masking, Compression= 0.060075300088152185 Result after pruning is  {'val_loss': 0.3921419084072113, 'val_acc': 0.861132800579071, 'val_top_1': 0.861132800579071, 'val_top_5': 0.996777355670929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3819, val_acc: 0.8654, val_top_1: 0.8654, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3688, val_acc: 0.8732, val_top_1: 0.8732, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.4138, val_acc: 0.8505, val_top_1: 0.8505, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3691, val_acc: 0.8720, val_top_1: 0.8720, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3763, val_acc: 0.8715, val_top_1: 0.8715, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3902, val_acc: 0.8655, val_top_1: 0.8655, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3658, val_acc: 0.8729, val_top_1: 0.8729, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3643, val_acc: 0.8726, val_top_1: 0.8726, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3628, val_acc: 0.8729, val_top_1: 0.8729, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3670, val_acc: 0.8726, val_top_1: 0.8726, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4025, val_acc: 0.8551, val_top_1: 0.8551, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3538, val_acc: 0.8773, val_top_1: 0.8773, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3636, val_acc: 0.8750, val_top_1: 0.8750, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3609, val_acc: 0.8738, val_top_1: 0.8738, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3687, val_acc: 0.8698, val_top_1: 0.8698, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3976, val_acc: 0.8622, val_top_1: 0.8622, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3583, val_acc: 0.8784, val_top_1: 0.8784, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3608, val_acc: 0.8761, val_top_1: 0.8761, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3510, val_acc: 0.8781, val_top_1: 0.8781, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3613, val_acc: 0.8710, val_top_1: 0.8710, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3467, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3464, val_acc: 0.8811, val_top_1: 0.8811, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3466, val_acc: 0.8791, val_top_1: 0.8791, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3520, val_acc: 0.8798, val_top_1: 0.8798, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3468, val_acc: 0.8803, val_top_1: 0.8803, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3368, val_acc: 0.8834, val_top_1: 0.8834, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3420, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3407, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3436, val_acc: 0.8791, val_top_1: 0.8791, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3806, val_acc: 0.8731, val_top_1: 0.8731, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3590, val_acc: 0.8794, val_top_1: 0.8794, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3376, val_acc: 0.8823, val_top_1: 0.8823, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3543, val_acc: 0.8756, val_top_1: 0.8756, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3828, val_acc: 0.8675, val_top_1: 0.8675, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3654, val_acc: 0.8693, val_top_1: 0.8693, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Compression= 0.060075300088152185 Result after pruning is  {'val_loss': 0.3815883994102478, 'val_acc': 0.861523449420929, 'val_top_1': 0.861523449420929, 'val_top_5': 0.996777355670929}\n",
      "Mask compression =  0.2 tensor(0.1355)\n",
      "After masking, Compression= 0.13524357189345942 Result after pruning is  {'val_loss': 0.380027711391449, 'val_acc': 0.86279296875, 'val_top_1': 0.86279296875, 'val_top_5': 0.996777355670929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3494, val_acc: 0.8769, val_top_1: 0.8769, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3305, val_acc: 0.8848, val_top_1: 0.8848, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3353, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3304, val_acc: 0.8839, val_top_1: 0.8839, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3342, val_acc: 0.8808, val_top_1: 0.8808, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3482, val_acc: 0.8748, val_top_1: 0.8748, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3336, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3391, val_acc: 0.8811, val_top_1: 0.8811, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3322, val_acc: 0.8849, val_top_1: 0.8849, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3484, val_acc: 0.8744, val_top_1: 0.8744, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3317, val_acc: 0.8830, val_top_1: 0.8830, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3379, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3304, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3445, val_acc: 0.8797, val_top_1: 0.8797, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3333, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3375, val_acc: 0.8799, val_top_1: 0.8799, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3638, val_acc: 0.8705, val_top_1: 0.8705, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3353, val_acc: 0.8860, val_top_1: 0.8860, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3299, val_acc: 0.8870, val_top_1: 0.8870, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3249, val_acc: 0.8879, val_top_1: 0.8879, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3213, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3305, val_acc: 0.8803, val_top_1: 0.8803, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3370, val_acc: 0.8862, val_top_1: 0.8862, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3287, val_acc: 0.8849, val_top_1: 0.8849, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3435, val_acc: 0.8794, val_top_1: 0.8794, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3322, val_acc: 0.8846, val_top_1: 0.8846, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3195, val_acc: 0.8871, val_top_1: 0.8871, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3466, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3648, val_acc: 0.8710, val_top_1: 0.8710, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3338, val_acc: 0.8878, val_top_1: 0.8878, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3367, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3243, val_acc: 0.8836, val_top_1: 0.8836, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3211, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3210, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3262, val_acc: 0.8860, val_top_1: 0.8860, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Compression= 0.13524357189345942 Result after pruning is  {'val_loss': 0.34277859330177307, 'val_acc': 0.8819335699081421, 'val_top_1': 0.8819335699081421, 'val_top_5': 0.9966796636581421}\n",
      "Mask compression =  0.3 tensor(0.2085)\n",
      "After masking, Compression= 0.2080762228025803 Result after pruning is  {'val_loss': 0.3513948917388916, 'val_acc': 0.8790038824081421, 'val_top_1': 0.8790038824081421, 'val_top_5': 0.996777355670929}\n",
      "At train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.3227, val_acc: 0.8878, val_top_1: 0.8878, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3287, val_acc: 0.8871, val_top_1: 0.8871, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3662, val_acc: 0.8781, val_top_1: 0.8781, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3470, val_acc: 0.8842, val_top_1: 0.8842, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3215, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3234, val_acc: 0.8879, val_top_1: 0.8879, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3466, val_acc: 0.8782, val_top_1: 0.8782, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3279, val_acc: 0.8874, val_top_1: 0.8874, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3277, val_acc: 0.8856, val_top_1: 0.8856, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3298, val_acc: 0.8853, val_top_1: 0.8853, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3537, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3688, val_acc: 0.8721, val_top_1: 0.8721, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3191, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3224, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3281, val_acc: 0.8874, val_top_1: 0.8874, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3271, val_acc: 0.8875, val_top_1: 0.8875, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3266, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3301, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3311, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3195, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3337, val_acc: 0.8852, val_top_1: 0.8852, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3338, val_acc: 0.8846, val_top_1: 0.8846, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3593, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3292, val_acc: 0.8853, val_top_1: 0.8853, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3542, val_acc: 0.8780, val_top_1: 0.8780, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3360, val_acc: 0.8855, val_top_1: 0.8855, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3204, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3237, val_acc: 0.8879, val_top_1: 0.8879, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3246, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3394, val_acc: 0.8840, val_top_1: 0.8840, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3365, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3222, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3344, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3452, val_acc: 0.8871, val_top_1: 0.8871, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3309, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Compression= 0.2080762228025803 Result after pruning is  {'val_loss': 0.35169991850852966, 'val_acc': 0.8818359375, 'val_top_1': 0.8818359375, 'val_top_5': 0.996777355670929}\n",
      "Mask compression =  0.4 tensor(0.2768)\n",
      "After masking, Compression= 0.2762870630494064 Result after pruning is  {'val_loss': 0.3567980229854584, 'val_acc': 0.8802734613418579, 'val_top_1': 0.8802734613418579, 'val_top_5': 0.996777355670929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3418, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3629, val_acc: 0.8814, val_top_1: 0.8814, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3999, val_acc: 0.8662, val_top_1: 0.8662, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3666, val_acc: 0.8764, val_top_1: 0.8764, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3301, val_acc: 0.8899, val_top_1: 0.8899, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3354, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3370, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3297, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3517, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3381, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3384, val_acc: 0.8876, val_top_1: 0.8876, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3721, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3488, val_acc: 0.8840, val_top_1: 0.8840, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3338, val_acc: 0.8864, val_top_1: 0.8864, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3644, val_acc: 0.8819, val_top_1: 0.8819, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3354, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3497, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3564, val_acc: 0.8841, val_top_1: 0.8841, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3405, val_acc: 0.8871, val_top_1: 0.8871, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3309, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3432, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3601, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3884, val_acc: 0.8808, val_top_1: 0.8808, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3401, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3850, val_acc: 0.8802, val_top_1: 0.8802, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3791, val_acc: 0.8783, val_top_1: 0.8783, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3939, val_acc: 0.8724, val_top_1: 0.8724, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3548, val_acc: 0.8860, val_top_1: 0.8860, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3584, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3723, val_acc: 0.8856, val_top_1: 0.8856, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3762, val_acc: 0.8777, val_top_1: 0.8777, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4143, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3562, val_acc: 0.8906, val_top_1: 0.8906, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3672, val_acc: 0.8832, val_top_1: 0.8832, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3673, val_acc: 0.8840, val_top_1: 0.8840, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Compression= 0.2762870630494064 Result after pruning is  {'val_loss': 0.37596431374549866, 'val_acc': 0.87890625, 'val_top_1': 0.87890625, 'val_top_5': 0.9970703125}\n",
      "Mask compression =  0.6 tensor(0.3787)\n",
      "After masking, Compression= 0.3779504205765317 Result after pruning is  {'val_loss': 0.38792550563812256, 'val_acc': 0.8758789300918579, 'val_top_1': 0.8758789300918579, 'val_top_5': 0.996777355670929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3646, val_acc: 0.8847, val_top_1: 0.8847, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3515, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3559, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3759, val_acc: 0.8835, val_top_1: 0.8835, val_top_5: 0.9979\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], val_loss: 0.3830, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3457, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3669, val_acc: 0.8875, val_top_1: 0.8875, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.4031, val_acc: 0.8727, val_top_1: 0.8727, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3564, val_acc: 0.8880, val_top_1: 0.8880, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3475, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3776, val_acc: 0.8819, val_top_1: 0.8819, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3722, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.4115, val_acc: 0.8720, val_top_1: 0.8720, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3565, val_acc: 0.8906, val_top_1: 0.8906, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4021, val_acc: 0.8742, val_top_1: 0.8742, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3677, val_acc: 0.8861, val_top_1: 0.8861, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3510, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3829, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3681, val_acc: 0.8841, val_top_1: 0.8841, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4180, val_acc: 0.8704, val_top_1: 0.8704, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3632, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4800, val_acc: 0.8738, val_top_1: 0.8738, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3695, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3747, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3668, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4691, val_acc: 0.8777, val_top_1: 0.8777, val_top_5: 0.9962\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3846, val_acc: 0.8859, val_top_1: 0.8859, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3979, val_acc: 0.8843, val_top_1: 0.8843, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3698, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3822, val_acc: 0.8870, val_top_1: 0.8870, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3975, val_acc: 0.8850, val_top_1: 0.8850, val_top_5: 0.9979\n",
      "wt desc =  None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-1286717405aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#     epochs=3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     history_prune,_ = fit(epochs, lr, model, train_loader, val_loader,\n\u001b[0;32m---> 54\u001b[0;31m                           mask_whole_model=mask_whole_model)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b0bc69812b15>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func, weight_description, mask_whole_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dlproject/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dlproject/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model_state_path=\"model_state/mod.pt\"\n",
    "model.load_state_dict(torch.load(model_state_path))\n",
    "weight_description= pickle.load( open( \"pickles/weight_description.p\", \"rb\" ) )\n",
    "\n",
    "result = evaluate(model, test_loader)\n",
    "print(\"Test result is \",result)\n",
    "\n",
    "\n",
    "# import copy\n",
    "# save_model_state=copy.deepcopy(model)\n",
    "\n",
    "total_size,nz_size=model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "print(\"Compression=\",compression)\n",
    "\n",
    "\n",
    "\n",
    "last_few=50\n",
    "mean_weight_description_weighted=get_weighted_mean_dict_weight_dict(weight_description,last_few)\n",
    "\n",
    "metrics={}\n",
    "metrics[\"prune_rate\"]=[]\n",
    "metrics[\"compression\"]=[]\n",
    "metrics[\"epochs\"]=[]\n",
    "metrics[\"top_5\"]=[]\n",
    "metrics[\"top_1\"]=[]\n",
    "\n",
    "prune_rate_range=[0.1,0.2,0.3,0.4,0.6,0.8,0.9,1.1,1.3,1.5,1.7,\n",
    "                  1.75,1.8,1.85,1.9,1.92,1.93,1.95,1.99,2.1,2.12,2.2,\n",
    "                 2.25,2.3,2.35,2.4,2.5,2.7,2.9,4]\n",
    "\n",
    "# prune_rate_range=[0.1,0.2,1.95,2.9,4]\n",
    "\n",
    "for prune_rate in prune_rate_range:\n",
    "    mask_whole_model=create_mask_from_mean_wt(model,mean_weight_description_weighted,prune_rate)\n",
    "    print(\"Mask compression = \",prune_rate,get_mask_compression(mask_whole_model))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # prune_rate=1.98 # compression of 0.9627\n",
    "    mask_whole_model=create_mask_from_mean_wt(model,mean_weight_description,prune_rate)\n",
    "    apply_mask_model(model,mask_whole_model)\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "    print(\"After masking, Compression=\",compression,\"Result after pruning is \",res)\n",
    "\n",
    "\n",
    "    epochs=35\n",
    "#     epochs=3    \n",
    "    history_prune,_ = fit(epochs, lr, model, train_loader, val_loader,\n",
    "                          mask_whole_model=mask_whole_model)\n",
    "\n",
    "\n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"Compression=\",compression,\"Result after pruning is \",res)\n",
    "    metrics[\"prune_rate\"].append(prune_rate)\n",
    "    metrics[\"compression\"].append(compression)\n",
    "    metrics[\"epochs\"].append(epochs)\n",
    "    metrics[\"top_5\"].append(res['val_top_5'])\n",
    "    metrics[\"top_1\"].append(res['val_top_1'])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe_results=pd.DataFrame(metrics)\n",
    "print(dataframe_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_results.to_csv(\"results_sheet/mean_weighted.csv\",\n",
    "                         index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression= 0.9589474464702054 Result after pruning is  {'val_loss': 0.41391271352767944, 'val_acc': 0.854296863079071, 'val_top_1': 0.854296863079071, 'val_top_5': 0.995898425579071}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "square root weighted mean of of all 50/50 epochs\n",
    "Compression= 0.9589474464702054 \n",
    "Result after pruning is  {'val_loss': 0.41391271352767944, \n",
    "'val_acc': 0.854296863079071, 'val_top_1': 0.854296863079071, \n",
    "'val_top_5': 0.995898425579071}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weighted mean of last 11/50 epochs\n",
    "Compression= 0.9589474464702054 \n",
    "Result after pruning is  {'val_loss': 0.4113593101501465, \n",
    "'val_acc': 0.8514648675918579, 'val_top_1': 0.8514648675918579, \n",
    "'val_top_5': 0.9959961175918579}\n",
    "\n",
    "\n",
    "simple mean of 50 epochs\n",
    "\n",
    "Compression= 0.9570237516580025 \n",
    "Result after pruning is  {'val_loss': 0.40947121381759644, \n",
    "'val_acc': 0.8536132574081421, 'val_top_1': 0.8536132574081421, \n",
    "'val_top_5': 0.9961913824081421}\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression= 0.960854664239049 Result after pruning is  {'val_loss': 1.0059268474578857, 'val_acc': 0.540332019329071, 'val_top_1': 0.540332019329071, 'val_top_5': 0.9852539300918579}\n"
     ]
    }
   ],
   "source": [
    "total_size,nz_size=model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "res = evaluate(model, test_loader)\n",
    "\n",
    "print(\"Compression=\",compression,\"Result after pruning is \",res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlkernel",
   "language": "python",
   "name": "dlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
