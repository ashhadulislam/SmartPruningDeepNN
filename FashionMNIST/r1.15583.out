Program with weighht pruning, dynamic, many ranges
Torch cuda  False
device  cpu
Test.py:567: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  training_data=torch.tensor(train_ds.dataset.data)
initial result [{'val_loss': 2.306201934814453, 'val_acc': 0.0322265625, 'val_top_1': 0.0322265625, 'val_top_5': 0.504589855670929}]
Test result is  {'val_loss': 0.3928965628147125, 'val_acc': 0.860644519329071, 'val_top_1': 0.860644519329071, 'val_top_5': 0.996777355670929}
Compression= 0.0
Mask compression =  0.1 tensor(0.0615)
After masking, Compression= 0.06138110577437984 Result after pruning is  {'val_loss': 0.39220890402793884, 'val_acc': 0.861035168170929, 'val_top_1': 0.861035168170929, 'val_top_5': 0.996777355670929}
At train
Epoch [0], val_loss: 0.3308, val_acc: 0.8834, val_top_1: 0.8834, val_top_5: 0.9972
Epoch [1], val_loss: 0.3495, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9970
Epoch [2], val_loss: 0.3383, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9971
Epoch [3], val_loss: 0.3334, val_acc: 0.8830, val_top_1: 0.8830, val_top_5: 0.9972
Epoch [4], val_loss: 0.3495, val_acc: 0.8769, val_top_1: 0.8769, val_top_5: 0.9974
Epoch [5], val_loss: 0.3624, val_acc: 0.8740, val_top_1: 0.8740, val_top_5: 0.9971
Epoch [6], val_loss: 0.3291, val_acc: 0.8855, val_top_1: 0.8855, val_top_5: 0.9968
Epoch [7], val_loss: 0.3289, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9970
Epoch [8], val_loss: 0.3282, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9967
Epoch [9], val_loss: 0.3204, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9974
Epoch [10], val_loss: 0.3456, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9972
Epoch [11], val_loss: 0.3180, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9972
Epoch [12], val_loss: 0.3355, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9973
Epoch [13], val_loss: 0.3187, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9974
Epoch [14], val_loss: 0.3246, val_acc: 0.8874, val_top_1: 0.8874, val_top_5: 0.9978
Epoch [15], val_loss: 0.3314, val_acc: 0.8833, val_top_1: 0.8833, val_top_5: 0.9974
Epoch [16], val_loss: 0.3162, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9975
Epoch [17], val_loss: 0.3164, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9974
Epoch [18], val_loss: 0.3324, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9976
Epoch [19], val_loss: 0.3135, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9977
Epoch [20], val_loss: 0.3165, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9973
Epoch [21], val_loss: 0.3201, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9974
Epoch [22], val_loss: 0.3170, val_acc: 0.8875, val_top_1: 0.8875, val_top_5: 0.9975
Epoch [23], val_loss: 0.3163, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9976
Epoch [24], val_loss: 0.3317, val_acc: 0.8851, val_top_1: 0.8851, val_top_5: 0.9972
Epoch [25], val_loss: 0.3095, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9975
Epoch [26], val_loss: 0.3102, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9976
Epoch [27], val_loss: 0.3895, val_acc: 0.8677, val_top_1: 0.8677, val_top_5: 0.9974
Epoch [28], val_loss: 0.3133, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9974
Epoch [29], val_loss: 0.3121, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9974
Epoch [30], val_loss: 0.3147, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9975
Epoch [31], val_loss: 0.3567, val_acc: 0.8710, val_top_1: 0.8710, val_top_5: 0.9977
Epoch [32], val_loss: 0.3133, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9976
Epoch [33], val_loss: 0.3036, val_acc: 0.8909, val_top_1: 0.8909, val_top_5: 0.9976
Epoch [34], val_loss: 0.3043, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9976
Compression= 0.06138110577437984 Result after pruning is  {'val_loss': 0.3489318788051605, 'val_acc': 0.8755859136581421, 'val_top_1': 0.8755859136581421, 'val_top_5': 0.99658203125}
Mask compression =  0.2 tensor(0.1393)
After masking, Compression= 0.13907036521366606 Result after pruning is  {'val_loss': 0.3500919044017792, 'val_acc': 0.8749023675918579, 'val_top_1': 0.8749023675918579, 'val_top_5': 0.996777355670929}
At train
Epoch [0], val_loss: 0.3263, val_acc: 0.8847, val_top_1: 0.8847, val_top_5: 0.9975
Epoch [1], val_loss: 0.3155, val_acc: 0.8865, val_top_1: 0.8865, val_top_5: 0.9977
Epoch [2], val_loss: 0.3277, val_acc: 0.8790, val_top_1: 0.8790, val_top_5: 0.9975
Epoch [3], val_loss: 0.3184, val_acc: 0.8862, val_top_1: 0.8862, val_top_5: 0.9979
Epoch [4], val_loss: 0.3025, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9975
Epoch [5], val_loss: 0.3327, val_acc: 0.8783, val_top_1: 0.8783, val_top_5: 0.9975
Epoch [6], val_loss: 0.3245, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9972
Epoch [7], val_loss: 0.3076, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9973
Epoch [8], val_loss: 0.3433, val_acc: 0.8753, val_top_1: 0.8753, val_top_5: 0.9975
Epoch [9], val_loss: 0.3028, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9974
Epoch [10], val_loss: 0.3173, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9976
Epoch [11], val_loss: 0.3090, val_acc: 0.8901, val_top_1: 0.8901, val_top_5: 0.9976
Epoch [12], val_loss: 0.3127, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9975
Epoch [13], val_loss: 0.3016, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9976
Epoch [14], val_loss: 0.3015, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9978
Epoch [15], val_loss: 0.3013, val_acc: 0.8909, val_top_1: 0.8909, val_top_5: 0.9977
Epoch [16], val_loss: 0.2977, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9975
Epoch [17], val_loss: 0.2983, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9977
Epoch [18], val_loss: 0.3070, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9978
Epoch [19], val_loss: 0.2946, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9976
Epoch [20], val_loss: 0.3081, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9977
Epoch [21], val_loss: 0.3281, val_acc: 0.8843, val_top_1: 0.8843, val_top_5: 0.9976
Epoch [22], val_loss: 0.3077, val_acc: 0.8901, val_top_1: 0.8901, val_top_5: 0.9978
Epoch [23], val_loss: 0.3230, val_acc: 0.8872, val_top_1: 0.8872, val_top_5: 0.9976
Epoch [24], val_loss: 0.3016, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9975
Epoch [25], val_loss: 0.3039, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9975
Epoch [26], val_loss: 0.2970, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9976
Epoch [27], val_loss: 0.2914, val_acc: 0.8985, val_top_1: 0.8985, val_top_5: 0.9977
Epoch [28], val_loss: 0.2958, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9977
Epoch [29], val_loss: 0.3181, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9978
Epoch [30], val_loss: 0.2975, val_acc: 0.8957, val_top_1: 0.8957, val_top_5: 0.9977
Epoch [31], val_loss: 0.3205, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9975
Epoch [32], val_loss: 0.3175, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9979
Epoch [33], val_loss: 0.3033, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9978
Epoch [34], val_loss: 0.3075, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9976
Compression= 0.13907036521366606 Result after pruning is  {'val_loss': 0.34468477964401245, 'val_acc': 0.8804687261581421, 'val_top_1': 0.8804687261581421, 'val_top_5': 0.99658203125}
Mask compression =  0.3 tensor(0.2139)
After masking, Compression= 0.21353012415452172 Result after pruning is  {'val_loss': 0.3492323160171509, 'val_acc': 0.8788086175918579, 'val_top_1': 0.8788086175918579, 'val_top_5': 0.996386706829071}
At train
Epoch [0], val_loss: 0.3154, val_acc: 0.8882, val_top_1: 0.8882, val_top_5: 0.9977
Epoch [1], val_loss: 0.3127, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9978
Epoch [2], val_loss: 0.3089, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9978
Epoch [3], val_loss: 0.3127, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9978
Epoch [4], val_loss: 0.2952, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9977
Epoch [5], val_loss: 0.2929, val_acc: 0.8994, val_top_1: 0.8994, val_top_5: 0.9978
Epoch [6], val_loss: 0.2937, val_acc: 0.8981, val_top_1: 0.8981, val_top_5: 0.9975
Epoch [7], val_loss: 0.2891, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9979
Epoch [8], val_loss: 0.2919, val_acc: 0.8974, val_top_1: 0.8974, val_top_5: 0.9979
Epoch [9], val_loss: 0.3061, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9977
Epoch [10], val_loss: 0.3155, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9977
Epoch [11], val_loss: 0.3032, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9974
Epoch [12], val_loss: 0.2960, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9977
Epoch [13], val_loss: 0.3324, val_acc: 0.8835, val_top_1: 0.8835, val_top_5: 0.9975
Epoch [14], val_loss: 0.3043, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9979
Epoch [15], val_loss: 0.2946, val_acc: 0.8957, val_top_1: 0.8957, val_top_5: 0.9979
Epoch [16], val_loss: 0.3129, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9973
Epoch [17], val_loss: 0.3102, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9976
Epoch [18], val_loss: 0.3340, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9978
Epoch [19], val_loss: 0.3393, val_acc: 0.8851, val_top_1: 0.8851, val_top_5: 0.9974
Epoch [20], val_loss: 0.2957, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9979
Epoch [21], val_loss: 0.3058, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9979
Epoch [22], val_loss: 0.3206, val_acc: 0.8899, val_top_1: 0.8899, val_top_5: 0.9977
Epoch [23], val_loss: 0.3092, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9976
Epoch [24], val_loss: 0.3745, val_acc: 0.8694, val_top_1: 0.8694, val_top_5: 0.9976
Epoch [25], val_loss: 0.3310, val_acc: 0.8849, val_top_1: 0.8849, val_top_5: 0.9979
Epoch [26], val_loss: 0.2991, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9978
Epoch [27], val_loss: 0.3011, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9974
Epoch [28], val_loss: 0.3075, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9977
Epoch [29], val_loss: 0.2939, val_acc: 0.8990, val_top_1: 0.8990, val_top_5: 0.9976
Epoch [30], val_loss: 0.3288, val_acc: 0.8872, val_top_1: 0.8872, val_top_5: 0.9975
Epoch [31], val_loss: 0.2988, val_acc: 0.8971, val_top_1: 0.8971, val_top_5: 0.9977
Epoch [32], val_loss: 0.3196, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9976
Epoch [33], val_loss: 0.2972, val_acc: 0.8980, val_top_1: 0.8980, val_top_5: 0.9976
Epoch [34], val_loss: 0.3001, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9977
Compression= 0.21353012415452172 Result after pruning is  {'val_loss': 0.34344804286956787, 'val_acc': 0.8836914300918579, 'val_top_1': 0.8836914300918579, 'val_top_5': 0.9971679449081421}
Mask compression =  0.4 tensor(0.2826)
After masking, Compression= 0.2820581474860151 Result after pruning is  {'val_loss': 0.3595576882362366, 'val_acc': 0.8765624761581421, 'val_top_1': 0.8765624761581421, 'val_top_5': 0.996874988079071}
At train
Epoch [0], val_loss: 0.2978, val_acc: 0.8984, val_top_1: 0.8984, val_top_5: 0.9977
Epoch [1], val_loss: 0.3241, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9979
Epoch [2], val_loss: 0.3074, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9974
Epoch [3], val_loss: 0.3126, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9974
Epoch [4], val_loss: 0.2985, val_acc: 0.8982, val_top_1: 0.8982, val_top_5: 0.9974
Epoch [5], val_loss: 0.2954, val_acc: 0.8987, val_top_1: 0.8987, val_top_5: 0.9978
Epoch [6], val_loss: 0.3284, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9976
Epoch [7], val_loss: 0.3277, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9975
Epoch [8], val_loss: 0.2984, val_acc: 0.8982, val_top_1: 0.8982, val_top_5: 0.9973
Epoch [9], val_loss: 0.3711, val_acc: 0.8844, val_top_1: 0.8844, val_top_5: 0.9974
Epoch [10], val_loss: 0.3068, val_acc: 0.8971, val_top_1: 0.8971, val_top_5: 0.9972
Epoch [11], val_loss: 0.3523, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9977
Epoch [12], val_loss: 0.2990, val_acc: 0.8998, val_top_1: 0.8998, val_top_5: 0.9977
Epoch [13], val_loss: 0.3224, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9975
Epoch [14], val_loss: 0.3110, val_acc: 0.8970, val_top_1: 0.8970, val_top_5: 0.9977
Epoch [15], val_loss: 0.3049, val_acc: 0.8999, val_top_1: 0.8999, val_top_5: 0.9974
Epoch [16], val_loss: 0.3144, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9978
Epoch [17], val_loss: 0.4025, val_acc: 0.8781, val_top_1: 0.8781, val_top_5: 0.9973
Epoch [18], val_loss: 0.3222, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9979
Epoch [19], val_loss: 0.3109, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9974
Epoch [20], val_loss: 0.3532, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9978
Epoch [21], val_loss: 0.3353, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9977
Epoch [22], val_loss: 0.3221, val_acc: 0.8965, val_top_1: 0.8965, val_top_5: 0.9974
Epoch [23], val_loss: 0.3123, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9975
Epoch [24], val_loss: 0.3143, val_acc: 0.8963, val_top_1: 0.8963, val_top_5: 0.9972
Epoch [25], val_loss: 0.3089, val_acc: 0.8987, val_top_1: 0.8987, val_top_5: 0.9976
Epoch [26], val_loss: 0.3119, val_acc: 0.8989, val_top_1: 0.8989, val_top_5: 0.9976
Epoch [27], val_loss: 0.3255, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9974
Epoch [28], val_loss: 0.3231, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9974
Epoch [29], val_loss: 0.4120, val_acc: 0.8713, val_top_1: 0.8713, val_top_5: 0.9977
Epoch [30], val_loss: 0.3208, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9975
Epoch [31], val_loss: 0.3287, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9976
Epoch [32], val_loss: 0.3500, val_acc: 0.8862, val_top_1: 0.8862, val_top_5: 0.9973
Epoch [33], val_loss: 0.4369, val_acc: 0.8744, val_top_1: 0.8745, val_top_5: 0.9955
Epoch [34], val_loss: 0.3368, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9979
Compression= 0.2820581474860151 Result after pruning is  {'val_loss': 0.3893526494503021, 'val_acc': 0.880566418170929, 'val_top_1': 0.880566418170929, 'val_top_5': 0.9964843988418579}
Mask compression =  0.6 tensor(0.3860)
After masking, Compression= 0.38530330117563705 Result after pruning is  {'val_loss': 0.4150298237800598, 'val_acc': 0.872265636920929, 'val_top_1': 0.872265636920929, 'val_top_5': 0.99560546875}
At train
Epoch [0], val_loss: 0.3395, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9977
Epoch [1], val_loss: 0.3199, val_acc: 0.8985, val_top_1: 0.8985, val_top_5: 0.9974
Epoch [2], val_loss: 0.3352, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9976
Epoch [3], val_loss: 0.3196, val_acc: 0.8977, val_top_1: 0.8977, val_top_5: 0.9977
Epoch [4], val_loss: 0.3230, val_acc: 0.8982, val_top_1: 0.8982, val_top_5: 0.9974
Epoch [5], val_loss: 0.3347, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9977
Epoch [6], val_loss: 0.3150, val_acc: 0.9003, val_top_1: 0.9003, val_top_5: 0.9973
Epoch [7], val_loss: 0.3356, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9976
Epoch [8], val_loss: 0.3224, val_acc: 0.8979, val_top_1: 0.8979, val_top_5: 0.9975
Epoch [9], val_loss: 0.3622, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9973
Epoch [10], val_loss: 0.4850, val_acc: 0.8672, val_top_1: 0.8672, val_top_5: 0.9968
Epoch [11], val_loss: 0.3581, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9972
Epoch [12], val_loss: 0.3656, val_acc: 0.8870, val_top_1: 0.8870, val_top_5: 0.9978
Epoch [13], val_loss: 0.3258, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9976
Epoch [14], val_loss: 0.4002, val_acc: 0.8747, val_top_1: 0.8747, val_top_5: 0.9979
Epoch [15], val_loss: 0.3280, val_acc: 0.8996, val_top_1: 0.8996, val_top_5: 0.9977
Epoch [16], val_loss: 0.4062, val_acc: 0.8823, val_top_1: 0.8823, val_top_5: 0.9971
Epoch [17], val_loss: 0.3292, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9976
Epoch [18], val_loss: 0.3423, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9976
Epoch [19], val_loss: 0.3916, val_acc: 0.8777, val_top_1: 0.8777, val_top_5: 0.9975
Epoch [20], val_loss: 0.3434, val_acc: 0.8975, val_top_1: 0.8975, val_top_5: 0.9976
Epoch [21], val_loss: 0.3523, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9971
Epoch [22], val_loss: 0.3460, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9977
Epoch [23], val_loss: 0.3579, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9973
Epoch [24], val_loss: 0.4101, val_acc: 0.8764, val_top_1: 0.8764, val_top_5: 0.9979
Epoch [25], val_loss: 0.3367, val_acc: 0.8974, val_top_1: 0.8974, val_top_5: 0.9975
Epoch [26], val_loss: 0.3498, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9979
Epoch [27], val_loss: 0.3364, val_acc: 0.8984, val_top_1: 0.8984, val_top_5: 0.9976
Epoch [28], val_loss: 0.3430, val_acc: 0.8979, val_top_1: 0.8979, val_top_5: 0.9975
Epoch [29], val_loss: 0.4046, val_acc: 0.8848, val_top_1: 0.8848, val_top_5: 0.9973
Epoch [30], val_loss: 0.3405, val_acc: 0.8986, val_top_1: 0.8986, val_top_5: 0.9977
Epoch [31], val_loss: 0.3648, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9972
Epoch [32], val_loss: 0.3974, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9976
Epoch [33], val_loss: 0.3758, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9976
Epoch [34], val_loss: 0.3564, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9974
Compression= 0.38530330117563705 Result after pruning is  {'val_loss': 0.4027293622493744, 'val_acc': 0.879199206829071, 'val_top_1': 0.879199206829071, 'val_top_5': 0.997363269329071}
Mask compression =  0.8 tensor(0.4796)
After masking, Compression= 0.4786663481104951 Result after pruning is  {'val_loss': 0.4193502962589264, 'val_acc': 0.8701171875, 'val_top_1': 0.8701171875, 'val_top_5': 0.997265636920929}
At train
Epoch [0], val_loss: 0.3754, val_acc: 0.8906, val_top_1: 0.8906, val_top_5: 0.9968
Epoch [1], val_loss: 0.3576, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9975
Epoch [2], val_loss: 0.3530, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9979
Epoch [3], val_loss: 0.3841, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9971
Epoch [4], val_loss: 0.4144, val_acc: 0.8766, val_top_1: 0.8766, val_top_5: 0.9974
Epoch [5], val_loss: 0.3689, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9968
Epoch [6], val_loss: 0.3571, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9977
Epoch [7], val_loss: 0.3606, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9974
Epoch [8], val_loss: 0.4022, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9975
Epoch [9], val_loss: 0.4719, val_acc: 0.8670, val_top_1: 0.8670, val_top_5: 0.9975
Epoch [10], val_loss: 0.4831, val_acc: 0.8713, val_top_1: 0.8713, val_top_5: 0.9973
Epoch [11], val_loss: 0.4096, val_acc: 0.8814, val_top_1: 0.8814, val_top_5: 0.9979
Epoch [12], val_loss: 0.3662, val_acc: 0.8971, val_top_1: 0.8971, val_top_5: 0.9974
Epoch [13], val_loss: 0.3904, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9979
Epoch [14], val_loss: 0.3610, val_acc: 0.8983, val_top_1: 0.8983, val_top_5: 0.9974
Epoch [15], val_loss: 0.3630, val_acc: 0.8960, val_top_1: 0.8960, val_top_5: 0.9974
Epoch [16], val_loss: 0.3856, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9975
Epoch [17], val_loss: 0.3810, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9971
Epoch [18], val_loss: 0.3594, val_acc: 0.8977, val_top_1: 0.8977, val_top_5: 0.9975
Epoch [19], val_loss: 0.3844, val_acc: 0.8901, val_top_1: 0.8901, val_top_5: 0.9978
Epoch [20], val_loss: 0.3833, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9976
Epoch [21], val_loss: 0.3941, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9977
Epoch [22], val_loss: 0.4035, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9973
Epoch [23], val_loss: 0.4525, val_acc: 0.8767, val_top_1: 0.8767, val_top_5: 0.9976
Epoch [24], val_loss: 0.3892, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9972
Epoch [25], val_loss: 0.4100, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9970
Epoch [26], val_loss: 0.3869, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9974
Epoch [27], val_loss: 0.4362, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9975
Epoch [28], val_loss: 0.3907, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9973
Epoch [29], val_loss: 0.3976, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9971
Epoch [30], val_loss: 0.3975, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9977
Epoch [31], val_loss: 0.3939, val_acc: 0.8968, val_top_1: 0.8968, val_top_5: 0.9974
Epoch [32], val_loss: 0.3993, val_acc: 0.8979, val_top_1: 0.8979, val_top_5: 0.9974
Epoch [33], val_loss: 0.3779, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9973
Epoch [34], val_loss: 0.3981, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9974
Compression= 0.4786663481104951 Result after pruning is  {'val_loss': 0.4526952803134918, 'val_acc': 0.880078136920929, 'val_top_1': 0.880078136920929, 'val_top_5': 0.9969726800918579}
Mask compression =  0.9 tensor(0.5260)
After masking, Compression= 0.5250368673845165 Result after pruning is  {'val_loss': 0.48855096101760864, 'val_acc': 0.872363269329071, 'val_top_1': 0.872363269329071, 'val_top_5': 0.996386706829071}
At train
Epoch [0], val_loss: 0.4208, val_acc: 0.8838, val_top_1: 0.8838, val_top_5: 0.9979
Epoch [1], val_loss: 0.4041, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9970
Epoch [2], val_loss: 0.4024, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9974
Epoch [3], val_loss: 0.4040, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9975
Epoch [4], val_loss: 0.3865, val_acc: 0.8960, val_top_1: 0.8960, val_top_5: 0.9973
Epoch [5], val_loss: 0.4132, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9973
Epoch [6], val_loss: 0.4297, val_acc: 0.8855, val_top_1: 0.8855, val_top_5: 0.9975
Epoch [7], val_loss: 0.3928, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9979
Epoch [8], val_loss: 0.4374, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9968
Epoch [9], val_loss: 0.4330, val_acc: 0.8865, val_top_1: 0.8865, val_top_5: 0.9968
Epoch [10], val_loss: 0.3970, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9975
Epoch [11], val_loss: 0.4252, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9968
Epoch [12], val_loss: 0.4308, val_acc: 0.8880, val_top_1: 0.8880, val_top_5: 0.9970
Epoch [13], val_loss: 0.4460, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9972
Epoch [14], val_loss: 0.4152, val_acc: 0.8973, val_top_1: 0.8973, val_top_5: 0.9975
Epoch [15], val_loss: 0.4066, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9978
Epoch [16], val_loss: 0.4074, val_acc: 0.8966, val_top_1: 0.8966, val_top_5: 0.9974
Epoch [17], val_loss: 0.4121, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9972
Epoch [18], val_loss: 0.4173, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9975
Epoch [19], val_loss: 0.4276, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9971
Epoch [20], val_loss: 0.4245, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9974
Epoch [21], val_loss: 0.4093, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9974
Epoch [22], val_loss: 0.4383, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9978
Epoch [23], val_loss: 0.4239, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9978
Epoch [24], val_loss: 0.4162, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9974
Epoch [25], val_loss: 0.4139, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9979
Epoch [26], val_loss: 0.4135, val_acc: 0.8975, val_top_1: 0.8975, val_top_5: 0.9977
Epoch [27], val_loss: 0.4833, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9973
Epoch [28], val_loss: 0.4694, val_acc: 0.8792, val_top_1: 0.8792, val_top_5: 0.9979
Epoch [29], val_loss: 0.4423, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9971
Epoch [30], val_loss: 0.4955, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9971
Epoch [31], val_loss: 0.4505, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9968
Epoch [32], val_loss: 0.4321, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9978
Epoch [33], val_loss: 0.4806, val_acc: 0.8806, val_top_1: 0.8806, val_top_5: 0.9978
Epoch [34], val_loss: 0.6294, val_acc: 0.8530, val_top_1: 0.8530, val_top_5: 0.9978
Compression= 0.5250368673845165 Result after pruning is  {'val_loss': 0.6946972608566284, 'val_acc': 0.8414062261581421, 'val_top_1': 0.8414062261581421, 'val_top_5': 0.997265636920929}
Mask compression =  1.1 tensor(0.6008)
After masking, Compression= 0.5997025893673639 Result after pruning is  {'val_loss': 1.3017547130584717, 'val_acc': 0.7572265863418579, 'val_top_1': 0.7572265863418579, 'val_top_5': 0.9966796636581421}
At train
Epoch [0], val_loss: 0.4291, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9968
Epoch [1], val_loss: 0.4145, val_acc: 0.8950, val_top_1: 0.8950, val_top_5: 0.9970
Epoch [2], val_loss: 0.4226, val_acc: 0.8956, val_top_1: 0.8956, val_top_5: 0.9974
Epoch [3], val_loss: 0.4198, val_acc: 0.8950, val_top_1: 0.8950, val_top_5: 0.9971
Epoch [4], val_loss: 0.4152, val_acc: 0.8962, val_top_1: 0.8962, val_top_5: 0.9976
Epoch [5], val_loss: 0.4214, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9975
Epoch [6], val_loss: 0.4369, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9971
Epoch [7], val_loss: 0.4489, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9967
Epoch [8], val_loss: 0.4479, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9973
Epoch [9], val_loss: 0.4307, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9971
Epoch [10], val_loss: 0.4357, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9976
Epoch [11], val_loss: 0.4281, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9975
Epoch [12], val_loss: 0.4439, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9971
Epoch [13], val_loss: 0.4445, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9970
Epoch [14], val_loss: 0.4605, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9971
Epoch [15], val_loss: 0.4377, val_acc: 0.8988, val_top_1: 0.8988, val_top_5: 0.9970
Epoch [16], val_loss: 0.4405, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9974
Epoch [17], val_loss: 0.4483, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9972
Epoch [18], val_loss: 0.4689, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9971
Epoch [19], val_loss: 0.4474, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9973
Epoch [20], val_loss: 0.5016, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9966
Epoch [21], val_loss: 0.4595, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9969
Epoch [22], val_loss: 0.4461, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9973
Epoch [23], val_loss: 0.4541, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9975
Epoch [24], val_loss: 0.4543, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9973
Epoch [25], val_loss: 0.4470, val_acc: 0.8958, val_top_1: 0.8958, val_top_5: 0.9972
Epoch [26], val_loss: 0.4693, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9970
Epoch [27], val_loss: 0.4620, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9973
Epoch [28], val_loss: 0.4625, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9969
Epoch [29], val_loss: 0.4730, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9969
Epoch [30], val_loss: 0.4601, val_acc: 0.8956, val_top_1: 0.8956, val_top_5: 0.9972
Epoch [31], val_loss: 0.4604, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9977
Epoch [32], val_loss: 0.5076, val_acc: 0.8878, val_top_1: 0.8878, val_top_5: 0.9967
Epoch [33], val_loss: 0.5252, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9969
Epoch [34], val_loss: 0.4695, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9970
Compression= 0.5997025893673639 Result after pruning is  {'val_loss': 0.541411280632019, 'val_acc': 0.8790038824081421, 'val_top_1': 0.8790038824081421, 'val_top_5': 0.9961913824081421}
Mask compression =  1.3 tensor(0.6674)
After masking, Compression= 0.666142147453061 Result after pruning is  {'val_loss': 0.6347576379776001, 'val_acc': 0.868457019329071, 'val_top_1': 0.868457019329071, 'val_top_5': 0.994921863079071}
At train
Epoch [0], val_loss: 0.4743, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9969
Epoch [1], val_loss: 0.4651, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9977
Epoch [2], val_loss: 0.5240, val_acc: 0.8848, val_top_1: 0.8848, val_top_5: 0.9964
Epoch [3], val_loss: 0.4679, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9974
Epoch [4], val_loss: 0.4640, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9975
Epoch [5], val_loss: 0.5052, val_acc: 0.8882, val_top_1: 0.8882, val_top_5: 0.9970
Epoch [6], val_loss: 0.4617, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9974
Epoch [7], val_loss: 0.4648, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9975
Epoch [8], val_loss: 0.4795, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9977
Epoch [9], val_loss: 0.4783, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9972
Epoch [10], val_loss: 0.4738, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9974
Epoch [11], val_loss: 0.4727, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9975
Epoch [12], val_loss: 0.4853, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9969
Epoch [13], val_loss: 0.4725, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9975
Epoch [14], val_loss: 0.4849, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9971
Epoch [15], val_loss: 0.5055, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9970
Epoch [16], val_loss: 0.5564, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9970
Epoch [17], val_loss: 0.5098, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9971
Epoch [18], val_loss: 0.5969, val_acc: 0.8737, val_top_1: 0.8737, val_top_5: 0.9976
Epoch [19], val_loss: 0.4936, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9972
Epoch [20], val_loss: 0.5048, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9972
Epoch [21], val_loss: 0.5039, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9970
Epoch [22], val_loss: 0.5010, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9970
Epoch [23], val_loss: 0.5440, val_acc: 0.8882, val_top_1: 0.8882, val_top_5: 0.9968
Epoch [24], val_loss: 0.4973, val_acc: 0.8974, val_top_1: 0.8974, val_top_5: 0.9971
Epoch [25], val_loss: 0.5049, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9972
Epoch [26], val_loss: 0.5386, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9968
Epoch [27], val_loss: 0.4966, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9971
Epoch [28], val_loss: 0.7920, val_acc: 0.8687, val_top_1: 0.8687, val_top_5: 0.9939
Epoch [29], val_loss: 0.5059, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9971
Epoch [30], val_loss: 0.5061, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9973
Epoch [31], val_loss: 0.5059, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9973
Epoch [32], val_loss: 0.5454, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9969
Epoch [33], val_loss: 0.5137, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9970
Epoch [34], val_loss: 0.5114, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9970
Compression= 0.666142147453061 Result after pruning is  {'val_loss': 0.5879220962524414, 'val_acc': 0.8785156011581421, 'val_top_1': 0.8785156011581421, 'val_top_5': 0.995410144329071}
Mask compression =  1.5 tensor(0.7233)
After masking, Compression= 0.7219292970069451 Result after pruning is  {'val_loss': 0.7010666728019714, 'val_acc': 0.8497070074081421, 'val_top_1': 0.8497070074081421, 'val_top_5': 0.993847668170929}
At train
Epoch [0], val_loss: 0.5123, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9970
Epoch [1], val_loss: 0.4892, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9972
Epoch [2], val_loss: 0.4889, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9964
Epoch [3], val_loss: 0.4952, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9967
Epoch [4], val_loss: 0.4974, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9971
Epoch [5], val_loss: 0.5159, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9962
Epoch [6], val_loss: 0.5193, val_acc: 0.8906, val_top_1: 0.8906, val_top_5: 0.9961
Epoch [7], val_loss: 0.5347, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9963
Epoch [8], val_loss: 0.5345, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9964
Epoch [9], val_loss: 0.5067, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9967
Epoch [10], val_loss: 0.5059, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9972
Epoch [11], val_loss: 0.5102, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9971
Epoch [12], val_loss: 0.5143, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9966
Epoch [13], val_loss: 0.5163, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9971
Epoch [14], val_loss: 0.5203, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9963
Epoch [15], val_loss: 0.5131, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9966
Epoch [16], val_loss: 0.5190, val_acc: 0.8962, val_top_1: 0.8962, val_top_5: 0.9971
Epoch [17], val_loss: 0.5404, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9963
Epoch [18], val_loss: 0.5407, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9970
Epoch [19], val_loss: 0.5281, val_acc: 0.8956, val_top_1: 0.8956, val_top_5: 0.9966
Epoch [20], val_loss: 0.5420, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9969
Epoch [21], val_loss: 0.5346, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9970
Epoch [22], val_loss: 0.5427, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9968
Epoch [23], val_loss: 0.5562, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9962
Epoch [24], val_loss: 0.5549, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9965
Epoch [25], val_loss: 0.5377, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9970
Epoch [26], val_loss: 0.5505, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9965
Epoch [27], val_loss: 0.5367, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9970
Epoch [28], val_loss: 0.5380, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9969
Epoch [29], val_loss: 0.5448, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9971
Epoch [30], val_loss: 0.5543, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9969
Epoch [31], val_loss: 0.5400, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9970
Epoch [32], val_loss: 0.5661, val_acc: 0.8904, val_top_1: 0.8904, val_top_5: 0.9965
Epoch [33], val_loss: 0.5483, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9969
Epoch [34], val_loss: 0.5773, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9972
Compression= 0.7219292970069451 Result after pruning is  {'val_loss': 0.680378794670105, 'val_acc': 0.878125011920929, 'val_top_1': 0.878125011920929, 'val_top_5': 0.994335949420929}
Mask compression =  1.7 tensor(0.7646)
After masking, Compression= 0.7631960521004111 Result after pruning is  {'val_loss': 0.9577813148498535, 'val_acc': 0.8431640863418579, 'val_top_1': 0.8431640863418579, 'val_top_5': 0.991992175579071}
At train
Epoch [0], val_loss: 0.5179, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9969
Epoch [1], val_loss: 0.5295, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9960
Epoch [2], val_loss: 0.5246, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9971
Epoch [3], val_loss: 0.5251, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9968
Epoch [4], val_loss: 0.5211, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9965
Epoch [5], val_loss: 0.5186, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9970
Epoch [6], val_loss: 0.5313, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9962
Epoch [7], val_loss: 0.5281, val_acc: 0.8958, val_top_1: 0.8958, val_top_5: 0.9963
Epoch [8], val_loss: 0.5265, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9966
Epoch [9], val_loss: 0.5356, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9964
Epoch [10], val_loss: 0.5332, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9966
Epoch [11], val_loss: 0.5718, val_acc: 0.8904, val_top_1: 0.8904, val_top_5: 0.9961
Epoch [12], val_loss: 0.5354, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9969
Epoch [13], val_loss: 0.5454, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9962
Epoch [14], val_loss: 0.5426, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9969
Epoch [15], val_loss: 0.5499, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9966
Epoch [16], val_loss: 0.5432, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9966
Epoch [17], val_loss: 0.5418, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9968
Epoch [18], val_loss: 0.5700, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9972
Epoch [19], val_loss: 0.5577, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9965
Epoch [20], val_loss: 0.5500, val_acc: 0.8909, val_top_1: 0.8909, val_top_5: 0.9970
Epoch [21], val_loss: 0.5743, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9964
Epoch [22], val_loss: 0.5687, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9961
Epoch [23], val_loss: 0.5537, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9972
Epoch [24], val_loss: 0.5541, val_acc: 0.8950, val_top_1: 0.8950, val_top_5: 0.9972
Epoch [25], val_loss: 0.5550, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9969
Epoch [26], val_loss: 0.6087, val_acc: 0.8841, val_top_1: 0.8841, val_top_5: 0.9969
Epoch [27], val_loss: 0.5641, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9963
Epoch [28], val_loss: 0.5786, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9967
Epoch [29], val_loss: 0.5761, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9968
Epoch [30], val_loss: 0.5700, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9969
Epoch [31], val_loss: 0.5783, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9964
Epoch [32], val_loss: 0.5781, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9967
Epoch [33], val_loss: 0.6064, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9963
Epoch [34], val_loss: 0.5762, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9965
Compression= 0.7631960521004111 Result after pruning is  {'val_loss': 0.6839939951896667, 'val_acc': 0.87890625, 'val_top_1': 0.87890625, 'val_top_5': 0.9942382574081421}
Mask compression =  1.75 tensor(0.7760)
After masking, Compression= 0.7745610927575156 Result after pruning is  {'val_loss': 0.9652184247970581, 'val_acc': 0.8433593511581421, 'val_top_1': 0.8433593511581421, 'val_top_5': 0.994921863079071}
At train
Epoch [0], val_loss: 0.6159, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9962
Epoch [1], val_loss: 0.5727, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9966
Epoch [2], val_loss: 0.5803, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9962
Epoch [3], val_loss: 0.5758, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9970
Epoch [4], val_loss: 0.5747, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9965
Epoch [5], val_loss: 0.5944, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9969
Epoch [6], val_loss: 0.5817, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9969
Epoch [7], val_loss: 0.5765, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9967
Epoch [8], val_loss: 0.5791, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9967
Epoch [9], val_loss: 0.5862, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9968
Epoch [10], val_loss: 0.5929, val_acc: 0.8966, val_top_1: 0.8966, val_top_5: 0.9966
Epoch [11], val_loss: 0.5903, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9959
Epoch [12], val_loss: 0.5933, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9964
Epoch [13], val_loss: 0.5883, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9968
Epoch [14], val_loss: 0.5952, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9966
Epoch [15], val_loss: 0.5955, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9964
Epoch [16], val_loss: 0.5998, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9970
Epoch [17], val_loss: 0.6021, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9960
Epoch [18], val_loss: 0.5997, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9968
Epoch [19], val_loss: 0.7686, val_acc: 0.8804, val_top_1: 0.8804, val_top_5: 0.9948
Epoch [20], val_loss: 0.5991, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9963
Epoch [21], val_loss: 0.6001, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9965
Epoch [22], val_loss: 0.6085, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9963
Epoch [23], val_loss: 0.6341, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9961
Epoch [24], val_loss: 0.6100, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9966
Epoch [25], val_loss: 0.6073, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9964
Epoch [26], val_loss: 0.6115, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9967
Epoch [27], val_loss: 0.6087, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9966
Epoch [28], val_loss: 0.6177, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9967
Epoch [29], val_loss: 0.6161, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9966
Epoch [30], val_loss: 0.6164, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9965
Epoch [31], val_loss: 0.6223, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9959
Epoch [32], val_loss: 0.6209, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9966
Epoch [33], val_loss: 0.6220, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9961
Epoch [34], val_loss: 0.6178, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9962
Compression= 0.7745610927575156 Result after pruning is  {'val_loss': 0.7276341319084167, 'val_acc': 0.88232421875, 'val_top_1': 0.88232421875, 'val_top_5': 0.994335949420929}
Mask compression =  1.8 tensor(0.7824)
After masking, Compression= 0.7809583048417792 Result after pruning is  {'val_loss': 0.8093630075454712, 'val_acc': 0.8760741949081421, 'val_top_1': 0.8760741949081421, 'val_top_5': 0.9921875}
At train
Epoch [0], val_loss: 0.6116, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9964
Epoch [1], val_loss: 0.6527, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9960
Epoch [2], val_loss: 0.6331, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9963
Epoch [3], val_loss: 0.6187, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9965
Epoch [4], val_loss: 0.6258, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9963
Epoch [5], val_loss: 0.6290, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9964
Epoch [6], val_loss: 0.6274, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9964
Epoch [7], val_loss: 0.6240, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9963
Epoch [8], val_loss: 0.6264, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9965
Epoch [9], val_loss: 0.6309, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9966
Epoch [10], val_loss: 0.6315, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9963
Epoch [11], val_loss: 0.6318, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9965
Epoch [12], val_loss: 0.6305, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9964
Epoch [13], val_loss: 0.6368, val_acc: 0.8950, val_top_1: 0.8950, val_top_5: 0.9966
Epoch [14], val_loss: 0.6382, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9963
Epoch [15], val_loss: 0.6345, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9963
Epoch [16], val_loss: 0.6382, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9966
Epoch [17], val_loss: 0.6396, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9963
Epoch [18], val_loss: 0.6416, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9964
Epoch [19], val_loss: 0.6475, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9958
Epoch [20], val_loss: 0.6481, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9963
Epoch [21], val_loss: 0.6399, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9965
Epoch [22], val_loss: 0.6466, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9962
Epoch [23], val_loss: 0.6473, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9966
Epoch [24], val_loss: 0.6479, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9963
Epoch [25], val_loss: 0.6510, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9966
Epoch [26], val_loss: 0.6575, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9964
Epoch [27], val_loss: 0.6525, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9964
Epoch [28], val_loss: 0.6545, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9963
Epoch [29], val_loss: 0.6555, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9963
Epoch [30], val_loss: 0.6563, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9964
Epoch [31], val_loss: 0.6673, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9959
Epoch [32], val_loss: 0.6583, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9964
Epoch [33], val_loss: 0.6589, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9963
Epoch [34], val_loss: 0.6602, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9964
Compression= 0.7809583048417792 Result after pruning is  {'val_loss': 0.7825964093208313, 'val_acc': 0.8807617425918579, 'val_top_1': 0.8807617425918579, 'val_top_5': 0.993847668170929}
Mask compression =  1.85 tensor(0.7879)
After masking, Compression= 0.7864328024979198 Result after pruning is  {'val_loss': 0.820624828338623, 'val_acc': 0.8770507574081421, 'val_top_1': 0.8770507574081421, 'val_top_5': 0.994140625}
At train
Epoch [0], val_loss: 0.6619, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9965
Epoch [1], val_loss: 0.6639, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9966
Epoch [2], val_loss: 0.6664, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9963
Epoch [3], val_loss: 0.6608, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9966
Epoch [4], val_loss: 0.6692, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9966
Epoch [5], val_loss: 0.6651, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9964
Epoch [6], val_loss: 0.6629, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9962
Epoch [7], val_loss: 0.6770, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9957
Epoch [8], val_loss: 0.6712, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9962
Epoch [9], val_loss: 0.6680, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9964
Epoch [10], val_loss: 0.6721, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9966
Epoch [11], val_loss: 0.6698, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9962
Epoch [12], val_loss: 0.6726, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9963
Epoch [13], val_loss: 0.6828, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9962
Epoch [14], val_loss: 0.6720, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9964
Epoch [15], val_loss: 0.6877, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9958
Epoch [16], val_loss: 0.6920, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9963
Epoch [17], val_loss: 0.6782, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9964
Epoch [18], val_loss: 0.6806, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9963
Epoch [19], val_loss: 0.6793, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9961
Epoch [20], val_loss: 0.6891, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9959
Epoch [21], val_loss: 0.6868, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9962
Epoch [22], val_loss: 0.6859, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9962
Epoch [23], val_loss: 0.6852, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9963
Epoch [24], val_loss: 0.6824, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9963
Epoch [25], val_loss: 0.6876, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9962
Epoch [26], val_loss: 0.6907, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9961
Epoch [27], val_loss: 0.6891, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9964
Epoch [28], val_loss: 0.6948, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9959
Epoch [29], val_loss: 0.6914, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9961
Epoch [30], val_loss: 0.6951, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9961
Epoch [31], val_loss: 0.6949, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9965
Epoch [32], val_loss: 0.6998, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9960
Epoch [33], val_loss: 0.6952, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9961
Epoch [34], val_loss: 0.6994, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9963
Compression= 0.7864328024979198 Result after pruning is  {'val_loss': 0.8258647918701172, 'val_acc': 0.880859375, 'val_top_1': 0.880859375, 'val_top_5': 0.9937499761581421}
Mask compression =  1.9 tensor(0.7934)
After masking, Compression= 0.7918702268065019 Result after pruning is  {'val_loss': 0.877253532409668, 'val_acc': 0.879101574420929, 'val_top_1': 0.879101574420929, 'val_top_5': 0.993457019329071}
At train
Epoch [0], val_loss: 0.6743, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9964
Epoch [1], val_loss: 0.6799, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9960
Epoch [2], val_loss: 0.7411, val_acc: 0.8851, val_top_1: 0.8851, val_top_5: 0.9963
Epoch [3], val_loss: 0.6793, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9959
Epoch [4], val_loss: 0.6844, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9963
Epoch [5], val_loss: 0.6817, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9963
Epoch [6], val_loss: 0.6836, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9960
Epoch [7], val_loss: 0.6863, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9962
Epoch [8], val_loss: 0.6908, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9962
Epoch [9], val_loss: 0.6880, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9962
Epoch [10], val_loss: 0.6923, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9961
Epoch [11], val_loss: 0.6867, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9961
Epoch [12], val_loss: 0.6898, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9961
Epoch [13], val_loss: 0.6924, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9964
Epoch [14], val_loss: 0.6917, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9961
Epoch [15], val_loss: 0.6949, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9962
Epoch [16], val_loss: 0.6952, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9962
Epoch [17], val_loss: 0.7018, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9960
Epoch [18], val_loss: 0.6968, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9962
Epoch [19], val_loss: 0.6981, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9958
Epoch [20], val_loss: 0.7010, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9961
Epoch [21], val_loss: 0.7014, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9962
Epoch [22], val_loss: 0.7027, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9959
Epoch [23], val_loss: 0.7041, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9962
Epoch [24], val_loss: 0.7054, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9964
Epoch [25], val_loss: 0.7095, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9963
Epoch [26], val_loss: 0.7047, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9961
Epoch [27], val_loss: 0.7075, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9962
Epoch [28], val_loss: 0.7060, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9962
Epoch [29], val_loss: 0.7096, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9962
Epoch [30], val_loss: 0.7153, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9960
Epoch [31], val_loss: 0.7114, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9961
Epoch [32], val_loss: 0.7182, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9960
Epoch [33], val_loss: 0.7104, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9962
Epoch [34], val_loss: 0.7164, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9962
Compression= 0.7918702268065019 Result after pruning is  {'val_loss': 0.8502093553543091, 'val_acc': 0.88037109375, 'val_top_1': 0.88037109375, 'val_top_5': 0.99365234375}
Mask compression =  1.92 tensor(0.7961)
After masking, Compression= 0.7945724619174336 Result after pruning is  {'val_loss': 0.850826621055603, 'val_acc': 0.8792968988418579, 'val_top_1': 0.8792968988418579, 'val_top_5': 0.9942382574081421}
At train
Epoch [0], val_loss: 0.7220, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9953
Epoch [1], val_loss: 0.7119, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9963
Epoch [2], val_loss: 0.7159, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9957
Epoch [3], val_loss: 0.7168, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9958
Epoch [4], val_loss: 0.7159, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9960
Epoch [5], val_loss: 0.7191, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9960
Epoch [6], val_loss: 0.7212, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9960
Epoch [7], val_loss: 0.7215, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9958
Epoch [8], val_loss: 0.7203, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9961
Epoch [9], val_loss: 0.7225, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9959
Epoch [10], val_loss: 0.7222, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9959
Epoch [11], val_loss: 0.7261, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9958
Epoch [12], val_loss: 0.7237, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9960
Epoch [13], val_loss: 0.7246, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9960
Epoch [14], val_loss: 0.7272, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9961
Epoch [15], val_loss: 0.7269, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9958
Epoch [16], val_loss: 0.7276, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9963
Epoch [17], val_loss: 0.7321, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9958
Epoch [18], val_loss: 0.7302, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9959
Epoch [19], val_loss: 0.7317, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9959
Epoch [20], val_loss: 0.7327, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9958
Epoch [21], val_loss: 0.7317, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9959
Epoch [22], val_loss: 0.7353, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9958
Epoch [23], val_loss: 0.7362, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9962
Epoch [24], val_loss: 0.7355, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9961
Epoch [25], val_loss: 0.7374, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9959
Epoch [26], val_loss: 0.7411, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9958
Epoch [27], val_loss: 0.7396, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9959
Epoch [28], val_loss: 0.7389, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9959
Epoch [29], val_loss: 0.7416, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9960
Epoch [30], val_loss: 0.7390, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9960
Epoch [31], val_loss: 0.7399, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9958
Epoch [32], val_loss: 0.7425, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9959
Epoch [33], val_loss: 0.7418, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9959
Epoch [34], val_loss: 0.7453, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9959
Compression= 0.7945724619174336 Result after pruning is  {'val_loss': 0.8815515637397766, 'val_acc': 0.881054699420929, 'val_top_1': 0.881054699420929, 'val_top_5': 0.993359386920929}
Mask compression =  1.93 tensor(0.7973)
After masking, Compression= 0.7958370749952629 Result after pruning is  {'val_loss': 0.8952773809432983, 'val_acc': 0.880664050579071, 'val_top_1': 0.880664050579071, 'val_top_5': 0.993359386920929}
At train
Epoch [0], val_loss: 0.7416, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9960
Epoch [1], val_loss: 0.7460, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9963
Epoch [2], val_loss: 0.7463, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9963
Epoch [3], val_loss: 0.7470, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9961
Epoch [4], val_loss: 0.7517, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9955
Epoch [5], val_loss: 0.7498, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9958
Epoch [6], val_loss: 0.7524, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9958
Epoch [7], val_loss: 0.7500, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9958
Epoch [8], val_loss: 0.7520, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9957
Epoch [9], val_loss: 0.7524, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9962
Epoch [10], val_loss: 0.7552, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9956
Epoch [11], val_loss: 0.7507, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9962
Epoch [12], val_loss: 0.7530, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9958
Epoch [13], val_loss: 0.7554, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9958
Epoch [14], val_loss: 0.7549, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9957
Epoch [15], val_loss: 0.7567, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9956
Epoch [16], val_loss: 0.7591, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9957
Epoch [17], val_loss: 0.7556, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9959
Epoch [18], val_loss: 0.7601, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9956
Epoch [19], val_loss: 0.7591, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9958
Epoch [20], val_loss: 0.7604, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9959
Epoch [21], val_loss: 0.7609, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9958
Epoch [22], val_loss: 0.7622, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9958
Epoch [23], val_loss: 0.7605, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9959
Epoch [24], val_loss: 0.7635, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9958
Epoch [25], val_loss: 0.7652, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9956
Epoch [26], val_loss: 0.7639, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9956
Epoch [27], val_loss: 0.7647, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9957
Epoch [28], val_loss: 0.7633, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9958
Epoch [29], val_loss: 0.7673, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9958
Epoch [30], val_loss: 0.7676, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9957
Epoch [31], val_loss: 0.7668, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9960
Epoch [32], val_loss: 0.7684, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9956
Epoch [33], val_loss: 0.7717, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9959
Epoch [34], val_loss: 0.7693, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9957
Compression= 0.7958370749952629 Result after pruning is  {'val_loss': 0.911446750164032, 'val_acc': 0.881054699420929, 'val_top_1': 0.881054699420929, 'val_top_5': 0.9937499761581421}
Mask compression =  1.95 tensor(0.7989)
After masking, Compression= 0.7974188711577594 Result after pruning is  {'val_loss': 0.9094796180725098, 'val_acc': 0.8827148675918579, 'val_top_1': 0.8827148675918579, 'val_top_5': 0.993359386920929}
At train
Epoch [0], val_loss: 0.7690, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9958
Epoch [1], val_loss: 0.7712, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9956
Epoch [2], val_loss: 0.7789, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9957
Epoch [3], val_loss: 0.7756, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9954
Epoch [4], val_loss: 0.7727, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9955
Epoch [5], val_loss: 0.7761, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9956
Epoch [6], val_loss: 0.7740, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9956
Epoch [7], val_loss: 0.7761, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9958
Epoch [8], val_loss: 0.7781, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9955
Epoch [9], val_loss: 0.7771, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9956
Epoch [10], val_loss: 0.7776, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9955
Epoch [11], val_loss: 0.7789, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9954
Epoch [12], val_loss: 0.7793, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9955
Epoch [13], val_loss: 0.7789, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9955
Epoch [14], val_loss: 0.7820, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9954
Epoch [15], val_loss: 0.7788, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9958
Epoch [16], val_loss: 0.7816, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9955
Epoch [17], val_loss: 0.7816, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9955
Epoch [18], val_loss: 0.7808, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9956
Epoch [19], val_loss: 0.7880, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9954
Epoch [20], val_loss: 0.7839, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9954
Epoch [21], val_loss: 0.7871, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9956
Epoch [22], val_loss: 0.7844, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9956
Epoch [23], val_loss: 0.7848, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9955
Epoch [24], val_loss: 0.7874, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9956
Epoch [25], val_loss: 0.7861, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9958
Epoch [26], val_loss: 0.7871, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9955
Epoch [27], val_loss: 0.7883, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9955
Epoch [28], val_loss: 0.7890, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9956
Epoch [29], val_loss: 0.7888, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9958
Epoch [30], val_loss: 0.7899, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9955
Epoch [31], val_loss: 0.7899, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9955
Epoch [32], val_loss: 0.7893, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9957
Epoch [33], val_loss: 0.7951, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9955
Epoch [34], val_loss: 0.7926, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9955
Compression= 0.7974188711577594 Result after pruning is  {'val_loss': 0.937314510345459, 'val_acc': 0.8809570074081421, 'val_top_1': 0.8809570074081421, 'val_top_5': 0.993457019329071}
Mask compression =  1.99 tensor(0.8022)
After masking, Compression= 0.800697802786268 Result after pruning is  {'val_loss': 0.9502919316291809, 'val_acc': 0.8833984136581421, 'val_top_1': 0.8833984136581421, 'val_top_5': 0.9930664300918579}
At train
Epoch [0], val_loss: 0.7923, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9955
Epoch [1], val_loss: 0.7928, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9955
Epoch [2], val_loss: 0.7908, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9956
Epoch [3], val_loss: 0.7940, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9955
Epoch [4], val_loss: 0.7915, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9956
Epoch [5], val_loss: 0.7943, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9956
Epoch [6], val_loss: 0.7938, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9954
Epoch [7], val_loss: 0.7959, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9954
Epoch [8], val_loss: 0.7982, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9955
Epoch [9], val_loss: 0.7970, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9955
Epoch [10], val_loss: 0.7967, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9956
Epoch [11], val_loss: 0.7976, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9957
Epoch [12], val_loss: 0.7984, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9955
Epoch [13], val_loss: 0.7997, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9956
Epoch [14], val_loss: 0.7986, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9956
Epoch [15], val_loss: 0.7999, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9955
Epoch [16], val_loss: 0.8031, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9956
Epoch [17], val_loss: 0.7994, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9956
Epoch [18], val_loss: 0.7994, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9955
Epoch [19], val_loss: 0.8023, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9955
Epoch [20], val_loss: 0.8006, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9957
Epoch [21], val_loss: 0.8012, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9957
Epoch [22], val_loss: 0.8073, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9957
Epoch [23], val_loss: 0.8057, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9955
Epoch [24], val_loss: 0.8058, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9954
Epoch [25], val_loss: 0.8027, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9956
Epoch [26], val_loss: 0.8045, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9955
Epoch [27], val_loss: 0.8047, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9956
Epoch [28], val_loss: 0.8072, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9954
Epoch [29], val_loss: 0.8073, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9955
Epoch [30], val_loss: 0.8085, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9954
Epoch [31], val_loss: 0.8085, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9955
Epoch [32], val_loss: 0.8080, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9956
Epoch [33], val_loss: 0.8097, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9956
Epoch [34], val_loss: 0.8088, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9955
Compression= 0.800697802786268 Result after pruning is  {'val_loss': 0.9548107981681824, 'val_acc': 0.882031261920929, 'val_top_1': 0.882031261920929, 'val_top_5': 0.9932616949081421}
Mask compression =  2.1 tensor(0.8134)
After masking, Compression= 0.811823926314662 Result after pruning is  {'val_loss': 1.0393197536468506, 'val_acc': 0.869921863079071, 'val_top_1': 0.869921863079071, 'val_top_5': 0.9930664300918579}
At train
Epoch [0], val_loss: 0.7729, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9954
Epoch [1], val_loss: 0.7372, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9956
Epoch [2], val_loss: 0.7372, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9957
Epoch [3], val_loss: 0.7500, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9956
Epoch [4], val_loss: 0.7748, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9951
Epoch [5], val_loss: 0.7512, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9956
Epoch [6], val_loss: 0.7513, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9958
Epoch [7], val_loss: 0.7545, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9958
Epoch [8], val_loss: 0.7521, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9957
Epoch [9], val_loss: 0.7540, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9958
Epoch [10], val_loss: 0.7589, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9955
Epoch [11], val_loss: 0.7588, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9958
Epoch [12], val_loss: 0.7581, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9958
Epoch [13], val_loss: 0.7629, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9956
Epoch [14], val_loss: 0.7623, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9954
Epoch [15], val_loss: 0.7630, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9957
Epoch [16], val_loss: 0.7650, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9957
Epoch [17], val_loss: 0.7632, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9957
Epoch [18], val_loss: 0.7668, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9958
Epoch [19], val_loss: 0.7702, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9958
Epoch [20], val_loss: 0.7726, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9955
Epoch [21], val_loss: 0.7722, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9956
Epoch [22], val_loss: 0.7706, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9958
Epoch [23], val_loss: 0.7729, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9958
Epoch [24], val_loss: 0.7716, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9958
Epoch [25], val_loss: 0.7736, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9958
Epoch [26], val_loss: 0.7730, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9959
Epoch [27], val_loss: 0.7786, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9956
Epoch [28], val_loss: 0.7761, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9956
Epoch [29], val_loss: 0.7789, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9955
Epoch [30], val_loss: 0.7793, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9956
Epoch [31], val_loss: 0.7809, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9956
Epoch [32], val_loss: 0.7788, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9956
Epoch [33], val_loss: 0.7816, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9960
Epoch [34], val_loss: 0.7818, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9957
Compression= 0.811823926314662 Result after pruning is  {'val_loss': 0.918309211730957, 'val_acc': 0.8814452886581421, 'val_top_1': 0.8814452886581421, 'val_top_5': 0.9927734136581421}
Mask compression =  2.12 tensor(0.8173)
After masking, Compression= 0.8157454626341849 Result after pruning is  {'val_loss': 0.9852597117424011, 'val_acc': 0.8741210699081421, 'val_top_1': 0.8741210699081421, 'val_top_5': 0.9932616949081421}
At train
Epoch [0], val_loss: 0.8251, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9957
Epoch [1], val_loss: 0.7523, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9957
Epoch [2], val_loss: 0.7489, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9957
Epoch [3], val_loss: 0.7501, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9957
Epoch [4], val_loss: 0.7559, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9959
Epoch [5], val_loss: 0.7606, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9956
Epoch [6], val_loss: 0.7555, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9958
Epoch [7], val_loss: 0.7604, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9958
Epoch [8], val_loss: 0.7594, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9958
Epoch [9], val_loss: 0.7647, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9958
Epoch [10], val_loss: 0.7642, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9958
Epoch [11], val_loss: 0.7666, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9959
Epoch [12], val_loss: 0.7658, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9957
Epoch [13], val_loss: 0.7672, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9959
Epoch [14], val_loss: 0.7702, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9958
Epoch [15], val_loss: 0.7724, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9957
Epoch [16], val_loss: 0.7695, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9958
Epoch [17], val_loss: 0.7724, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9958
Epoch [18], val_loss: 0.7756, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9958
Epoch [19], val_loss: 0.7761, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9958
Epoch [20], val_loss: 0.7739, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9959
Epoch [21], val_loss: 0.7762, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9958
Epoch [22], val_loss: 0.7750, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9958
Epoch [23], val_loss: 0.7772, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9958
Epoch [24], val_loss: 0.7794, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9957
Epoch [25], val_loss: 0.7802, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9957
Epoch [26], val_loss: 0.7804, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9956
Epoch [27], val_loss: 0.7807, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9957
Epoch [28], val_loss: 0.7792, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9957
Epoch [29], val_loss: 0.7828, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9956
Epoch [30], val_loss: 0.7852, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9954
Epoch [31], val_loss: 0.7834, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9955
Epoch [32], val_loss: 0.7836, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9957
Epoch [33], val_loss: 0.7865, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9956
Epoch [34], val_loss: 0.7878, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9957
Compression= 0.8157454626341849 Result after pruning is  {'val_loss': 0.9320432543754578, 'val_acc': 0.881640613079071, 'val_top_1': 0.881640613079071, 'val_top_5': 0.992968738079071}
Mask compression =  2.2 tensor(0.8236)
After masking, Compression= 0.8220685280233315 Result after pruning is  {'val_loss': 0.9844657182693481, 'val_acc': 0.8749023675918579, 'val_top_1': 0.8749023675918579, 'val_top_5': 0.992871105670929}
At train
Epoch [0], val_loss: 0.7521, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9950
Epoch [1], val_loss: 0.7582, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9955
Epoch [2], val_loss: 0.7617, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9953
Epoch [3], val_loss: 0.7640, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9956
Epoch [4], val_loss: 0.7620, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9955
Epoch [5], val_loss: 0.7655, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9955
Epoch [6], val_loss: 0.7674, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9955
Epoch [7], val_loss: 0.7744, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9951
Epoch [8], val_loss: 0.7688, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9954
Epoch [9], val_loss: 0.7715, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9956
Epoch [10], val_loss: 0.7706, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9954
Epoch [11], val_loss: 0.7735, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9954
Epoch [12], val_loss: 0.7735, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9954
Epoch [13], val_loss: 0.7761, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9955
Epoch [14], val_loss: 0.7767, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9954
Epoch [15], val_loss: 0.7819, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9952
Epoch [16], val_loss: 0.7791, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9953
Epoch [17], val_loss: 0.7800, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9954
Epoch [18], val_loss: 0.7772, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9954
Epoch [19], val_loss: 0.7794, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9953
Epoch [20], val_loss: 0.7800, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9955
Epoch [21], val_loss: 0.7814, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9953
Epoch [22], val_loss: 0.7807, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9953
Epoch [23], val_loss: 0.7810, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9953
Epoch [24], val_loss: 0.7824, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9953
Epoch [25], val_loss: 0.7851, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9952
Epoch [26], val_loss: 0.7859, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9953
Epoch [27], val_loss: 0.7824, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9954
Epoch [28], val_loss: 0.7856, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9954
Epoch [29], val_loss: 0.7874, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9951
Epoch [30], val_loss: 0.7864, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9952
Epoch [31], val_loss: 0.7883, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9953
Epoch [32], val_loss: 0.7897, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9951
Epoch [33], val_loss: 0.7878, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9951
Epoch [34], val_loss: 0.7915, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9952
Compression= 0.8220685280233315 Result after pruning is  {'val_loss': 0.9278858304023743, 'val_acc': 0.883496105670929, 'val_top_1': 0.883496105670929, 'val_top_5': 0.9930664300918579}
Mask compression =  2.25 tensor(0.8285)
After masking, Compression= 0.8269374943360164 Result after pruning is  {'val_loss': 0.9623771905899048, 'val_acc': 0.876757800579071, 'val_top_1': 0.876757800579071, 'val_top_5': 0.9935547113418579}
At train
Epoch [0], val_loss: 0.7734, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9952
Epoch [1], val_loss: 0.7655, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9950
Epoch [2], val_loss: 0.7704, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9950
Epoch [3], val_loss: 0.7702, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9949
Epoch [4], val_loss: 0.7715, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9950
Epoch [5], val_loss: 0.7735, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9951
Epoch [6], val_loss: 0.7737, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9953
Epoch [7], val_loss: 0.7724, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9953
Epoch [8], val_loss: 0.7764, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9951
Epoch [9], val_loss: 0.7783, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9952
Epoch [10], val_loss: 0.7802, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9952
Epoch [11], val_loss: 0.7793, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9952
Epoch [12], val_loss: 0.7790, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9954
Epoch [13], val_loss: 0.7813, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9953
Epoch [14], val_loss: 0.7815, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9953
Epoch [15], val_loss: 0.7871, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9953
Epoch [16], val_loss: 0.7828, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9953
Epoch [17], val_loss: 0.7833, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9951
Epoch [18], val_loss: 0.7840, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9954
Epoch [19], val_loss: 0.7886, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9954
Epoch [20], val_loss: 0.7875, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9953
Epoch [21], val_loss: 0.7880, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9952
Epoch [22], val_loss: 0.7912, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9952
Epoch [23], val_loss: 0.7912, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9953
Epoch [24], val_loss: 0.7914, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9952
Epoch [25], val_loss: 0.7933, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9955
Epoch [26], val_loss: 0.7928, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9953
Epoch [27], val_loss: 0.7917, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9954
Epoch [28], val_loss: 0.7939, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9953
Epoch [29], val_loss: 0.7966, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9953
Epoch [30], val_loss: 0.7985, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9953
Epoch [31], val_loss: 0.7961, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9954
Epoch [32], val_loss: 0.7978, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9953
Epoch [33], val_loss: 0.7972, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9953
Epoch [34], val_loss: 0.7970, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9953
Compression= 0.8269374943360164 Result after pruning is  {'val_loss': 0.9326993227005005, 'val_acc': 0.8836914300918579, 'val_top_1': 0.8836914300918579, 'val_top_5': 0.992871105670929}
Mask compression =  2.3 tensor(0.8331)
After masking, Compression= 0.8315716627808306 Result after pruning is  {'val_loss': 1.0237185955047607, 'val_acc': 0.87158203125, 'val_top_1': 0.87158203125, 'val_top_5': 0.990917980670929}
At train
Epoch [0], val_loss: 0.7579, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9952
Epoch [1], val_loss: 0.7641, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9948
Epoch [2], val_loss: 0.7647, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9948
Epoch [3], val_loss: 0.7636, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9950
Epoch [4], val_loss: 0.7736, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9951
Epoch [5], val_loss: 0.7709, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9949
Epoch [6], val_loss: 0.7691, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9952
Epoch [7], val_loss: 0.7725, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9952
Epoch [8], val_loss: 0.7734, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9951
Epoch [9], val_loss: 0.7759, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9949
Epoch [10], val_loss: 0.7768, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9951
Epoch [11], val_loss: 0.7798, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9951
Epoch [12], val_loss: 0.7763, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9951
Epoch [13], val_loss: 0.7773, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9951
Epoch [14], val_loss: 0.7783, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9952
Epoch [15], val_loss: 0.7825, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9951
Epoch [16], val_loss: 0.7823, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9952
Epoch [17], val_loss: 0.7831, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9952
Epoch [18], val_loss: 0.7830, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9951
Epoch [19], val_loss: 0.7876, val_acc: 0.8946, val_top_1: 0.8946, val_top_5: 0.9950
Epoch [20], val_loss: 0.7832, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9951
Epoch [21], val_loss: 0.7870, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9951
Epoch [22], val_loss: 0.7881, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9952
Epoch [23], val_loss: 0.7858, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9951
Epoch [24], val_loss: 0.7872, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9951
Epoch [25], val_loss: 0.7906, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9952
Epoch [26], val_loss: 0.7900, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9952
Epoch [27], val_loss: 0.7913, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9953
Epoch [28], val_loss: 0.7903, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9952
Epoch [29], val_loss: 0.7941, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9952
Epoch [30], val_loss: 0.7942, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9952
Epoch [31], val_loss: 0.7931, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9952
Epoch [32], val_loss: 0.7971, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9952
Epoch [33], val_loss: 0.7945, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9952
Epoch [34], val_loss: 0.7974, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9952
Compression= 0.8315716627808306 Result after pruning is  {'val_loss': 0.934141993522644, 'val_acc': 0.88232421875, 'val_top_1': 0.88232421875, 'val_top_5': 0.9925781488418579}
Mask compression =  2.35 tensor(0.8374)
After masking, Compression= 0.8358474555325792 Result after pruning is  {'val_loss': 1.037584662437439, 'val_acc': 0.869824230670929, 'val_top_1': 0.869824230670929, 'val_top_5': 0.9927734136581421}
At train
Epoch [0], val_loss: 1.3360, val_acc: 0.8463, val_top_1: 0.8463, val_top_5: 0.9943
Epoch [1], val_loss: 0.7737, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9953
Epoch [2], val_loss: 0.7717, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9950
Epoch [3], val_loss: 0.7760, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9950
Epoch [4], val_loss: 0.7726, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9950
Epoch [5], val_loss: 0.7768, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9950
Epoch [6], val_loss: 0.7780, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9954
Epoch [7], val_loss: 0.7803, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9951
Epoch [8], val_loss: 0.7809, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9951
Epoch [9], val_loss: 0.7785, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9952
Epoch [10], val_loss: 0.7805, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9950
Epoch [11], val_loss: 0.7815, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9951
Epoch [12], val_loss: 0.7824, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9950
Epoch [13], val_loss: 0.7825, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9951
Epoch [14], val_loss: 0.7853, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9951
Epoch [15], val_loss: 0.7825, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9952
Epoch [16], val_loss: 0.7871, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9951
Epoch [17], val_loss: 0.7862, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9950
Epoch [18], val_loss: 0.7885, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9951
Epoch [19], val_loss: 0.7900, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9951
Epoch [20], val_loss: 0.7897, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9950
Epoch [21], val_loss: 0.7907, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9950
Epoch [22], val_loss: 0.7909, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9950
Epoch [23], val_loss: 0.7883, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9950
Epoch [24], val_loss: 0.7923, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9951
Epoch [25], val_loss: 0.7946, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9949
Epoch [26], val_loss: 0.7943, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9950
Epoch [27], val_loss: 0.7959, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9951
Epoch [28], val_loss: 0.7965, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9950
Epoch [29], val_loss: 0.7968, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9950
Epoch [30], val_loss: 0.7953, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9952
Epoch [31], val_loss: 0.7975, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9950
Epoch [32], val_loss: 0.7996, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9950
Epoch [33], val_loss: 0.7983, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9950
Epoch [34], val_loss: 0.8040, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9950
Compression= 0.8358474555325792 Result after pruning is  {'val_loss': 0.9434018135070801, 'val_acc': 0.8827148675918579, 'val_top_1': 0.8827148675918579, 'val_top_5': 0.992480456829071}
Mask compression =  2.4 tensor(0.8414)
After masking, Compression= 0.8398472578080589 Result after pruning is  {'val_loss': 1.0548063516616821, 'val_acc': 0.8768554925918579, 'val_top_1': 0.8768554925918579, 'val_top_5': 0.991894543170929}
At train
Epoch [0], val_loss: 0.7936, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9953
Epoch [1], val_loss: 0.7958, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9954
Epoch [2], val_loss: 0.7945, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9952
Epoch [3], val_loss: 0.7946, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9949
Epoch [4], val_loss: 0.8000, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9951
Epoch [5], val_loss: 0.7962, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9952
Epoch [6], val_loss: 0.7956, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9953
Epoch [7], val_loss: 0.7991, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9955
Epoch [8], val_loss: 0.7999, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9952
Epoch [9], val_loss: 0.8003, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9953
Epoch [10], val_loss: 0.8007, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9949
Epoch [11], val_loss: 0.8027, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9951
Epoch [12], val_loss: 0.8005, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9951
Epoch [13], val_loss: 0.8020, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9952
Epoch [14], val_loss: 0.8021, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9950
Epoch [15], val_loss: 0.8037, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9950
Epoch [16], val_loss: 0.8044, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9950
Epoch [17], val_loss: 0.8044, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9950
Epoch [18], val_loss: 0.8052, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9950
Epoch [19], val_loss: 0.8071, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9951
Epoch [20], val_loss: 0.8075, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9950
Epoch [21], val_loss: 0.8068, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9949
Epoch [22], val_loss: 0.8089, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9951
Epoch [23], val_loss: 0.8104, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9951
Epoch [24], val_loss: 0.8089, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9951
Epoch [25], val_loss: 0.8135, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9950
Epoch [26], val_loss: 0.8111, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9950
Epoch [27], val_loss: 0.8105, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9950
Epoch [28], val_loss: 0.8131, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9951
Epoch [29], val_loss: 0.8137, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9949
Epoch [30], val_loss: 0.8141, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9950
Epoch [31], val_loss: 0.8153, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9949
Epoch [32], val_loss: 0.8154, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9950
Epoch [33], val_loss: 0.8152, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9949
Epoch [34], val_loss: 0.8171, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9951
Compression= 0.8398472578080589 Result after pruning is  {'val_loss': 0.9552738070487976, 'val_acc': 0.88330078125, 'val_top_1': 0.88330078125, 'val_top_5': 0.9930664300918579}
Mask compression =  2.5 tensor(0.8491)
After masking, Compression= 0.8475008444484722 Result after pruning is  {'val_loss': 1.2437245845794678, 'val_acc': 0.8544921875, 'val_top_1': 0.8544921875, 'val_top_5': 0.991503894329071}
At train
Epoch [0], val_loss: 0.7848, val_acc: 0.8871, val_top_1: 0.8871, val_top_5: 0.9953
Epoch [1], val_loss: 0.7853, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9949
Epoch [2], val_loss: 0.7828, val_acc: 0.8909, val_top_1: 0.8909, val_top_5: 0.9952
Epoch [3], val_loss: 0.7852, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9950
Epoch [4], val_loss: 0.7873, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9950
Epoch [5], val_loss: 0.7886, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9950
Epoch [6], val_loss: 0.7895, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9950
Epoch [7], val_loss: 0.7914, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9952
Epoch [8], val_loss: 0.7896, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9949
Epoch [9], val_loss: 0.7924, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9951
Epoch [10], val_loss: 0.7913, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9948
Epoch [11], val_loss: 0.7953, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9951
Epoch [12], val_loss: 0.7930, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9950
Epoch [13], val_loss: 0.7925, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9950
Epoch [14], val_loss: 0.8007, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9951
Epoch [15], val_loss: 0.7973, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9948
Epoch [16], val_loss: 0.7958, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9949
Epoch [17], val_loss: 0.7965, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9949
Epoch [18], val_loss: 0.7967, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9948
Epoch [19], val_loss: 0.7986, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9949
Epoch [20], val_loss: 0.8009, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9949
Epoch [21], val_loss: 0.7987, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9949
Epoch [22], val_loss: 0.8017, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9949
Epoch [23], val_loss: 0.8021, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9950
Epoch [24], val_loss: 0.8027, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9949
Epoch [25], val_loss: 0.8032, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9950
Epoch [26], val_loss: 0.8052, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9948
Epoch [27], val_loss: 0.8052, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9950
Epoch [28], val_loss: 0.8043, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9949
Epoch [29], val_loss: 0.8087, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9951
Epoch [30], val_loss: 0.8060, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9949
Epoch [31], val_loss: 0.8082, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9951
Epoch [32], val_loss: 0.8072, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9950
Epoch [33], val_loss: 0.8091, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9949
Epoch [34], val_loss: 0.8109, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9951
Compression= 0.8475008444484722 Result after pruning is  {'val_loss': 0.9391568303108215, 'val_acc': 0.88134765625, 'val_top_1': 0.88134765625, 'val_top_5': 0.993457019329071}
Mask compression =  2.7 tensor(0.8658)
After masking, Compression= 0.8641508967630849 Result after pruning is  {'val_loss': 1.7176692485809326, 'val_acc': 0.81201171875, 'val_top_1': 0.81201171875, 'val_top_5': 0.990527331829071}
At train
Epoch [0], val_loss: 0.7415, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9945
Epoch [1], val_loss: 0.7394, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9947
Epoch [2], val_loss: 0.7494, val_acc: 0.8861, val_top_1: 0.8861, val_top_5: 0.9947
Epoch [3], val_loss: 0.7390, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9947
Epoch [4], val_loss: 0.7364, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9948
Epoch [5], val_loss: 0.7423, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9948
Epoch [6], val_loss: 0.7415, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9949
Epoch [7], val_loss: 0.7442, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9949
Epoch [8], val_loss: 0.7443, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9948
Epoch [9], val_loss: 0.7486, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9950
Epoch [10], val_loss: 0.7474, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9947
Epoch [11], val_loss: 0.7572, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9945
Epoch [12], val_loss: 0.7528, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9947
Epoch [13], val_loss: 0.7555, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9950
Epoch [14], val_loss: 0.7501, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9949
Epoch [15], val_loss: 0.7564, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9949
Epoch [16], val_loss: 0.7552, val_acc: 0.8899, val_top_1: 0.8899, val_top_5: 0.9949
Epoch [17], val_loss: 0.7602, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9948
Epoch [18], val_loss: 0.7579, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9949
Epoch [19], val_loss: 0.7587, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9948
Epoch [20], val_loss: 0.7567, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9950
Epoch [21], val_loss: 0.7583, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9948
Epoch [22], val_loss: 0.7624, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9950
Epoch [23], val_loss: 0.7635, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9949
Epoch [24], val_loss: 0.7642, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9950
Epoch [25], val_loss: 0.7653, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9950
Epoch [26], val_loss: 0.7658, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9949
Epoch [27], val_loss: 0.7686, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9950
Epoch [28], val_loss: 0.7722, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9948
Epoch [29], val_loss: 0.7696, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9951
Epoch [30], val_loss: 0.7725, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9949
Epoch [31], val_loss: 0.7747, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9948
Epoch [32], val_loss: 0.7725, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9949
Epoch [33], val_loss: 0.7729, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9948
Epoch [34], val_loss: 0.7751, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9950
Compression= 0.8641508967630849 Result after pruning is  {'val_loss': 0.8941099047660828, 'val_acc': 0.881054699420929, 'val_top_1': 0.881054699420929, 'val_top_5': 0.9925781488418579}
Mask compression =  2.9 tensor(0.8806)
After masking, Compression= 0.8789060890913735 Result after pruning is  {'val_loss': 2.894517421722412, 'val_acc': 0.7098633050918579, 'val_top_1': 0.7098633050918579, 'val_top_5': 0.9815429449081421}
At train
Epoch [0], val_loss: 0.7009, val_acc: 0.8882, val_top_1: 0.8882, val_top_5: 0.9937
Epoch [1], val_loss: 0.6969, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9932
Epoch [2], val_loss: 0.8645, val_acc: 0.8707, val_top_1: 0.8707, val_top_5: 0.9921
Epoch [3], val_loss: 0.6938, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9934
Epoch [4], val_loss: 0.7070, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9938
Epoch [5], val_loss: 0.6860, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9938
Epoch [6], val_loss: 0.6978, val_acc: 0.8875, val_top_1: 0.8875, val_top_5: 0.9937
Epoch [7], val_loss: 0.7031, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9941
Epoch [8], val_loss: 0.6964, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9937
Epoch [9], val_loss: 0.7024, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9942
Epoch [10], val_loss: 0.7018, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9940
Epoch [11], val_loss: 0.6992, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9941
Epoch [12], val_loss: 0.7063, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9940
Epoch [13], val_loss: 0.7023, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9937
Epoch [14], val_loss: 0.7054, val_acc: 0.8881, val_top_1: 0.8881, val_top_5: 0.9941
Epoch [15], val_loss: 0.7124, val_acc: 0.8879, val_top_1: 0.8879, val_top_5: 0.9940
Epoch [16], val_loss: 0.7190, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9942
Epoch [17], val_loss: 0.7111, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9940
Epoch [18], val_loss: 0.7144, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9938
Epoch [19], val_loss: 0.7166, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9937
Epoch [20], val_loss: 0.7204, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9941
Epoch [21], val_loss: 0.7197, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9941
Epoch [22], val_loss: 0.7180, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9943
Epoch [23], val_loss: 0.7215, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9940
Epoch [24], val_loss: 0.7240, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9940
Epoch [25], val_loss: 0.7283, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9942
Epoch [26], val_loss: 0.7350, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9941
Epoch [27], val_loss: 0.7234, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9943
Epoch [28], val_loss: 0.7283, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9940
Epoch [29], val_loss: 0.7332, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9942
Epoch [30], val_loss: 0.7410, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9942
Epoch [31], val_loss: 0.7307, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9942
Epoch [32], val_loss: 0.7326, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9943
Epoch [33], val_loss: 0.7343, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9943
Epoch [34], val_loss: 0.7340, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9942
Compression= 0.8789060890913735 Result after pruning is  {'val_loss': 0.843891441822052, 'val_acc': 0.880859375, 'val_top_1': 0.880859375, 'val_top_5': 0.9913085699081421}
Mask compression =  4 tensor(0.9400)
After masking, Compression= 0.9382110874024765 Result after pruning is  {'val_loss': 4.491276741027832, 'val_acc': 0.41455078125, 'val_top_1': 0.41455078125, 'val_top_5': 0.949511706829071}
At train
Epoch [0], val_loss: 0.5213, val_acc: 0.8646, val_top_1: 0.8646, val_top_5: 0.9851
Epoch [1], val_loss: 0.4702, val_acc: 0.8725, val_top_1: 0.8725, val_top_5: 0.9879
Epoch [2], val_loss: 0.4551, val_acc: 0.8769, val_top_1: 0.8769, val_top_5: 0.9884
Epoch [3], val_loss: 0.4431, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9893
Epoch [4], val_loss: 0.4496, val_acc: 0.8749, val_top_1: 0.8749, val_top_5: 0.9895
Epoch [5], val_loss: 0.4445, val_acc: 0.8842, val_top_1: 0.8842, val_top_5: 0.9890
Epoch [6], val_loss: 0.4380, val_acc: 0.8836, val_top_1: 0.8836, val_top_5: 0.9895
Epoch [7], val_loss: 0.4322, val_acc: 0.8802, val_top_1: 0.8802, val_top_5: 0.9889
Epoch [8], val_loss: 0.4248, val_acc: 0.8816, val_top_1: 0.8816, val_top_5: 0.9906
Epoch [9], val_loss: 0.4259, val_acc: 0.8848, val_top_1: 0.8848, val_top_5: 0.9895
Epoch [10], val_loss: 0.4224, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9897
Epoch [11], val_loss: 0.4614, val_acc: 0.8722, val_top_1: 0.8722, val_top_5: 0.9921
Epoch [12], val_loss: 0.4216, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9904
Epoch [13], val_loss: 0.4363, val_acc: 0.8869, val_top_1: 0.8869, val_top_5: 0.9895
Epoch [14], val_loss: 0.4221, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9910
Epoch [15], val_loss: 0.4296, val_acc: 0.8824, val_top_1: 0.8824, val_top_5: 0.9911
Epoch [16], val_loss: 0.4296, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9916
Epoch [17], val_loss: 0.4244, val_acc: 0.8864, val_top_1: 0.8864, val_top_5: 0.9908
Epoch [18], val_loss: 0.4332, val_acc: 0.8861, val_top_1: 0.8861, val_top_5: 0.9908
Epoch [19], val_loss: 0.4239, val_acc: 0.8838, val_top_1: 0.8838, val_top_5: 0.9910
Epoch [20], val_loss: 0.4340, val_acc: 0.8838, val_top_1: 0.8838, val_top_5: 0.9901
Epoch [21], val_loss: 0.4343, val_acc: 0.8869, val_top_1: 0.8869, val_top_5: 0.9913
Epoch [22], val_loss: 0.4464, val_acc: 0.8811, val_top_1: 0.8811, val_top_5: 0.9919
Epoch [23], val_loss: 0.4380, val_acc: 0.8841, val_top_1: 0.8841, val_top_5: 0.9912
Epoch [24], val_loss: 0.4373, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9906
Epoch [25], val_loss: 0.4496, val_acc: 0.8862, val_top_1: 0.8862, val_top_5: 0.9922
Epoch [26], val_loss: 0.4583, val_acc: 0.8842, val_top_1: 0.8842, val_top_5: 0.9920
Epoch [27], val_loss: 0.4438, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9927
Epoch [28], val_loss: 0.4419, val_acc: 0.8816, val_top_1: 0.8816, val_top_5: 0.9925
Epoch [29], val_loss: 0.4410, val_acc: 0.8855, val_top_1: 0.8855, val_top_5: 0.9926
Epoch [30], val_loss: 0.4716, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9921
Epoch [31], val_loss: 0.4513, val_acc: 0.8861, val_top_1: 0.8861, val_top_5: 0.9923
Epoch [32], val_loss: 0.4489, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9925
Epoch [33], val_loss: 0.4542, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9926
Epoch [34], val_loss: 0.4650, val_acc: 0.8836, val_top_1: 0.8836, val_top_5: 0.9934
Compression= 0.9382110874024765 Result after pruning is  {'val_loss': 0.5297672748565674, 'val_acc': 0.8736327886581421, 'val_top_1': 0.8736327886581421, 'val_top_5': 0.991406261920929}
   prune_rate  compression  epochs     top_5     top_1
0         0.1     0.061381      35  0.996582  0.875586
1         0.2     0.139070      35  0.996582  0.880469
2         0.3     0.213530      35  0.997168  0.883691
3         0.4     0.282058      35  0.996484  0.880566
4         0.6     0.385303      35  0.997363  0.879199
