{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "    \n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def countDiffMasks(mask1,mask2):\n",
    "    total_diff=0\n",
    "    for i in range(len(mask1)):\n",
    "        m_1=mask1[i].flatten()\n",
    "        m_2=mask2[i].flatten()\n",
    "        count_same=(m_1 == m_2).sum()\n",
    "        count_different=m_1.flatten().shape[0]-count_same\n",
    "        total_diff+=count_different\n",
    "    return total_diff\n",
    "\n",
    "\n",
    "def get_mask_compression(mask_whole_model):\n",
    "    num_total=0\n",
    "    num_non_zeros=0\n",
    "    for mask_each_layer in mask_whole_model:\n",
    "        num_total+=torch.numel(mask_each_layer)\n",
    "        num_non_zeros+=torch.count_nonzero(mask_each_layer)\n",
    "        \n",
    "    return (num_total-num_non_zeros)/num_total\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def prune_model_get_mask(model,prune_rate):\n",
    "    '''\n",
    "    works purely on the model to get\n",
    "    mask\n",
    "    '''\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.std(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "            threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    " \n",
    "    \n",
    "def get_thresholds_each_layer(model,prune_rate):\n",
    "    thresholds_per_layer=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "            abs_std=torch.std(torch.abs(params.data))\n",
    "            threshold=abs_std*prune_rate\n",
    "            thresholds_per_layer.append(threshold)\n",
    "    return thresholds_per_layer\n",
    "    \n",
    "                \n",
    "def apply_mask_model(model,list_mask_whole_model):\n",
    "    mask_layer_count=0\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=list_mask_whole_model[mask_layer_count]\n",
    "            with torch.no_grad():\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#                 print(\"Devices are \",params.device,mask_layer.device)\n",
    "                mask_layer=mask_layer.to(device)\n",
    "    \n",
    "                params.data=params.data*mask_layer            \n",
    "            mask_layer_count+=1\n",
    "    \n",
    "\n",
    "def store_weights_in_dic(weight_description,model):\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            if nm not in weight_description:\n",
    "                weight_description[nm]={}\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                if index_component not in weight_description[nm]:\n",
    "                    weight_description[nm][index_component]={}\n",
    "                values=params[index_component]\n",
    "                flat_values=values.flatten()\n",
    "                for index_wt in range(flat_values.shape[0]):\n",
    "                    if index_wt not in weight_description[nm][index_component]:\n",
    "                        weight_description[nm][index_component][index_wt]=[]\n",
    "                    weight_description[nm][index_component][index_wt].append(flat_values[index_wt].detach().item())\n",
    "    return weight_description\n",
    "\n",
    "\n",
    "def get_boolean_dict_weight_dict(weight_description,prune_rate,thresholds_per_layer):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of 1s and 0s to show\n",
    "    how many times weight is more than threshold\n",
    "    per layer\n",
    "    '''\n",
    "    boolean_weight_description={}\n",
    "    count=0\n",
    "    for layer in weight_description.keys():  \n",
    "#         print(\"Count = \",count)\n",
    "        threshold_this_layer=thresholds_per_layer[count]\n",
    "#         print(\"Threshold for layer \",count,layer,\"is \",threshold_this_layer)\n",
    "        if layer not in boolean_weight_description:\n",
    "            boolean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in boolean_weight_description[layer]:\n",
    "                boolean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in boolean_weight_description[layer][index_component]:\n",
    "                    boolean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt]\n",
    "                all_wts_boolean=[]\n",
    "                for wt in all_wts:\n",
    "                    if abs(wt)>threshold_this_layer:\n",
    "                        all_wts_boolean.append(1)\n",
    "                    else:\n",
    "                        all_wts_boolean.append(0)\n",
    "                boolean_weight_description[layer][index_component][index_wt]=all_wts_boolean                    \n",
    "        count+=1\n",
    "        \n",
    "    return boolean_weight_description\n",
    "    \n",
    "# create mask from boolean weight dictionary\n",
    "def create_mask_from_boolean_wt(model,boolean_wt_dict):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.var(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "#             threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten() \n",
    "                mask_vals=[]\n",
    "                for val_index in range(re_shaped_values.shape[0]):\n",
    "                    boolean_vals=boolean_wt_dict[nm][index_component][val_index]\n",
    "                    m = stats.mode(boolean_vals)\n",
    "#                     print(\"Verdict for this weight is \",m[0][0])\n",
    "                    mask_vals.append(m[0][0])\n",
    "#                 mask_vals = (torch.abs(re_shaped_values)>threshold).float()                \n",
    "                mask_vals=np.asarray(mask_vals)\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=torch.from_numpy(mask_vals)\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part copied from shrinkbench\n",
    "\n",
    "def nonzero(tensor):\n",
    "    \"\"\"Returns absolute number of values different from 0\n",
    "\n",
    "    Arguments:\n",
    "        tensor {numpy.ndarray} -- Array to compute over\n",
    "\n",
    "    Returns:\n",
    "        int -- Number of nonzero elements\n",
    "    \"\"\"\n",
    "    return np.sum(tensor != 0.0)\n",
    "\n",
    "\n",
    "def model_size(model, as_bits=False):\n",
    "    \"\"\"Returns absolute and nonzero model size\n",
    "\n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Network to compute model size over\n",
    "\n",
    "    Keyword Arguments:\n",
    "        as_bits {bool} -- Whether to account for the size of dtype\n",
    "\n",
    "    Returns:\n",
    "        int -- Total number of weight & bias params\n",
    "        int -- Out total_params exactly how many are nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for tensor in model.parameters():\n",
    "        t = np.prod(tensor.shape)\n",
    "        nz = nonzero(tensor.detach().cpu().numpy())\n",
    "        if as_bits:\n",
    "            bits = dtype2bits[tensor.dtype]\n",
    "            t *= bits\n",
    "            nz *= bits\n",
    "        total_params += t\n",
    "        nonzero_params += nz\n",
    "    return int(total_params), int(nonzero_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))        \n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes how many correct outputs with respect to targets\n",
    "\n",
    "    Does NOT compute accuracy but just a raw amount of correct\n",
    "    outputs given target labels. This is done for each value in\n",
    "    topk. A value is considered correct if target is in the topk\n",
    "    highest values of output.\n",
    "    The values returned are upperbounded by the given batch size\n",
    "\n",
    "    [description]\n",
    "\n",
    "    Arguments:\n",
    "        output {torch.Tensor} -- Output prediction of the model\n",
    "        target {torch.Tensor} -- Target labels from data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "    Returns:\n",
    "        List(int) -- Number of correct values for each topk\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        # Only need to do topk for highest k, reuse for the rest\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(torch.tensor(correct_k.item()))\n",
    "        return res\n",
    "\n",
    "\n",
    "# below copied from shrinkbench, to be used later\n",
    "# def accuracy(model, dataloader, topk=(1,)):\n",
    "#     \"\"\"Compute accuracy of a model over a dataloader for various topk\n",
    "\n",
    "#     Arguments:\n",
    "#         model {torch.nn.Module} -- Network to evaluate\n",
    "#         dataloader {torch.utils.data.DataLoader} -- Data to iterate over\n",
    "\n",
    "#     Keyword Arguments:\n",
    "#         topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "#     Returns:\n",
    "#         List(float) -- List of accuracies for each topk\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Use same device as model\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     accs = np.zeros(len(topk))\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for i, (input, target) in enumerate(dataloader):\n",
    "#             input = input.to(device)\n",
    "#             target = target.to(device)\n",
    "#             output = model(input)\n",
    "\n",
    "#             accs += np.array(correct(output, target, topk))\n",
    "\n",
    "#     # Normalize over data length\n",
    "#     accs /= len(dataloader.dataset)\n",
    "\n",
    "#     return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LENET 300-100 for MNIST and comparison\n",
    "class FashionMnistNet(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)        \n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.fc4.is_classifier = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))        \n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        top_1, top_5 = correct(out, labels,topk=(1,5))\n",
    "#         print(\"Batch is \",batch[1].shape)\n",
    "        \n",
    "        top_1=top_1/batch[1].shape[0]\n",
    "        top_5=top_5/batch[1].shape[0]\n",
    "\n",
    "#         print(\"corr\",top_1,top_5)\n",
    "#         return {'val_loss': loss, 'val_acc': acc}\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'top_1': top_1, 'top_5': top_5}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        \n",
    "        batch_top_1s = [x['top_1'] for x in outputs]\n",
    "#         print(batch_top_1s)\n",
    "        epoch_top_1 = torch.stack(batch_top_1s).mean()      # Combine top_1\n",
    "        \n",
    "        batch_top_5s = [x['top_5'] for x in outputs]\n",
    "        epoch_top_5 = torch.stack(batch_top_5s).mean()      # Combine top_5\n",
    "        \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),\n",
    "               'val_top_1': epoch_top_1.item(), 'val_top_5': epoch_top_5.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_top_1: {:.4f}, val_top_5: {:.4f}\".format(\n",
    "                                epoch, result['val_loss'], result['val_acc'], \n",
    "                                result['val_top_1'], result['val_top_5']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     print(\"outputs are \",outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD,\n",
    "        weight_description=None,mask_whole_model=None):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    print(\"At train\")\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if mask_whole_model:\n",
    "#                 print(\"Applying mask\")\n",
    "                apply_mask_model(model,mask_whole_model)\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        print(\"wt desc = \",weight_description)\n",
    "        if weight_description!=None:\n",
    "            print(\"going for weight\")\n",
    "            weight_description=store_weights_in_dic(weight_description,model)\n",
    "    return history, weight_description\n",
    "\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "\n",
    "# val_size = 10000\n",
    "# train_size = len(dataset) - val_size\n",
    "\n",
    "# train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "# print(len(train_ds), len(val_ds))\n",
    "\n",
    "\n",
    "# batch_size=128\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# shape=dataset[0][0].shape\n",
    "# input_size=1\n",
    "# for s in shape:\n",
    "#     input_size*=s\n",
    "# print(input_size)\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print('images.shape:', images.shape)\n",
    "#     inputs = images.reshape(-1, input_size)\n",
    "#     print('inputs.shape:', inputs.shape)\n",
    "#     break\n",
    "    \n",
    "# input_size = inputs.shape[-1]\n",
    "# print(input_size)\n",
    "# hidden_size = 32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program with weighht pruning, dynamic, many ranges\n",
      "Torch cuda  False\n",
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Program with weighht pruning, dynamic, many ranges\")\n",
    "\n",
    "print(\"Torch cuda \",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(\"device \",device)\n",
    "\n",
    "\n",
    "\n",
    "dataset = FashionMNIST(root='data/', download=True, transform=ToTensor())\n",
    "\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = FashionMNIST(root='data/', train=False,transform=ToTensor())\n",
    "\n",
    "val_size = 10000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "shape=dataset[0][0].shape\n",
    "input_size=1\n",
    "for s in shape:\n",
    "    input_size*=s\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/gfxhome/asislam25/.conda/envs/dlproject/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "targets=train_ds.dataset.targets\n",
    "training_data=torch.tensor(train_ds.dataset.data)\n",
    "\n",
    "training_data = training_data.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial result [{'val_loss': 2.3055808544158936, 'val_acc': 0.03623046725988388, 'val_top_1': 0.03623046725988388, 'val_top_5': 0.5213867425918579}]\n"
     ]
    }
   ],
   "source": [
    "model=FashionMnistNet()\n",
    "\n",
    "history = [evaluate(model, val_loader)]\n",
    "print(\"initial result\",history)\n",
    "# weight_description={}\n",
    "epochs=20\n",
    "lr=0.01\n",
    "# history2,weight_description = fit(epochs, lr, model, train_loader, val_loader,weight_description=weight_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result is  {'val_loss': 0.3928965628147125, 'val_acc': 0.860644519329071, 'val_top_1': 0.860644519329071, 'val_top_5': 0.996777355670929}\n",
      "Compression= 0.0\n",
      "Mask compression =  0.1 tensor(0.0350)\n",
      "After pruning, Compression= 0.03497664379103814 Result after pruning is  {'val_loss': 0.3937625288963318, 'val_acc': 0.8609374761581421, 'val_top_1': 0.8609374761581421, 'val_top_5': 0.9966796636581421}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3293, val_acc: 0.8868, val_top_1: 0.8868, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3307, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3344, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3383, val_acc: 0.8801, val_top_1: 0.8801, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3387, val_acc: 0.8787, val_top_1: 0.8787, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3363, val_acc: 0.8799, val_top_1: 0.8799, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3316, val_acc: 0.8836, val_top_1: 0.8836, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3281, val_acc: 0.8843, val_top_1: 0.8843, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3415, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3445, val_acc: 0.8780, val_top_1: 0.8780, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3303, val_acc: 0.8847, val_top_1: 0.8847, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3360, val_acc: 0.8790, val_top_1: 0.8790, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3255, val_acc: 0.8842, val_top_1: 0.8842, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3195, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3496, val_acc: 0.8755, val_top_1: 0.8755, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3351, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3160, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3213, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3141, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3219, val_acc: 0.8859, val_top_1: 0.8859, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3092, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3177, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3618, val_acc: 0.8693, val_top_1: 0.8693, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3107, val_acc: 0.8915, val_top_1: 0.8915, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3166, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3115, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3398, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3205, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3145, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3088, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3148, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3212, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3166, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3074, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3244, val_acc: 0.8880, val_top_1: 0.8880, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Compression= 0.03497664379103814 Result after pruning is  {'val_loss': 0.3703049421310425, 'val_acc': 0.8702148199081421, 'val_top_1': 0.8702148199081421, 'val_top_5': 0.996289074420929}\n",
      "Mask compression =  0.3 tensor(0.1243)\n",
      "After pruning, Compression= 0.12408037501750686 Result after pruning is  {'val_loss': 0.36543112993240356, 'val_acc': 0.8719726800918579, 'val_top_1': 0.8719726800918579, 'val_top_5': 0.9961913824081421}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3081, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.2995, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3069, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3007, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3002, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3118, val_acc: 0.8912, val_top_1: 0.8912, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3416, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3275, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3119, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3260, val_acc: 0.8904, val_top_1: 0.8904, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3029, val_acc: 0.8957, val_top_1: 0.8957, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.2977, val_acc: 0.8980, val_top_1: 0.8980, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3020, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3010, val_acc: 0.8958, val_top_1: 0.8958, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3074, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.2989, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.2952, val_acc: 0.8979, val_top_1: 0.8979, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.2990, val_acc: 0.8943, val_top_1: 0.8943, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3223, val_acc: 0.8882, val_top_1: 0.8882, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3278, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.2996, val_acc: 0.8965, val_top_1: 0.8965, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3057, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3077, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3049, val_acc: 0.8981, val_top_1: 0.8981, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3002, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3302, val_acc: 0.8838, val_top_1: 0.8838, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.2965, val_acc: 0.8971, val_top_1: 0.8971, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3498, val_acc: 0.8784, val_top_1: 0.8784, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.2964, val_acc: 0.9001, val_top_1: 0.9001, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3112, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3031, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3180, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3079, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3184, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3084, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Compression= 0.12408037501750686 Result after pruning is  {'val_loss': 0.34727776050567627, 'val_acc': 0.878222644329071, 'val_top_1': 0.878222644329071, 'val_top_5': 0.9971679449081421}\n",
      "Mask compression =  0.4 tensor(0.1968)\n",
      "After pruning, Compression= 0.19646814575592556 Result after pruning is  {'val_loss': 0.3583146929740906, 'val_acc': 0.8753906488418579, 'val_top_1': 0.8753906488418579, 'val_top_5': 0.9971679449081421}\n",
      "At train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.2991, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3001, val_acc: 0.8977, val_top_1: 0.8977, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3145, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9981\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3125, val_acc: 0.8962, val_top_1: 0.8962, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3091, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3085, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.2949, val_acc: 0.9000, val_top_1: 0.9000, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3138, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3013, val_acc: 0.8980, val_top_1: 0.8980, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3026, val_acc: 0.8987, val_top_1: 0.8987, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3048, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3197, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.2994, val_acc: 0.8991, val_top_1: 0.8991, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3105, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3052, val_acc: 0.8991, val_top_1: 0.8991, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.2986, val_acc: 0.9000, val_top_1: 0.9000, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3010, val_acc: 0.8992, val_top_1: 0.8992, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3165, val_acc: 0.8960, val_top_1: 0.8960, val_top_5: 0.9981\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3083, val_acc: 0.8991, val_top_1: 0.8991, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3398, val_acc: 0.8844, val_top_1: 0.8844, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3157, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.2980, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3228, val_acc: 0.8913, val_top_1: 0.8913, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3191, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9982\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3028, val_acc: 0.8994, val_top_1: 0.8994, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3016, val_acc: 0.8979, val_top_1: 0.8979, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3300, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3367, val_acc: 0.8906, val_top_1: 0.8906, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3110, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3061, val_acc: 0.8993, val_top_1: 0.8993, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3208, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3249, val_acc: 0.8974, val_top_1: 0.8974, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3116, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9981\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3721, val_acc: 0.8790, val_top_1: 0.8790, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3241, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9982\n",
      "wt desc =  None\n",
      "Compression= 0.19646814575592556 Result after pruning is  {'val_loss': 0.3659829795360565, 'val_acc': 0.8779296875, 'val_top_1': 0.8779296875, 'val_top_5': 0.996874988079071}\n",
      "Mask compression =  0.8 tensor(0.3830)\n",
      "After pruning, Compression= 0.3822921215017177 Result after pruning is  {'val_loss': 0.3732176721096039, 'val_acc': 0.8719726800918579, 'val_top_1': 0.8719726800918579, 'val_top_5': 0.9964843988418579}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3005, val_acc: 0.8990, val_top_1: 0.8990, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3073, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3180, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3186, val_acc: 0.8987, val_top_1: 0.8987, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3059, val_acc: 0.9013, val_top_1: 0.9013, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3240, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3392, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3134, val_acc: 0.8989, val_top_1: 0.8989, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3206, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3308, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3169, val_acc: 0.8993, val_top_1: 0.8993, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3170, val_acc: 0.9007, val_top_1: 0.9007, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3347, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3275, val_acc: 0.8966, val_top_1: 0.8966, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3148, val_acc: 0.8998, val_top_1: 0.8998, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3127, val_acc: 0.9004, val_top_1: 0.9004, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3202, val_acc: 0.8994, val_top_1: 0.8994, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3232, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3340, val_acc: 0.8963, val_top_1: 0.8963, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3179, val_acc: 0.9006, val_top_1: 0.9006, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3693, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3236, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3352, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3243, val_acc: 0.8988, val_top_1: 0.8988, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3190, val_acc: 0.9007, val_top_1: 0.9007, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3274, val_acc: 0.8981, val_top_1: 0.8981, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3567, val_acc: 0.8901, val_top_1: 0.8901, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3572, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3431, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9980\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3649, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3260, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3353, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9979\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4173, val_acc: 0.8736, val_top_1: 0.8736, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3425, val_acc: 0.8964, val_top_1: 0.8964, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3466, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9976\n",
      "wt desc =  None\n",
      "Compression= 0.3822921215017177 Result after pruning is  {'val_loss': 0.38398224115371704, 'val_acc': 0.8818359375, 'val_top_1': 0.8818359375, 'val_top_5': 0.99609375}\n",
      "Mask compression =  1.1 tensor(0.5664)\n",
      "After pruning, Compression= 0.5653767887890196 Result after pruning is  {'val_loss': 0.48811429738998413, 'val_acc': 0.844921886920929, 'val_top_1': 0.844921886920929, 'val_top_5': 0.994921863079071}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3103, val_acc: 0.8990, val_top_1: 0.8990, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3202, val_acc: 0.8960, val_top_1: 0.8960, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3154, val_acc: 0.8984, val_top_1: 0.8984, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3200, val_acc: 0.8975, val_top_1: 0.8975, val_top_5: 0.9973\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], val_loss: 0.3360, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9978\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3217, val_acc: 0.8983, val_top_1: 0.8983, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3331, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9977\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3183, val_acc: 0.8986, val_top_1: 0.8986, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3674, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3248, val_acc: 0.8973, val_top_1: 0.8973, val_top_5: 0.9974\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3357, val_acc: 0.8984, val_top_1: 0.8984, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3571, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9975\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3509, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3306, val_acc: 0.8965, val_top_1: 0.8965, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3440, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3428, val_acc: 0.8988, val_top_1: 0.8988, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3428, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3360, val_acc: 0.8993, val_top_1: 0.8993, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3353, val_acc: 0.8987, val_top_1: 0.8987, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3537, val_acc: 0.8956, val_top_1: 0.8956, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3790, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3507, val_acc: 0.8960, val_top_1: 0.8960, val_top_5: 0.9973\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3469, val_acc: 0.8980, val_top_1: 0.8980, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3541, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3542, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9969\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3573, val_acc: 0.8990, val_top_1: 0.8990, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.3584, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.3495, val_acc: 0.8986, val_top_1: 0.8986, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3479, val_acc: 0.8985, val_top_1: 0.8985, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.3557, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.3615, val_acc: 0.8968, val_top_1: 0.8968, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.3624, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.3527, val_acc: 0.8987, val_top_1: 0.8987, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.3590, val_acc: 0.8984, val_top_1: 0.8984, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3701, val_acc: 0.8999, val_top_1: 0.8999, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Compression= 0.5653767887890196 Result after pruning is  {'val_loss': 0.4125429093837738, 'val_acc': 0.882617175579071, 'val_top_1': 0.882617175579071, 'val_top_5': 0.99609375}\n",
      "Mask compression =  1.3 tensor(0.7053)\n",
      "After pruning, Compression= 0.7039240078760267 Result after pruning is  {'val_loss': 0.5932134389877319, 'val_acc': 0.8431640863418579, 'val_top_1': 0.8431640863418579, 'val_top_5': 0.9930664300918579}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3480, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9961\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3491, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9963\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3447, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3524, val_acc: 0.8986, val_top_1: 0.8986, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3813, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9963\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3424, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3432, val_acc: 0.8962, val_top_1: 0.8962, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3875, val_acc: 0.8890, val_top_1: 0.8890, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3501, val_acc: 0.8978, val_top_1: 0.8978, val_top_5: 0.9972\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3575, val_acc: 0.8960, val_top_1: 0.8960, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3525, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3706, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3543, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3665, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9962\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3880, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9961\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3767, val_acc: 0.8964, val_top_1: 0.8964, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3639, val_acc: 0.8965, val_top_1: 0.8965, val_top_5: 0.9971\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3661, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3705, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3690, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9968\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3740, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3826, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.3804, val_acc: 0.8968, val_top_1: 0.8968, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.3841, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.3857, val_acc: 0.8975, val_top_1: 0.8975, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.3885, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9961\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4009, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4307, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9958\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.3811, val_acc: 0.8970, val_top_1: 0.8970, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4060, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9961\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4111, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9962\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.5180, val_acc: 0.8651, val_top_1: 0.8651, val_top_5: 0.9970\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4035, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4007, val_acc: 0.8967, val_top_1: 0.8967, val_top_5: 0.9961\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.3898, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Compression= 0.7039240078760267 Result after pruning is  {'val_loss': 0.4384363293647766, 'val_acc': 0.883593738079071, 'val_top_1': 0.883593738079071, 'val_top_5': 0.99560546875}\n",
      "Mask compression =  1.5 tensor(0.7970)\n",
      "After pruning, Compression= 0.7954539837371583 Result after pruning is  {'val_loss': 1.1250451803207397, 'val_acc': 0.733105480670929, 'val_top_1': 0.733105480670929, 'val_top_5': 0.9945312738418579}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.3641, val_acc: 0.8936, val_top_1: 0.8936, val_top_5: 0.9963\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3675, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3614, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3703, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3752, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3637, val_acc: 0.8971, val_top_1: 0.8971, val_top_5: 0.9958\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3778, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3791, val_acc: 0.8963, val_top_1: 0.8963, val_top_5: 0.9959\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], val_loss: 0.3766, val_acc: 0.8968, val_top_1: 0.8968, val_top_5: 0.9963\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3787, val_acc: 0.8970, val_top_1: 0.8970, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3904, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9962\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3748, val_acc: 0.8979, val_top_1: 0.8979, val_top_5: 0.9962\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3993, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3780, val_acc: 0.8976, val_top_1: 0.8976, val_top_5: 0.9959\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.3966, val_acc: 0.8968, val_top_1: 0.8968, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4181, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.4012, val_acc: 0.8963, val_top_1: 0.8963, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.4050, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4027, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4039, val_acc: 0.8966, val_top_1: 0.8966, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.4325, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9964\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.3982, val_acc: 0.8970, val_top_1: 0.8970, val_top_5: 0.9962\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4931, val_acc: 0.8766, val_top_1: 0.8766, val_top_5: 0.9967\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4069, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4137, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.5197, val_acc: 0.8736, val_top_1: 0.8736, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4107, val_acc: 0.8970, val_top_1: 0.8970, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4132, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4174, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9965\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4158, val_acc: 0.8972, val_top_1: 0.8972, val_top_5: 0.9958\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4157, val_acc: 0.8969, val_top_1: 0.8969, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4278, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4164, val_acc: 0.9000, val_top_1: 0.9000, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4266, val_acc: 0.8950, val_top_1: 0.8950, val_top_5: 0.9959\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.4340, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Compression= 0.7954539837371583 Result after pruning is  {'val_loss': 0.4856298863887787, 'val_acc': 0.8792968988418579, 'val_top_1': 0.8792968988418579, 'val_top_5': 0.994921863079071}\n",
      "Mask compression =  1.7 tensor(0.8434)\n",
      "After pruning, Compression= 0.8418039067069805 Result after pruning is  {'val_loss': 0.7667604684829712, 'val_acc': 0.821972668170929, 'val_top_1': 0.821972668170929, 'val_top_5': 0.987500011920929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4007, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.4187, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.4024, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.4117, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3976, val_acc: 0.8970, val_top_1: 0.8970, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.4043, val_acc: 0.8954, val_top_1: 0.8954, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.4058, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.4014, val_acc: 0.8958, val_top_1: 0.8958, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.4076, val_acc: 0.8949, val_top_1: 0.8949, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.4068, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4240, val_acc: 0.8961, val_top_1: 0.8961, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.4106, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.4111, val_acc: 0.8957, val_top_1: 0.8957, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.4121, val_acc: 0.8959, val_top_1: 0.8959, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4148, val_acc: 0.8963, val_top_1: 0.8963, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4244, val_acc: 0.8971, val_top_1: 0.8971, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.4212, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.4495, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4420, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4513, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.4720, val_acc: 0.8851, val_top_1: 0.8851, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4726, val_acc: 0.8855, val_top_1: 0.8855, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4353, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4285, val_acc: 0.8962, val_top_1: 0.8962, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4517, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4388, val_acc: 0.8958, val_top_1: 0.8958, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4539, val_acc: 0.8932, val_top_1: 0.8932, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4516, val_acc: 0.8952, val_top_1: 0.8952, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4403, val_acc: 0.8955, val_top_1: 0.8955, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4458, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4466, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4735, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4757, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4649, val_acc: 0.8951, val_top_1: 0.8951, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.4548, val_acc: 0.8950, val_top_1: 0.8950, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Compression= 0.8418039067069805 Result after pruning is  {'val_loss': 0.506775975227356, 'val_acc': 0.878125011920929, 'val_top_1': 0.878125011920929, 'val_top_5': 0.9940429925918579}\n",
      "Mask compression =  1.8 tensor(0.8649)\n",
      "After pruning, Compression= 0.8632652556825203 Result after pruning is  {'val_loss': 1.0640084743499756, 'val_acc': 0.7860351800918579, 'val_top_1': 0.7860351800918579, 'val_top_5': 0.9913085699081421}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4326, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.4331, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.4376, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.4422, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.4318, val_acc: 0.8923, val_top_1: 0.8923, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.4534, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.4625, val_acc: 0.8870, val_top_1: 0.8870, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.4500, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.4396, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.4412, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4464, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.4480, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9944\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], val_loss: 0.4582, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.4480, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4857, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4658, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.4841, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.4583, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4717, val_acc: 0.8944, val_top_1: 0.8944, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4936, val_acc: 0.8839, val_top_1: 0.8839, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.4660, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4815, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4931, val_acc: 0.8867, val_top_1: 0.8867, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4883, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4747, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4741, val_acc: 0.8929, val_top_1: 0.8929, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4799, val_acc: 0.8938, val_top_1: 0.8938, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4763, val_acc: 0.8958, val_top_1: 0.8958, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4851, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4946, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4814, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.5014, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4954, val_acc: 0.8914, val_top_1: 0.8914, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4920, val_acc: 0.8947, val_top_1: 0.8947, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.4969, val_acc: 0.8942, val_top_1: 0.8942, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Compression= 0.8632652556825203 Result after pruning is  {'val_loss': 0.5596030950546265, 'val_acc': 0.8790038824081421, 'val_top_1': 0.8790038824081421, 'val_top_5': 0.9931640625}\n",
      "Mask compression =  2.1 tensor(0.8971)\n",
      "After pruning, Compression= 0.8953707746681936 Result after pruning is  {'val_loss': 1.9949538707733154, 'val_acc': 0.631054699420929, 'val_top_1': 0.631054699420929, 'val_top_5': 0.9498046636581421}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4116, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.4007, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.4156, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.4030, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.4032, val_acc: 0.8933, val_top_1: 0.8933, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.4142, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.4182, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.4247, val_acc: 0.8931, val_top_1: 0.8931, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.4091, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.4108, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4200, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.4310, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.4181, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.4382, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4247, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4368, val_acc: 0.8894, val_top_1: 0.8894, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.4396, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.4324, val_acc: 0.8940, val_top_1: 0.8940, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4416, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4364, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.4496, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4617, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4425, val_acc: 0.8939, val_top_1: 0.8939, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4465, val_acc: 0.8945, val_top_1: 0.8945, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4621, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4481, val_acc: 0.8953, val_top_1: 0.8953, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4518, val_acc: 0.8935, val_top_1: 0.8935, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4593, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4683, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4596, val_acc: 0.8934, val_top_1: 0.8934, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4725, val_acc: 0.8919, val_top_1: 0.8919, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4777, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4655, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4863, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.4813, val_acc: 0.8925, val_top_1: 0.8925, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Compression= 0.8953707746681936 Result after pruning is  {'val_loss': 0.546613872051239, 'val_acc': 0.879101574420929, 'val_top_1': 0.879101574420929, 'val_top_5': 0.9927734136581421}\n",
      "Mask compression =  2.4 tensor(0.9194)\n",
      "After pruning, Compression= 0.9176230217249817 Result after pruning is  {'val_loss': 2.258958101272583, 'val_acc': 0.63330078125, 'val_top_1': 0.63330078125, 'val_top_5': 0.926464855670929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4379, val_acc: 0.8856, val_top_1: 0.8856, val_top_5: 0.9911\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.4644, val_acc: 0.8763, val_top_1: 0.8763, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.4142, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9925\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.4160, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9921\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.4185, val_acc: 0.8876, val_top_1: 0.8876, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.4343, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9913\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.4178, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.4246, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.4282, val_acc: 0.8902, val_top_1: 0.8902, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.4262, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4295, val_acc: 0.8916, val_top_1: 0.8916, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.4327, val_acc: 0.8924, val_top_1: 0.8924, val_top_5: 0.9929\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.4420, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.4376, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9929\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4533, val_acc: 0.8900, val_top_1: 0.8900, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4586, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9920\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], val_loss: 0.4466, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.4435, val_acc: 0.8926, val_top_1: 0.8926, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4579, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4506, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.4592, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4572, val_acc: 0.8927, val_top_1: 0.8927, val_top_5: 0.9925\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4589, val_acc: 0.8918, val_top_1: 0.8918, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4608, val_acc: 0.8905, val_top_1: 0.8905, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4626, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4720, val_acc: 0.8922, val_top_1: 0.8922, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4609, val_acc: 0.8948, val_top_1: 0.8948, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4833, val_acc: 0.8909, val_top_1: 0.8909, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4783, val_acc: 0.8928, val_top_1: 0.8928, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4767, val_acc: 0.8899, val_top_1: 0.8899, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4745, val_acc: 0.8930, val_top_1: 0.8930, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4775, val_acc: 0.8941, val_top_1: 0.8941, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4810, val_acc: 0.8937, val_top_1: 0.8937, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4855, val_acc: 0.8920, val_top_1: 0.8920, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.5309, val_acc: 0.8789, val_top_1: 0.8789, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Compression= 0.9176230217249817 Result after pruning is  {'val_loss': 0.5839354395866394, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9932616949081421}\n",
      "Mask compression =  2.9 tensor(0.9442)\n",
      "After pruning, Compression= 0.9423838986332292 Result after pruning is  {'val_loss': 4.299267768859863, 'val_acc': 0.40556639432907104, 'val_top_1': 0.40556639432907104, 'val_top_5': 0.890917956829071}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4238, val_acc: 0.8743, val_top_1: 0.8743, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.4012, val_acc: 0.8828, val_top_1: 0.8828, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3930, val_acc: 0.8833, val_top_1: 0.8833, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3905, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3910, val_acc: 0.8825, val_top_1: 0.8825, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3866, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.3827, val_acc: 0.8881, val_top_1: 0.8881, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3803, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.4044, val_acc: 0.8773, val_top_1: 0.8773, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3893, val_acc: 0.8880, val_top_1: 0.8880, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.3851, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.3930, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.3951, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.3947, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4117, val_acc: 0.8864, val_top_1: 0.8864, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.3926, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.3913, val_acc: 0.8911, val_top_1: 0.8911, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.3987, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.3962, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.3993, val_acc: 0.8864, val_top_1: 0.8864, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.3987, val_acc: 0.8921, val_top_1: 0.8921, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4070, val_acc: 0.8908, val_top_1: 0.8908, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4007, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4108, val_acc: 0.8904, val_top_1: 0.8904, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4066, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4142, val_acc: 0.8876, val_top_1: 0.8876, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4205, val_acc: 0.8843, val_top_1: 0.8843, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4218, val_acc: 0.8860, val_top_1: 0.8860, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4197, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4197, val_acc: 0.8910, val_top_1: 0.8910, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4180, val_acc: 0.8882, val_top_1: 0.8882, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4223, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4288, val_acc: 0.8906, val_top_1: 0.8906, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4296, val_acc: 0.8893, val_top_1: 0.8893, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.4323, val_acc: 0.8886, val_top_1: 0.8886, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Compression= 0.9423838986332292 Result after pruning is  {'val_loss': 0.484125554561615, 'val_acc': 0.8798828125, 'val_top_1': 0.8798828125, 'val_top_5': 0.992871105670929}\n",
      "Mask compression =  3.1 tensor(0.9535)\n",
      "After pruning, Compression= 0.9516604740445375 Result after pruning is  {'val_loss': 1.7202870845794678, 'val_acc': 0.625, 'val_top_1': 0.625, 'val_top_5': 0.9814453125}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4069, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.3924, val_acc: 0.8839, val_top_1: 0.8839, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.3964, val_acc: 0.8850, val_top_1: 0.8850, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.3957, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.3962, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9958\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.3949, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9959\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.4095, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.3962, val_acc: 0.8903, val_top_1: 0.8903, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.3972, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.3972, val_acc: 0.8917, val_top_1: 0.8917, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4031, val_acc: 0.8875, val_top_1: 0.8875, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.4020, val_acc: 0.8898, val_top_1: 0.8898, val_top_5: 0.9960\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.4053, val_acc: 0.8895, val_top_1: 0.8895, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.4058, val_acc: 0.8892, val_top_1: 0.8892, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.4066, val_acc: 0.8907, val_top_1: 0.8907, val_top_5: 0.9959\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4124, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.4169, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.4334, val_acc: 0.8862, val_top_1: 0.8862, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4180, val_acc: 0.8888, val_top_1: 0.8888, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4199, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9956\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 0.4238, val_acc: 0.8897, val_top_1: 0.8897, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4229, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4298, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4297, val_acc: 0.8881, val_top_1: 0.8881, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4284, val_acc: 0.8896, val_top_1: 0.8896, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4330, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4312, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.4361, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.4383, val_acc: 0.8878, val_top_1: 0.8878, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.4464, val_acc: 0.8885, val_top_1: 0.8885, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.4384, val_acc: 0.8881, val_top_1: 0.8881, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.4463, val_acc: 0.8889, val_top_1: 0.8889, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.4599, val_acc: 0.8849, val_top_1: 0.8849, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.4922, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.4508, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Compression= 0.9516604740445375 Result after pruning is  {'val_loss': 0.5052984952926636, 'val_acc': 0.8790038824081421, 'val_top_1': 0.8790038824081421, 'val_top_5': 0.99365234375}\n",
      "Mask compression =  3.15 tensor(0.9572)\n",
      "After pruning, Compression= 0.9554337169738262 Result after pruning is  {'val_loss': 0.7606550455093384, 'val_acc': 0.835742175579071, 'val_top_1': 0.835742175579071, 'val_top_5': 0.991992175579071}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.4483, val_acc: 0.8855, val_top_1: 0.8855, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.4489, val_acc: 0.8875, val_top_1: 0.8875, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.4483, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.4472, val_acc: 0.8891, val_top_1: 0.8891, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.4613, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.4538, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.4542, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.4530, val_acc: 0.8887, val_top_1: 0.8887, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.4665, val_acc: 0.8843, val_top_1: 0.8843, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.4602, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.4706, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.4760, val_acc: 0.8859, val_top_1: 0.8859, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.4694, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.4673, val_acc: 0.8873, val_top_1: 0.8873, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.5096, val_acc: 0.8798, val_top_1: 0.8798, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.4693, val_acc: 0.8862, val_top_1: 0.8862, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.4928, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.5013, val_acc: 0.8850, val_top_1: 0.8850, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.4770, val_acc: 0.8876, val_top_1: 0.8876, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.4838, val_acc: 0.8864, val_top_1: 0.8864, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.4769, val_acc: 0.8874, val_top_1: 0.8874, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.4872, val_acc: 0.8884, val_top_1: 0.8884, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.4836, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.4830, val_acc: 0.8856, val_top_1: 0.8856, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.4842, val_acc: 0.8876, val_top_1: 0.8876, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.4939, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.4947, val_acc: 0.8863, val_top_1: 0.8863, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.5216, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.5241, val_acc: 0.8849, val_top_1: 0.8849, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.5087, val_acc: 0.8876, val_top_1: 0.8876, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.5156, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.5156, val_acc: 0.8859, val_top_1: 0.8859, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.5188, val_acc: 0.8848, val_top_1: 0.8848, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.5230, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.5297, val_acc: 0.8844, val_top_1: 0.8844, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.5319, val_acc: 0.8841, val_top_1: 0.8841, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.5252, val_acc: 0.8870, val_top_1: 0.8870, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.5601, val_acc: 0.8808, val_top_1: 0.8808, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.5284, val_acc: 0.8883, val_top_1: 0.8883, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.5559, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.5616, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.5323, val_acc: 0.8867, val_top_1: 0.8867, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.5441, val_acc: 0.8866, val_top_1: 0.8866, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.5502, val_acc: 0.8871, val_top_1: 0.8871, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.5383, val_acc: 0.8860, val_top_1: 0.8860, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.5480, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.5515, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.5526, val_acc: 0.8850, val_top_1: 0.8850, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.5445, val_acc: 0.8877, val_top_1: 0.8877, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.5588, val_acc: 0.8853, val_top_1: 0.8853, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.5484, val_acc: 0.8867, val_top_1: 0.8867, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.5608, val_acc: 0.8869, val_top_1: 0.8869, val_top_5: 0.9916\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.5666, val_acc: 0.8825, val_top_1: 0.8825, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.5567, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.5561, val_acc: 0.8865, val_top_1: 0.8865, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.5706, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.5669, val_acc: 0.8846, val_top_1: 0.8846, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.5756, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Compression= 0.9574150814377868 Result after pruning is  {'val_loss': 0.6302942633628845, 'val_acc': 0.87744140625, 'val_top_1': 0.87744140625, 'val_top_5': 0.9927734136581421}\n",
      "Mask compression =  3.35 tensor(0.9629)\n",
      "After pruning, Compression= 0.961052388759361 Result after pruning is  {'val_loss': 0.9979394674301147, 'val_acc': 0.8353515863418579, 'val_top_1': 0.8353515863418579, 'val_top_5': 0.9873046875}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.5689, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9931\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.5599, val_acc: 0.8802, val_top_1: 0.8802, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.5581, val_acc: 0.8832, val_top_1: 0.8832, val_top_5: 0.9928\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.5444, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.5510, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.5453, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.5450, val_acc: 0.8840, val_top_1: 0.8840, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.5505, val_acc: 0.8854, val_top_1: 0.8854, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.5466, val_acc: 0.8861, val_top_1: 0.8861, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.5476, val_acc: 0.8868, val_top_1: 0.8868, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.5405, val_acc: 0.8867, val_top_1: 0.8867, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.5531, val_acc: 0.8830, val_top_1: 0.8830, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.5519, val_acc: 0.8865, val_top_1: 0.8865, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.5451, val_acc: 0.8843, val_top_1: 0.8843, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.5710, val_acc: 0.8838, val_top_1: 0.8838, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.5646, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.5526, val_acc: 0.8864, val_top_1: 0.8864, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.5655, val_acc: 0.8840, val_top_1: 0.8840, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.5661, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.5661, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.5589, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.5619, val_acc: 0.8859, val_top_1: 0.8859, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.5628, val_acc: 0.8857, val_top_1: 0.8857, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.5663, val_acc: 0.8846, val_top_1: 0.8846, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.5634, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.5709, val_acc: 0.8859, val_top_1: 0.8859, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.5847, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.8207, val_acc: 0.8455, val_top_1: 0.8455, val_top_5: 0.9966\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.5736, val_acc: 0.8858, val_top_1: 0.8858, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.5867, val_acc: 0.8868, val_top_1: 0.8868, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.5831, val_acc: 0.8852, val_top_1: 0.8852, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.5777, val_acc: 0.8856, val_top_1: 0.8856, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.5770, val_acc: 0.8870, val_top_1: 0.8870, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.5880, val_acc: 0.8846, val_top_1: 0.8846, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.5836, val_acc: 0.8849, val_top_1: 0.8849, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Compression= 0.961052388759361 Result after pruning is  {'val_loss': 0.6420601606369019, 'val_acc': 0.873046875, 'val_top_1': 0.873046875, 'val_top_5': 0.9930664300918579}\n",
      "Mask compression =  3.4 tensor(0.9650)\n",
      "After pruning, Compression= 0.9631696888310362 Result after pruning is  {'val_loss': 1.100311279296875, 'val_acc': 0.797070324420929, 'val_top_1': 0.797070324420929, 'val_top_5': 0.993847668170929}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.5729, val_acc: 0.8830, val_top_1: 0.8830, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.5815, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.5769, val_acc: 0.8816, val_top_1: 0.8816, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.5835, val_acc: 0.8823, val_top_1: 0.8823, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.5779, val_acc: 0.8825, val_top_1: 0.8825, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.5929, val_acc: 0.8812, val_top_1: 0.8812, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.5954, val_acc: 0.8833, val_top_1: 0.8833, val_top_5: 0.9918\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.5854, val_acc: 0.8847, val_top_1: 0.8847, val_top_5: 0.9925\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.5886, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.5939, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.5855, val_acc: 0.8835, val_top_1: 0.8835, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.5889, val_acc: 0.8821, val_top_1: 0.8821, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.6091, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.5927, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.5968, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.5966, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6012, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.5946, val_acc: 0.8841, val_top_1: 0.8841, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6002, val_acc: 0.8840, val_top_1: 0.8840, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6075, val_acc: 0.8824, val_top_1: 0.8824, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6046, val_acc: 0.8845, val_top_1: 0.8845, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6057, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6128, val_acc: 0.8823, val_top_1: 0.8823, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.6205, val_acc: 0.8802, val_top_1: 0.8802, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6162, val_acc: 0.8818, val_top_1: 0.8818, val_top_5: 0.9920\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6187, val_acc: 0.8823, val_top_1: 0.8823, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6204, val_acc: 0.8811, val_top_1: 0.8811, val_top_5: 0.9940\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6246, val_acc: 0.8845, val_top_1: 0.8845, val_top_5: 0.9920\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6168, val_acc: 0.8832, val_top_1: 0.8832, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6160, val_acc: 0.8831, val_top_1: 0.8831, val_top_5: 0.9942\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6406, val_acc: 0.8793, val_top_1: 0.8793, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6793, val_acc: 0.8739, val_top_1: 0.8739, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6440, val_acc: 0.8794, val_top_1: 0.8794, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.7182, val_acc: 0.8757, val_top_1: 0.8757, val_top_5: 0.9896\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6337, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Compression= 0.9631696888310362 Result after pruning is  {'val_loss': 0.6921290159225464, 'val_acc': 0.8749023675918579, 'val_top_1': 0.8749023675918579, 'val_top_5': 0.99365234375}\n",
      "Mask compression =  3.5 tensor(0.9670)\n",
      "After pruning, Compression= 0.9652046036859147 Result after pruning is  {'val_loss': 1.045121192932129, 'val_acc': 0.8272460699081421, 'val_top_1': 0.8272460699081421, 'val_top_5': 0.987109363079071}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.5934, val_acc: 0.8802, val_top_1: 0.8802, val_top_5: 0.9919\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.5928, val_acc: 0.8816, val_top_1: 0.8816, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.6130, val_acc: 0.8715, val_top_1: 0.8715, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.5888, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.5932, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9946\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], val_loss: 0.6028, val_acc: 0.8781, val_top_1: 0.8781, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.5881, val_acc: 0.8806, val_top_1: 0.8806, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.6357, val_acc: 0.8727, val_top_1: 0.8727, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.5998, val_acc: 0.8804, val_top_1: 0.8804, val_top_5: 0.9928\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.6098, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.5983, val_acc: 0.8797, val_top_1: 0.8797, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.6127, val_acc: 0.8780, val_top_1: 0.8780, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.5978, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.6388, val_acc: 0.8814, val_top_1: 0.8814, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.6121, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.6053, val_acc: 0.8824, val_top_1: 0.8824, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6157, val_acc: 0.8812, val_top_1: 0.8812, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.6087, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6098, val_acc: 0.8825, val_top_1: 0.8825, val_top_5: 0.9948\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6110, val_acc: 0.8825, val_top_1: 0.8825, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6223, val_acc: 0.8792, val_top_1: 0.8792, val_top_5: 0.9925\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6413, val_acc: 0.8756, val_top_1: 0.8756, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6262, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.6270, val_acc: 0.8807, val_top_1: 0.8807, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6199, val_acc: 0.8814, val_top_1: 0.8814, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6253, val_acc: 0.8827, val_top_1: 0.8827, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6557, val_acc: 0.8773, val_top_1: 0.8773, val_top_5: 0.9921\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6235, val_acc: 0.8829, val_top_1: 0.8829, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6317, val_acc: 0.8804, val_top_1: 0.8804, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6299, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6422, val_acc: 0.8804, val_top_1: 0.8804, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6384, val_acc: 0.8811, val_top_1: 0.8811, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6341, val_acc: 0.8830, val_top_1: 0.8830, val_top_5: 0.9928\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.6428, val_acc: 0.8828, val_top_1: 0.8828, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6384, val_acc: 0.8847, val_top_1: 0.8847, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Compression= 0.9652046036859147 Result after pruning is  {'val_loss': 0.6938765645027161, 'val_acc': 0.874804675579071, 'val_top_1': 0.874804675579071, 'val_top_5': 0.994140625}\n",
      "Mask compression =  3.6 tensor(0.9690)\n",
      "After pruning, Compression= 0.9671942066715549 Result after pruning is  {'val_loss': 0.8371752500534058, 'val_acc': 0.8519531488418579, 'val_top_1': 0.8519531488418579, 'val_top_5': 0.991503894329071}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.6321, val_acc: 0.8772, val_top_1: 0.8772, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.6322, val_acc: 0.8775, val_top_1: 0.8775, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.6285, val_acc: 0.8779, val_top_1: 0.8779, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.6208, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.6302, val_acc: 0.8798, val_top_1: 0.8798, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.6379, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.6337, val_acc: 0.8800, val_top_1: 0.8800, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.6228, val_acc: 0.8797, val_top_1: 0.8797, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.6258, val_acc: 0.8808, val_top_1: 0.8808, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.6340, val_acc: 0.8784, val_top_1: 0.8784, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.6367, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.6409, val_acc: 0.8807, val_top_1: 0.8807, val_top_5: 0.9943\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.6452, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.6309, val_acc: 0.8824, val_top_1: 0.8824, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.6302, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.6408, val_acc: 0.8793, val_top_1: 0.8793, val_top_5: 0.9958\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6345, val_acc: 0.8823, val_top_1: 0.8823, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.6400, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6412, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6498, val_acc: 0.8806, val_top_1: 0.8806, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6422, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6448, val_acc: 0.8782, val_top_1: 0.8782, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6441, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.7146, val_acc: 0.8681, val_top_1: 0.8681, val_top_5: 0.9959\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6528, val_acc: 0.8805, val_top_1: 0.8805, val_top_5: 0.9939\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6471, val_acc: 0.8802, val_top_1: 0.8802, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6550, val_acc: 0.8799, val_top_1: 0.8799, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6561, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6503, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6541, val_acc: 0.8790, val_top_1: 0.8790, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6521, val_acc: 0.8791, val_top_1: 0.8791, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6626, val_acc: 0.8787, val_top_1: 0.8787, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6584, val_acc: 0.8798, val_top_1: 0.8798, val_top_5: 0.9955\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.6786, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6623, val_acc: 0.8807, val_top_1: 0.8807, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Compression= 0.9671942066715549 Result after pruning is  {'val_loss': 0.7266407012939453, 'val_acc': 0.873046875, 'val_top_1': 0.873046875, 'val_top_5': 0.9935547113418579}\n",
      "Mask compression =  3.7 tensor(0.9708)\n",
      "After pruning, Compression= 0.9689654888326839 Result after pruning is  {'val_loss': 1.4015833139419556, 'val_acc': 0.7911132574081421, 'val_top_1': 0.7911132574081421, 'val_top_5': 0.9886718988418579}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.6428, val_acc: 0.8772, val_top_1: 0.8772, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.6212, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.6058, val_acc: 0.8782, val_top_1: 0.8782, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.6021, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.6046, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.6033, val_acc: 0.8837, val_top_1: 0.8837, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.6094, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.6063, val_acc: 0.8803, val_top_1: 0.8803, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.6144, val_acc: 0.8785, val_top_1: 0.8785, val_top_5: 0.9951\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], val_loss: 0.6061, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.6098, val_acc: 0.8819, val_top_1: 0.8819, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.6136, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.6138, val_acc: 0.8836, val_top_1: 0.8836, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.6162, val_acc: 0.8801, val_top_1: 0.8801, val_top_5: 0.9954\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.6203, val_acc: 0.8826, val_top_1: 0.8826, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.6156, val_acc: 0.8825, val_top_1: 0.8825, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6241, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9953\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.6408, val_acc: 0.8797, val_top_1: 0.8797, val_top_5: 0.9941\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6163, val_acc: 0.8793, val_top_1: 0.8793, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6351, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9956\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6495, val_acc: 0.8749, val_top_1: 0.8749, val_top_5: 0.9957\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6235, val_acc: 0.8801, val_top_1: 0.8801, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6280, val_acc: 0.8822, val_top_1: 0.8822, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.6420, val_acc: 0.8792, val_top_1: 0.8792, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6368, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6266, val_acc: 0.8838, val_top_1: 0.8838, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6254, val_acc: 0.8828, val_top_1: 0.8828, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6351, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9945\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6323, val_acc: 0.8834, val_top_1: 0.8834, val_top_5: 0.9949\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6354, val_acc: 0.8828, val_top_1: 0.8828, val_top_5: 0.9947\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6781, val_acc: 0.8756, val_top_1: 0.8756, val_top_5: 0.9946\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6434, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9951\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6342, val_acc: 0.8819, val_top_1: 0.8819, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.6350, val_acc: 0.8814, val_top_1: 0.8814, val_top_5: 0.9950\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6546, val_acc: 0.8800, val_top_1: 0.8800, val_top_5: 0.9952\n",
      "wt desc =  None\n",
      "Compression= 0.9689654888326839 Result after pruning is  {'val_loss': 0.7125850915908813, 'val_acc': 0.87109375, 'val_top_1': 0.87109375, 'val_top_5': 0.992968738079071}\n",
      "Mask compression =  3.8 tensor(0.9727)\n",
      "After pruning, Compression= 0.9708644680798477 Result after pruning is  {'val_loss': 2.8165676593780518, 'val_acc': 0.7730468511581421, 'val_top_1': 0.7730468511581421, 'val_top_5': 0.9544922113418579}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.6697, val_acc: 0.8708, val_top_1: 0.8708, val_top_5: 0.9915\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.6235, val_acc: 0.8784, val_top_1: 0.8784, val_top_5: 0.9898\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.6266, val_acc: 0.8783, val_top_1: 0.8783, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.6237, val_acc: 0.8752, val_top_1: 0.8752, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.6311, val_acc: 0.8768, val_top_1: 0.8768, val_top_5: 0.9909\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.6178, val_acc: 0.8782, val_top_1: 0.8782, val_top_5: 0.9906\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.6173, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9906\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.6133, val_acc: 0.8778, val_top_1: 0.8778, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.6138, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9903\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.6451, val_acc: 0.8778, val_top_1: 0.8778, val_top_5: 0.9900\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.6167, val_acc: 0.8765, val_top_1: 0.8765, val_top_5: 0.9905\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.6279, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9902\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.6172, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9910\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.6130, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9905\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.6195, val_acc: 0.8785, val_top_1: 0.8785, val_top_5: 0.9911\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.6337, val_acc: 0.8736, val_top_1: 0.8736, val_top_5: 0.9917\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6190, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9905\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.6176, val_acc: 0.8801, val_top_1: 0.8801, val_top_5: 0.9907\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6405, val_acc: 0.8737, val_top_1: 0.8737, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6162, val_acc: 0.8808, val_top_1: 0.8808, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6271, val_acc: 0.8787, val_top_1: 0.8787, val_top_5: 0.9907\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6295, val_acc: 0.8761, val_top_1: 0.8761, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6285, val_acc: 0.8793, val_top_1: 0.8793, val_top_5: 0.9906\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.6226, val_acc: 0.8787, val_top_1: 0.8787, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6496, val_acc: 0.8762, val_top_1: 0.8762, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6497, val_acc: 0.8775, val_top_1: 0.8775, val_top_5: 0.9909\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6235, val_acc: 0.8813, val_top_1: 0.8813, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6301, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9925\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6442, val_acc: 0.8788, val_top_1: 0.8788, val_top_5: 0.9902\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6343, val_acc: 0.8794, val_top_1: 0.8794, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6399, val_acc: 0.8783, val_top_1: 0.8783, val_top_5: 0.9908\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6366, val_acc: 0.8788, val_top_1: 0.8788, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6456, val_acc: 0.8776, val_top_1: 0.8776, val_top_5: 0.9929\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.6392, val_acc: 0.8796, val_top_1: 0.8796, val_top_5: 0.9921\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6543, val_acc: 0.8792, val_top_1: 0.8792, val_top_5: 0.9899\n",
      "wt desc =  None\n",
      "Compression= 0.9708644680798477 Result after pruning is  {'val_loss': 0.7128380537033081, 'val_acc': 0.869335949420929, 'val_top_1': 0.869335949420929, 'val_top_5': 0.989453136920929}\n",
      "Mask compression =  3.9 tensor(0.9745)\n",
      "After pruning, Compression= 0.9726481080234962 Result after pruning is  {'val_loss': 1.6833877563476562, 'val_acc': 0.758007824420929, 'val_top_1': 0.758007824420929, 'val_top_5': 0.9693359136581421}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.6185, val_acc: 0.8760, val_top_1: 0.8760, val_top_5: 0.9904\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.6178, val_acc: 0.8751, val_top_1: 0.8751, val_top_5: 0.9909\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.6407, val_acc: 0.8746, val_top_1: 0.8746, val_top_5: 0.9901\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.6106, val_acc: 0.8791, val_top_1: 0.8791, val_top_5: 0.9910\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.6381, val_acc: 0.8742, val_top_1: 0.8742, val_top_5: 0.9908\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.6058, val_acc: 0.8804, val_top_1: 0.8804, val_top_5: 0.9908\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.6095, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.6141, val_acc: 0.8798, val_top_1: 0.8798, val_top_5: 0.9909\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.6227, val_acc: 0.8805, val_top_1: 0.8805, val_top_5: 0.9908\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.6058, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9908\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.6040, val_acc: 0.8797, val_top_1: 0.8797, val_top_5: 0.9915\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.6087, val_acc: 0.8794, val_top_1: 0.8794, val_top_5: 0.9914\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.6207, val_acc: 0.8815, val_top_1: 0.8815, val_top_5: 0.9906\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], val_loss: 0.6244, val_acc: 0.8741, val_top_1: 0.8741, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.6113, val_acc: 0.8805, val_top_1: 0.8805, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.6114, val_acc: 0.8809, val_top_1: 0.8809, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6179, val_acc: 0.8821, val_top_1: 0.8821, val_top_5: 0.9910\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.6079, val_acc: 0.8830, val_top_1: 0.8830, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6216, val_acc: 0.8763, val_top_1: 0.8763, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6196, val_acc: 0.8774, val_top_1: 0.8774, val_top_5: 0.9938\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6430, val_acc: 0.8735, val_top_1: 0.8735, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6378, val_acc: 0.8742, val_top_1: 0.8742, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6136, val_acc: 0.8805, val_top_1: 0.8805, val_top_5: 0.9913\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.6213, val_acc: 0.8819, val_top_1: 0.8819, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6262, val_acc: 0.8765, val_top_1: 0.8765, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6167, val_acc: 0.8831, val_top_1: 0.8831, val_top_5: 0.9928\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6215, val_acc: 0.8820, val_top_1: 0.8820, val_top_5: 0.9908\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6354, val_acc: 0.8792, val_top_1: 0.8792, val_top_5: 0.9914\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6318, val_acc: 0.8805, val_top_1: 0.8805, val_top_5: 0.9916\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6243, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9929\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6259, val_acc: 0.8812, val_top_1: 0.8812, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6628, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9914\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6471, val_acc: 0.8785, val_top_1: 0.8785, val_top_5: 0.9912\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.6378, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9910\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6399, val_acc: 0.8817, val_top_1: 0.8817, val_top_5: 0.9911\n",
      "wt desc =  None\n",
      "Compression= 0.9726481080234962 Result after pruning is  {'val_loss': 0.6957920789718628, 'val_acc': 0.8711913824081421, 'val_top_1': 0.8711913824081421, 'val_top_5': 0.9911133050918579}\n",
      "Mask compression =  4 tensor(0.9759)\n",
      "After pruning, Compression= 0.9740692530132393 Result after pruning is  {'val_loss': 0.9584047198295593, 'val_acc': 0.8231445550918579, 'val_top_1': 0.8231445550918579, 'val_top_5': 0.9906250238418579}\n",
      "At train\n",
      "Epoch [0], val_loss: 0.6252, val_acc: 0.8725, val_top_1: 0.8725, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 0.6174, val_acc: 0.8775, val_top_1: 0.8775, val_top_5: 0.9924\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 0.6199, val_acc: 0.8798, val_top_1: 0.8798, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 0.6196, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 0.6128, val_acc: 0.8766, val_top_1: 0.8766, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 0.6125, val_acc: 0.8766, val_top_1: 0.8766, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 0.6201, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 0.6102, val_acc: 0.8775, val_top_1: 0.8775, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 0.6125, val_acc: 0.8779, val_top_1: 0.8779, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 0.6165, val_acc: 0.8781, val_top_1: 0.8781, val_top_5: 0.9926\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 0.6153, val_acc: 0.8804, val_top_1: 0.8804, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 0.6144, val_acc: 0.8773, val_top_1: 0.8773, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 0.6197, val_acc: 0.8791, val_top_1: 0.8791, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 0.6154, val_acc: 0.8773, val_top_1: 0.8773, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 0.6169, val_acc: 0.8781, val_top_1: 0.8781, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 0.6164, val_acc: 0.8795, val_top_1: 0.8795, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 0.6231, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9930\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 0.6224, val_acc: 0.8758, val_top_1: 0.8758, val_top_5: 0.9937\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 0.6218, val_acc: 0.8765, val_top_1: 0.8765, val_top_5: 0.9935\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 0.6397, val_acc: 0.8794, val_top_1: 0.8794, val_top_5: 0.9921\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 0.6237, val_acc: 0.8782, val_top_1: 0.8782, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 0.6630, val_acc: 0.8779, val_top_1: 0.8779, val_top_5: 0.9918\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 0.6342, val_acc: 0.8757, val_top_1: 0.8757, val_top_5: 0.9936\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 0.6310, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9932\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 0.6463, val_acc: 0.8708, val_top_1: 0.8708, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 0.6349, val_acc: 0.8779, val_top_1: 0.8779, val_top_5: 0.9927\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 0.6358, val_acc: 0.8749, val_top_1: 0.8749, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 0.6321, val_acc: 0.8810, val_top_1: 0.8810, val_top_5: 0.9934\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 0.6305, val_acc: 0.8792, val_top_1: 0.8792, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 0.6565, val_acc: 0.8771, val_top_1: 0.8771, val_top_5: 0.9929\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 0.6350, val_acc: 0.8763, val_top_1: 0.8763, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 0.6379, val_acc: 0.8746, val_top_1: 0.8746, val_top_5: 0.9929\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 0.6755, val_acc: 0.8683, val_top_1: 0.8683, val_top_5: 0.9944\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 0.6440, val_acc: 0.8770, val_top_1: 0.8770, val_top_5: 0.9931\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 0.6373, val_acc: 0.8786, val_top_1: 0.8786, val_top_5: 0.9933\n",
      "wt desc =  None\n",
      "Compression= 0.9740692530132393 Result after pruning is  {'val_loss': 0.7039856910705566, 'val_acc': 0.8685547113418579, 'val_top_1': 0.8685547113418579, 'val_top_5': 0.992480456829071}\n",
      "Mask compression =  6 tensor(0.9878)\n",
      "After pruning, Compression= 0.9858997701452451 Result after pruning is  {'val_loss': 8.507152557373047, 'val_acc': 0.24980469048023224, 'val_top_1': 0.24980469048023224, 'val_top_5': 0.567187488079071}\n",
      "At train\n",
      "Epoch [0], val_loss: 1.5317, val_acc: 0.4782, val_top_1: 0.4782, val_top_5: 0.8350\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 1.4174, val_acc: 0.5024, val_top_1: 0.5024, val_top_5: 0.9249\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.3489, val_acc: 0.5119, val_top_1: 0.5119, val_top_5: 0.9438\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.2989, val_acc: 0.5200, val_top_1: 0.5200, val_top_5: 0.9503\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.2637, val_acc: 0.5234, val_top_1: 0.5234, val_top_5: 0.9527\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.2327, val_acc: 0.5324, val_top_1: 0.5324, val_top_5: 0.9572\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.2066, val_acc: 0.5376, val_top_1: 0.5376, val_top_5: 0.9593\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.1858, val_acc: 0.5456, val_top_1: 0.5456, val_top_5: 0.9619\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.1685, val_acc: 0.5480, val_top_1: 0.5480, val_top_5: 0.9628\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.1509, val_acc: 0.5543, val_top_1: 0.5543, val_top_5: 0.9634\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.1364, val_acc: 0.5570, val_top_1: 0.5570, val_top_5: 0.9656\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.1250, val_acc: 0.5601, val_top_1: 0.5601, val_top_5: 0.9659\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.1139, val_acc: 0.5639, val_top_1: 0.5639, val_top_5: 0.9677\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.1038, val_acc: 0.5639, val_top_1: 0.5639, val_top_5: 0.9665\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.0960, val_acc: 0.5899, val_top_1: 0.5899, val_top_5: 0.9692\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.0865, val_acc: 0.5506, val_top_1: 0.5506, val_top_5: 0.9697\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.0804, val_acc: 0.5922, val_top_1: 0.5922, val_top_5: 0.9688\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], val_loss: 1.0729, val_acc: 0.5916, val_top_1: 0.5916, val_top_5: 0.9712\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.0657, val_acc: 0.5489, val_top_1: 0.5489, val_top_5: 0.9706\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.0604, val_acc: 0.5480, val_top_1: 0.5480, val_top_5: 0.9711\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.0558, val_acc: 0.5521, val_top_1: 0.5521, val_top_5: 0.9701\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.0507, val_acc: 0.5496, val_top_1: 0.5496, val_top_5: 0.9722\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.0447, val_acc: 0.5517, val_top_1: 0.5517, val_top_5: 0.9718\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.0418, val_acc: 0.5507, val_top_1: 0.5507, val_top_5: 0.9733\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.0386, val_acc: 0.5497, val_top_1: 0.5497, val_top_5: 0.9744\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.0325, val_acc: 0.5521, val_top_1: 0.5521, val_top_5: 0.9741\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.0302, val_acc: 0.5490, val_top_1: 0.5490, val_top_5: 0.9750\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.0312, val_acc: 0.5476, val_top_1: 0.5476, val_top_5: 0.9740\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.0244, val_acc: 0.5504, val_top_1: 0.5504, val_top_5: 0.9744\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.0188, val_acc: 0.5534, val_top_1: 0.5534, val_top_5: 0.9743\n",
      "wt desc =  None\n",
      "Epoch [30], val_loss: 1.0166, val_acc: 0.5549, val_top_1: 0.5549, val_top_5: 0.9747\n",
      "wt desc =  None\n",
      "Epoch [31], val_loss: 1.0133, val_acc: 0.5523, val_top_1: 0.5523, val_top_5: 0.9756\n",
      "wt desc =  None\n",
      "Epoch [32], val_loss: 1.0137, val_acc: 0.5965, val_top_1: 0.5965, val_top_5: 0.9754\n",
      "wt desc =  None\n",
      "Epoch [33], val_loss: 1.0101, val_acc: 0.5953, val_top_1: 0.5953, val_top_5: 0.9730\n",
      "wt desc =  None\n",
      "Epoch [34], val_loss: 1.0067, val_acc: 0.5556, val_top_1: 0.5556, val_top_5: 0.9742\n",
      "wt desc =  None\n",
      "Compression= 0.9858997701452451 Result after pruning is  {'val_loss': 1.037490963935852, 'val_acc': 0.551464855670929, 'val_top_1': 0.551464855670929, 'val_top_5': 0.9715820550918579}\n"
     ]
    }
   ],
   "source": [
    "model_state_path=\"model_state/mod.pt\"\n",
    "model.load_state_dict(torch.load(model_state_path))\n",
    "\n",
    "result = evaluate(model, test_loader)\n",
    "print(\"Test result is \",result)\n",
    "\n",
    "\n",
    "total_size,nz_size=model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "print(\"Compression=\",compression)\n",
    "\n",
    "\n",
    "\n",
    "prune_rate=3.403 # compression of 0.9627\n",
    "\n",
    "\n",
    "metrics={}\n",
    "metrics[\"prune_rate\"]=[]\n",
    "metrics[\"compression\"]=[]\n",
    "metrics[\"epochs\"]=[]\n",
    "metrics[\"top_5\"]=[]\n",
    "metrics[\"top_1\"]=[]\n",
    "\n",
    "\n",
    "\n",
    "prune_rate_range=[0.1,0.3,0.4,0.8,1.1,1.3,1.5,1.7,\n",
    "                  1.8,2.1,2.4,2.9,3.1,3.15,3.12,3.35,3.4,3.5,3.6,3.7,3.8,3.9,4,6]\n",
    "\n",
    "# prune_rate_range=[0.1,1.5,3.9,4,6]\n",
    "\n",
    "for prune_rate in prune_rate_range:\n",
    "    mask_whole_model=prune_model_get_mask(model,prune_rate)\n",
    "\n",
    "    print(\"Mask compression = \",prune_rate,get_mask_compression(mask_whole_model))\n",
    "\n",
    "\n",
    "    apply_mask_model(model,mask_whole_model)\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"After pruning, Compression=\",compression,\"Result after pruning is \",res)\n",
    "\n",
    "\n",
    "    epochs=35\n",
    "    history_prune,_ = fit(epochs, lr, model, train_loader, val_loader,\n",
    "                          mask_whole_model=mask_whole_model)\n",
    "\n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"Compression=\",compression,\"Result after pruning is \",res)\n",
    "    metrics[\"prune_rate\"].append(prune_rate)\n",
    "    metrics[\"compression\"].append(compression)\n",
    "    metrics[\"epochs\"].append(epochs)\n",
    "    metrics[\"top_5\"].append(res['val_top_5'])\n",
    "    metrics[\"top_1\"].append(res['val_top_1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prune_rate  compression  epochs     top_5     top_1\n",
      "0         0.1     0.034977      35  0.996289  0.870215\n",
      "1         0.3     0.124080      35  0.997168  0.878223\n",
      "2         0.4     0.196468      35  0.996875  0.877930\n",
      "3         0.8     0.382292      35  0.996094  0.881836\n",
      "4         1.1     0.565377      35  0.996094  0.882617\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe_results=pd.DataFrame(metrics)\n",
    "print(dataframe_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_results.to_csv(\"results_sheet/song_hn.csv\",\n",
    "                         index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression= 0.9588732997750884 Result after pruning is  {'val_loss': 0.47730427980422974, 'val_acc': 0.841601550579071, 'val_top_1': 0.841601550579071, 'val_top_5': 0.9935547113418579}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlkernel",
   "language": "python",
   "name": "dlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
