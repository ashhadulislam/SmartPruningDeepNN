{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "    \n",
    "\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def countDiffMasks(mask1,mask2):\n",
    "    total_diff=0\n",
    "    for i in range(len(mask1)):\n",
    "        m_1=mask1[i].flatten()\n",
    "        m_2=mask2[i].flatten()\n",
    "        count_same=(m_1 == m_2).sum()\n",
    "        count_different=m_1.flatten().shape[0]-count_same\n",
    "        total_diff+=count_different\n",
    "    return total_diff\n",
    "\n",
    "\n",
    "def get_mask_compression(mask_whole_model):\n",
    "    num_total=0\n",
    "    num_non_zeros=0\n",
    "    for mask_each_layer in mask_whole_model:\n",
    "        num_total+=torch.numel(mask_each_layer)\n",
    "        num_non_zeros+=torch.count_nonzero(mask_each_layer)\n",
    "        \n",
    "    return (num_total-num_non_zeros)/num_total\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def prune_model_get_mask(model,prune_rate):\n",
    "    '''\n",
    "    works purely on the model to get\n",
    "    mask\n",
    "    '''\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.std(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "            threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    " \n",
    "    \n",
    "def get_thresholds_each_layer(model,prune_rate):\n",
    "    thresholds_per_layer=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "            abs_std=torch.std(torch.abs(params.data))\n",
    "            threshold=abs_std*prune_rate\n",
    "            thresholds_per_layer.append(threshold)\n",
    "    return thresholds_per_layer\n",
    "    \n",
    "                \n",
    "def apply_mask_model(model,list_mask_whole_model):\n",
    "    mask_layer_count=0\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=list_mask_whole_model[mask_layer_count]\n",
    "            with torch.no_grad():\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#                 print(\"Devices are \",params.device,mask_layer.device)\n",
    "                mask_layer=mask_layer.to(device)\n",
    "    \n",
    "                params.data=params.data*mask_layer            \n",
    "            mask_layer_count+=1\n",
    "    \n",
    "\n",
    "def store_weights_in_dic(weight_description,model):\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            if nm not in weight_description:\n",
    "                weight_description[nm]={}\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                if index_component not in weight_description[nm]:\n",
    "                    weight_description[nm][index_component]={}\n",
    "                values=params[index_component]\n",
    "                flat_values=values.flatten()\n",
    "                for index_wt in range(flat_values.shape[0]):\n",
    "                    if index_wt not in weight_description[nm][index_component]:\n",
    "                        weight_description[nm][index_component][index_wt]=[]\n",
    "                    weight_description[nm][index_component][index_wt].append(flat_values[index_wt].detach().item())\n",
    "    return weight_description\n",
    "\n",
    "\n",
    "def get_boolean_dict_weight_dict(weight_description,prune_rate,thresholds_per_layer):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of 1s and 0s to show\n",
    "    how many times weight is more than threshold\n",
    "    per layer\n",
    "    '''\n",
    "    boolean_weight_description={}\n",
    "    count=0\n",
    "    for layer in weight_description.keys():  \n",
    "#         print(\"Count = \",count)\n",
    "        threshold_this_layer=thresholds_per_layer[count]\n",
    "#         print(\"Threshold for layer \",count,layer,\"is \",threshold_this_layer)\n",
    "        if layer not in boolean_weight_description:\n",
    "            boolean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in boolean_weight_description[layer]:\n",
    "                boolean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in boolean_weight_description[layer][index_component]:\n",
    "                    boolean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt]\n",
    "                all_wts_boolean=[]\n",
    "                for wt in all_wts:\n",
    "                    if abs(wt)>threshold_this_layer:\n",
    "                        all_wts_boolean.append(1)\n",
    "                    else:\n",
    "                        all_wts_boolean.append(0)\n",
    "                boolean_weight_description[layer][index_component][index_wt]=all_wts_boolean                    \n",
    "        count+=1\n",
    "        \n",
    "    return boolean_weight_description\n",
    "    \n",
    "# create mask from boolean weight dictionary\n",
    "def create_mask_from_boolean_wt(model,boolean_wt_dict):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.var(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "#             threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten() \n",
    "                mask_vals=[]\n",
    "                for val_index in range(re_shaped_values.shape[0]):\n",
    "                    boolean_vals=boolean_wt_dict[nm][index_component][val_index]\n",
    "                    m = stats.mode(boolean_vals)\n",
    "#                     print(\"Verdict for this weight is \",m[0][0])\n",
    "                    mask_vals.append(m[0][0])\n",
    "#                 mask_vals = (torch.abs(re_shaped_values)>threshold).float()                \n",
    "                mask_vals=np.asarray(mask_vals)\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=torch.from_numpy(mask_vals)\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part copied from shrinkbench\n",
    "\n",
    "def nonzero(tensor):\n",
    "    \"\"\"Returns absolute number of values different from 0\n",
    "\n",
    "    Arguments:\n",
    "        tensor {numpy.ndarray} -- Array to compute over\n",
    "\n",
    "    Returns:\n",
    "        int -- Number of nonzero elements\n",
    "    \"\"\"\n",
    "    return np.sum(tensor != 0.0)\n",
    "\n",
    "\n",
    "def model_size(model, as_bits=False):\n",
    "    \"\"\"Returns absolute and nonzero model size\n",
    "\n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Network to compute model size over\n",
    "\n",
    "    Keyword Arguments:\n",
    "        as_bits {bool} -- Whether to account for the size of dtype\n",
    "\n",
    "    Returns:\n",
    "        int -- Total number of weight & bias params\n",
    "        int -- Out total_params exactly how many are nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for tensor in model.parameters():\n",
    "        t = np.prod(tensor.shape)\n",
    "        nz = nonzero(tensor.detach().cpu().numpy())\n",
    "        if as_bits:\n",
    "            bits = dtype2bits[tensor.dtype]\n",
    "            t *= bits\n",
    "            nz *= bits\n",
    "        total_params += t\n",
    "        nonzero_params += nz\n",
    "    return int(total_params), int(nonzero_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))        \n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes how many correct outputs with respect to targets\n",
    "\n",
    "    Does NOT compute accuracy but just a raw amount of correct\n",
    "    outputs given target labels. This is done for each value in\n",
    "    topk. A value is considered correct if target is in the topk\n",
    "    highest values of output.\n",
    "    The values returned are upperbounded by the given batch size\n",
    "\n",
    "    [description]\n",
    "\n",
    "    Arguments:\n",
    "        output {torch.Tensor} -- Output prediction of the model\n",
    "        target {torch.Tensor} -- Target labels from data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "    Returns:\n",
    "        List(int) -- Number of correct values for each topk\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        # Only need to do topk for highest k, reuse for the rest\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(torch.tensor(correct_k.item()))\n",
    "        return res\n",
    "\n",
    "\n",
    "# below copied from shrinkbench, to be used later\n",
    "# def accuracy(model, dataloader, topk=(1,)):\n",
    "#     \"\"\"Compute accuracy of a model over a dataloader for various topk\n",
    "\n",
    "#     Arguments:\n",
    "#         model {torch.nn.Module} -- Network to evaluate\n",
    "#         dataloader {torch.utils.data.DataLoader} -- Data to iterate over\n",
    "\n",
    "#     Keyword Arguments:\n",
    "#         topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "#     Returns:\n",
    "#         List(float) -- List of accuracies for each topk\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Use same device as model\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     accs = np.zeros(len(topk))\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for i, (input, target) in enumerate(dataloader):\n",
    "#             input = input.to(device)\n",
    "#             target = target.to(device)\n",
    "#             output = model(input)\n",
    "\n",
    "#             accs += np.array(correct(output, target, topk))\n",
    "\n",
    "#     # Normalize over data length\n",
    "#     accs /= len(dataloader.dataset)\n",
    "\n",
    "#     return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LENET 300-100 for MNIST and comparison\n",
    "class FashionMnistNet(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)        \n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.fc4.is_classifier = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))        \n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        top_1, top_5 = correct(out, labels,topk=(1,5))\n",
    "#         print(\"Batch is \",batch[1].shape)\n",
    "        \n",
    "        top_1=top_1/batch[1].shape[0]\n",
    "        top_5=top_5/batch[1].shape[0]\n",
    "\n",
    "#         print(\"corr\",top_1,top_5)\n",
    "#         return {'val_loss': loss, 'val_acc': acc}\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'top_1': top_1, 'top_5': top_5}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        \n",
    "        batch_top_1s = [x['top_1'] for x in outputs]\n",
    "#         print(batch_top_1s)\n",
    "        epoch_top_1 = torch.stack(batch_top_1s).mean()      # Combine top_1\n",
    "        \n",
    "        batch_top_5s = [x['top_5'] for x in outputs]\n",
    "        epoch_top_5 = torch.stack(batch_top_5s).mean()      # Combine top_5\n",
    "        \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),\n",
    "               'val_top_1': epoch_top_1.item(), 'val_top_5': epoch_top_5.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_top_1: {:.4f}, val_top_5: {:.4f}\".format(\n",
    "                                epoch, result['val_loss'], result['val_acc'], \n",
    "                                result['val_top_1'], result['val_top_5']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     print(\"outputs are \",outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD,\n",
    "        weight_description=None,mask_whole_model=None,\n",
    "       model_state_path=None):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    print(\"At train\")\n",
    "    history = []\n",
    "    best_so_far=-999        \n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if mask_whole_model:\n",
    "#                 print(\"Applying mask\")\n",
    "                apply_mask_model(model,mask_whole_model)\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        if best_so_far<result[\"val_top_1\"]:\n",
    "            best_so_far=result[\"val_top_1\"]\n",
    "            if model_state_path:\n",
    "                torch.save(model.state_dict(), model_state_path)\n",
    "        \n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        print(\"wt desc = \",weight_description)\n",
    "        if weight_description!=None:\n",
    "            print(\"going for weight\")\n",
    "            weight_description=store_weights_in_dic(weight_description,model)\n",
    "    return history, weight_description\n",
    "\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program with Song Han fashionmnist\n",
      "Torch cuda  False\n",
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Program with Song Han fashionmnist\")\n",
    "\n",
    "print(\"Torch cuda \",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(\"device \",device)\n",
    "\n",
    "\n",
    "\n",
    "dataset = FashionMNIST(root='data/', download=True, transform=ToTensor())\n",
    "\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = FashionMNIST(root='data/', train=False,transform=ToTensor())\n",
    "\n",
    "val_size = 10000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "shape=dataset[0][0].shape\n",
    "input_size=1\n",
    "for s in shape:\n",
    "    input_size*=s\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3293965/3744492648.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_data=torch.tensor(train_ds.dataset.data)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "targets=train_ds.dataset.targets\n",
    "training_data=torch.tensor(train_ds.dataset.data)\n",
    "\n",
    "training_data = training_data.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial result [{'val_loss': 2.3034565448760986, 'val_acc': 0.06425781548023224, 'val_top_1': 0.06425781548023224, 'val_top_5': 0.5044921636581421}]\n"
     ]
    }
   ],
   "source": [
    "model=FashionMnistNet()\n",
    "\n",
    "history = [evaluate(model, val_loader)]\n",
    "print(\"initial result\",history)\n",
    "# weight_description={}\n",
    "epochs=20\n",
    "lr=0.01\n",
    "# history2,weight_description = fit(epochs, lr, model, train_loader, val_loader,weight_description=weight_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  4.51 tensor(0.9918)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2247, val_acc: 0.1933, val_top_1: 0.1933, val_top_5: 0.5932\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.1195, val_acc: 0.2677, val_top_1: 0.2677, val_top_5: 0.7926\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.9755, val_acc: 0.2396, val_top_1: 0.2396, val_top_5: 0.8080\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.8737, val_acc: 0.2556, val_top_1: 0.2556, val_top_5: 0.8263\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.8006, val_acc: 0.3021, val_top_1: 0.3021, val_top_5: 0.8563\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.7427, val_acc: 0.3485, val_top_1: 0.3485, val_top_5: 0.8680\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.6945, val_acc: 0.3612, val_top_1: 0.3612, val_top_5: 0.8693\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.6547, val_acc: 0.4254, val_top_1: 0.4254, val_top_5: 0.8568\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.6211, val_acc: 0.4434, val_top_1: 0.4434, val_top_5: 0.8579\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.5938, val_acc: 0.4414, val_top_1: 0.4414, val_top_5: 0.8624\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.5705, val_acc: 0.4500, val_top_1: 0.4500, val_top_5: 0.8614\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.5506, val_acc: 0.4574, val_top_1: 0.4574, val_top_5: 0.8616\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.5338, val_acc: 0.4551, val_top_1: 0.4551, val_top_5: 0.8657\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.5183, val_acc: 0.4657, val_top_1: 0.4657, val_top_5: 0.8468\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.5048, val_acc: 0.4710, val_top_1: 0.4710, val_top_5: 0.8490\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.4931, val_acc: 0.4714, val_top_1: 0.4714, val_top_5: 0.8480\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.4816, val_acc: 0.4744, val_top_1: 0.4744, val_top_5: 0.8569\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.4706, val_acc: 0.4796, val_top_1: 0.4796, val_top_5: 0.8549\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.4588, val_acc: 0.4811, val_top_1: 0.4811, val_top_5: 0.8608\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.4484, val_acc: 0.4816, val_top_1: 0.4816, val_top_5: 0.9139\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.4383, val_acc: 0.4798, val_top_1: 0.4798, val_top_5: 0.8674\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4297, val_acc: 0.4841, val_top_1: 0.4841, val_top_5: 0.9160\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4213, val_acc: 0.4838, val_top_1: 0.4838, val_top_5: 0.9176\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4143, val_acc: 0.4839, val_top_1: 0.4839, val_top_5: 0.8639\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4067, val_acc: 0.4854, val_top_1: 0.4854, val_top_5: 0.8701\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4008, val_acc: 0.4854, val_top_1: 0.4854, val_top_5: 0.8717\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.3949, val_acc: 0.4850, val_top_1: 0.4850, val_top_5: 0.8727\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.3886, val_acc: 0.4869, val_top_1: 0.4869, val_top_5: 0.8762\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.3845, val_acc: 0.4879, val_top_1: 0.4879, val_top_5: 0.8711\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.3789, val_acc: 0.4894, val_top_1: 0.4894, val_top_5: 0.8758\n",
      "wt desc =  None\n",
      "Compression= 0.9898789761165256 Result after pruning and retraining is  {'val_loss': 1.3647205829620361, 'val_acc': 0.503222644329071, 'val_top_1': 0.503222644329071, 'val_top_5': 0.8817383050918579}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  4.53 tensor(0.9919)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2245, val_acc: 0.1932, val_top_1: 0.1932, val_top_5: 0.5955\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.1187, val_acc: 0.2677, val_top_1: 0.2677, val_top_5: 0.7923\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.9749, val_acc: 0.2398, val_top_1: 0.2398, val_top_5: 0.8076\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.8746, val_acc: 0.2570, val_top_1: 0.2570, val_top_5: 0.8242\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.8018, val_acc: 0.3136, val_top_1: 0.3136, val_top_5: 0.8612\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.7446, val_acc: 0.3415, val_top_1: 0.3415, val_top_5: 0.8673\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.6965, val_acc: 0.3683, val_top_1: 0.3683, val_top_5: 0.8675\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.6564, val_acc: 0.4363, val_top_1: 0.4363, val_top_5: 0.8514\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.6232, val_acc: 0.4427, val_top_1: 0.4427, val_top_5: 0.8526\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.5955, val_acc: 0.4520, val_top_1: 0.4520, val_top_5: 0.8577\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.5724, val_acc: 0.4482, val_top_1: 0.4482, val_top_5: 0.8388\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.5527, val_acc: 0.4543, val_top_1: 0.4543, val_top_5: 0.8405\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.5356, val_acc: 0.4623, val_top_1: 0.4623, val_top_5: 0.8447\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.5207, val_acc: 0.4611, val_top_1: 0.4611, val_top_5: 0.8434\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.5069, val_acc: 0.4697, val_top_1: 0.4697, val_top_5: 0.8479\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.4949, val_acc: 0.4718, val_top_1: 0.4718, val_top_5: 0.8481\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.4837, val_acc: 0.4738, val_top_1: 0.4738, val_top_5: 0.8535\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.4739, val_acc: 0.4782, val_top_1: 0.4782, val_top_5: 0.8497\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.4631, val_acc: 0.4783, val_top_1: 0.4783, val_top_5: 0.8583\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.4520, val_acc: 0.4792, val_top_1: 0.4792, val_top_5: 0.8609\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.4417, val_acc: 0.4818, val_top_1: 0.4818, val_top_5: 0.8630\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4329, val_acc: 0.4840, val_top_1: 0.4840, val_top_5: 0.8627\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4249, val_acc: 0.4839, val_top_1: 0.4839, val_top_5: 0.8657\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4172, val_acc: 0.4833, val_top_1: 0.4833, val_top_5: 0.8650\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4099, val_acc: 0.4830, val_top_1: 0.4830, val_top_5: 0.8643\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4042, val_acc: 0.4847, val_top_1: 0.4847, val_top_5: 0.8727\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.3981, val_acc: 0.4828, val_top_1: 0.4828, val_top_5: 0.8789\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.3918, val_acc: 0.4851, val_top_1: 0.4851, val_top_5: 0.8785\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.3871, val_acc: 0.4880, val_top_1: 0.4880, val_top_5: 0.9204\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.3814, val_acc: 0.4866, val_top_1: 0.4866, val_top_5: 0.8780\n",
      "wt desc =  None\n",
      "Compression= 0.9900684621151581 Result after pruning and retraining is  {'val_loss': 1.3674055337905884, 'val_acc': 0.5029296875, 'val_top_1': 0.5029296875, 'val_top_5': 0.882519543170929}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  4.55 tensor(0.9921)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2285, val_acc: 0.1934, val_top_1: 0.1934, val_top_5: 0.5873\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.1269, val_acc: 0.2676, val_top_1: 0.2676, val_top_5: 0.7894\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.9850, val_acc: 0.2387, val_top_1: 0.2387, val_top_5: 0.8035\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.8838, val_acc: 0.2500, val_top_1: 0.2500, val_top_5: 0.8265\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.8101, val_acc: 0.2970, val_top_1: 0.2970, val_top_5: 0.8587\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.7522, val_acc: 0.3396, val_top_1: 0.3396, val_top_5: 0.8686\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.7035, val_acc: 0.3558, val_top_1: 0.3558, val_top_5: 0.8695\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.6630, val_acc: 0.4187, val_top_1: 0.4187, val_top_5: 0.8560\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.6284, val_acc: 0.4440, val_top_1: 0.4440, val_top_5: 0.8566\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.6000, val_acc: 0.4510, val_top_1: 0.4510, val_top_5: 0.8572\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.5761, val_acc: 0.4487, val_top_1: 0.4487, val_top_5: 0.8603\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.5561, val_acc: 0.4556, val_top_1: 0.4556, val_top_5: 0.8605\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], val_loss: 1.5385, val_acc: 0.4556, val_top_1: 0.4556, val_top_5: 0.8412\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.5228, val_acc: 0.4670, val_top_1: 0.4670, val_top_5: 0.8484\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.5097, val_acc: 0.4655, val_top_1: 0.4655, val_top_5: 0.8649\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.4972, val_acc: 0.4720, val_top_1: 0.4720, val_top_5: 0.8631\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.4861, val_acc: 0.4759, val_top_1: 0.4759, val_top_5: 0.8504\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.4760, val_acc: 0.4757, val_top_1: 0.4757, val_top_5: 0.8546\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.4652, val_acc: 0.4783, val_top_1: 0.4783, val_top_5: 0.8586\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.4548, val_acc: 0.4804, val_top_1: 0.4804, val_top_5: 0.8564\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.4444, val_acc: 0.4825, val_top_1: 0.4825, val_top_5: 0.8572\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4345, val_acc: 0.4797, val_top_1: 0.4797, val_top_5: 0.8623\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4261, val_acc: 0.4843, val_top_1: 0.4843, val_top_5: 0.8625\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4186, val_acc: 0.4812, val_top_1: 0.4812, val_top_5: 0.8686\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4110, val_acc: 0.4825, val_top_1: 0.4825, val_top_5: 0.8694\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4051, val_acc: 0.4856, val_top_1: 0.4856, val_top_5: 0.8725\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.3984, val_acc: 0.4856, val_top_1: 0.4856, val_top_5: 0.8710\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.3927, val_acc: 0.4845, val_top_1: 0.4845, val_top_5: 0.8736\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.3874, val_acc: 0.4869, val_top_1: 0.4869, val_top_5: 0.8743\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.3828, val_acc: 0.4873, val_top_1: 0.4873, val_top_5: 0.8754\n",
      "wt desc =  None\n",
      "Compression= 0.9902703058963099 Result after pruning and retraining is  {'val_loss': 1.3688132762908936, 'val_acc': 0.4991210997104645, 'val_top_1': 0.4991210997104645, 'val_top_5': 0.880566418170929}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  4.57 tensor(0.9924)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2365, val_acc: 0.1935, val_top_1: 0.1935, val_top_5: 0.5783\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.1541, val_acc: 0.1957, val_top_1: 0.1957, val_top_5: 0.7951\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.0368, val_acc: 0.2570, val_top_1: 0.2570, val_top_5: 0.7999\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.9103, val_acc: 0.2495, val_top_1: 0.2495, val_top_5: 0.8162\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.8277, val_acc: 0.2912, val_top_1: 0.2912, val_top_5: 0.8496\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.7661, val_acc: 0.3438, val_top_1: 0.3438, val_top_5: 0.8671\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.7158, val_acc: 0.3569, val_top_1: 0.3569, val_top_5: 0.8706\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.6730, val_acc: 0.4286, val_top_1: 0.4286, val_top_5: 0.8526\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.6377, val_acc: 0.4312, val_top_1: 0.4312, val_top_5: 0.8619\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.6081, val_acc: 0.4480, val_top_1: 0.4480, val_top_5: 0.8599\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.5831, val_acc: 0.4511, val_top_1: 0.4511, val_top_5: 0.8387\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.5621, val_acc: 0.4565, val_top_1: 0.4565, val_top_5: 0.8398\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.5440, val_acc: 0.4619, val_top_1: 0.4619, val_top_5: 0.8426\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.5286, val_acc: 0.4658, val_top_1: 0.4658, val_top_5: 0.8451\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.5146, val_acc: 0.4673, val_top_1: 0.4673, val_top_5: 0.8513\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.5020, val_acc: 0.4729, val_top_1: 0.4729, val_top_5: 0.8513\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.4910, val_acc: 0.4695, val_top_1: 0.4695, val_top_5: 0.8586\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.4799, val_acc: 0.4776, val_top_1: 0.4776, val_top_5: 0.8551\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.4691, val_acc: 0.4790, val_top_1: 0.4790, val_top_5: 0.8532\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.4578, val_acc: 0.4779, val_top_1: 0.4779, val_top_5: 0.8604\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.4478, val_acc: 0.4812, val_top_1: 0.4812, val_top_5: 0.8597\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4387, val_acc: 0.4817, val_top_1: 0.4817, val_top_5: 0.8629\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4307, val_acc: 0.4807, val_top_1: 0.4807, val_top_5: 0.8689\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4230, val_acc: 0.4833, val_top_1: 0.4833, val_top_5: 0.8653\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4164, val_acc: 0.4843, val_top_1: 0.4843, val_top_5: 0.8708\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4092, val_acc: 0.4831, val_top_1: 0.4831, val_top_5: 0.9224\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.4040, val_acc: 0.4839, val_top_1: 0.4839, val_top_5: 0.9221\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.3981, val_acc: 0.4864, val_top_1: 0.4864, val_top_5: 0.8728\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.3926, val_acc: 0.4843, val_top_1: 0.4843, val_top_5: 0.8763\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.3879, val_acc: 0.4862, val_top_1: 0.4862, val_top_5: 0.8779\n",
      "wt desc =  None\n",
      "Compression= 0.9905009845033407 Result after pruning and retraining is  {'val_loss': 1.3753347396850586, 'val_acc': 0.49951171875, 'val_top_1': 0.49951171875, 'val_top_5': 0.8846679925918579}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.01 tensor(0.9957)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2723, val_acc: 0.1857, val_top_1: 0.1857, val_top_5: 0.5627\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.2257, val_acc: 0.1877, val_top_1: 0.1877, val_top_5: 0.6806\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.1574, val_acc: 0.2256, val_top_1: 0.2256, val_top_5: 0.7021\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.0930, val_acc: 0.2274, val_top_1: 0.2274, val_top_5: 0.7008\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.0434, val_acc: 0.2275, val_top_1: 0.2275, val_top_5: 0.7209\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.0029, val_acc: 0.2436, val_top_1: 0.2436, val_top_5: 0.7855\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.9651, val_acc: 0.2457, val_top_1: 0.2457, val_top_5: 0.7850\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.9244, val_acc: 0.2491, val_top_1: 0.2491, val_top_5: 0.7721\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.8676, val_acc: 0.3075, val_top_1: 0.3075, val_top_5: 0.7813\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.7863, val_acc: 0.3559, val_top_1: 0.3559, val_top_5: 0.8063\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.7174, val_acc: 0.3685, val_top_1: 0.3685, val_top_5: 0.8357\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.6714, val_acc: 0.3854, val_top_1: 0.3854, val_top_5: 0.8561\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.6380, val_acc: 0.3895, val_top_1: 0.3895, val_top_5: 0.8144\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.6117, val_acc: 0.4022, val_top_1: 0.4022, val_top_5: 0.8166\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.5907, val_acc: 0.4082, val_top_1: 0.4082, val_top_5: 0.8176\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.5719, val_acc: 0.4124, val_top_1: 0.4124, val_top_5: 0.8192\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.5559, val_acc: 0.4164, val_top_1: 0.4164, val_top_5: 0.8207\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.5408, val_acc: 0.4238, val_top_1: 0.4238, val_top_5: 0.8215\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.5274, val_acc: 0.4249, val_top_1: 0.4249, val_top_5: 0.8231\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.5152, val_acc: 0.4293, val_top_1: 0.4293, val_top_5: 0.8223\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.5038, val_acc: 0.4297, val_top_1: 0.4297, val_top_5: 0.8232\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4941, val_acc: 0.4289, val_top_1: 0.4289, val_top_5: 0.8252\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4848, val_acc: 0.4309, val_top_1: 0.4309, val_top_5: 0.8240\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4763, val_acc: 0.4314, val_top_1: 0.4314, val_top_5: 0.8242\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4692, val_acc: 0.4312, val_top_1: 0.4312, val_top_5: 0.8255\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4621, val_acc: 0.4345, val_top_1: 0.4345, val_top_5: 0.8734\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26], val_loss: 1.4554, val_acc: 0.4348, val_top_1: 0.4348, val_top_5: 0.8238\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.4497, val_acc: 0.4345, val_top_1: 0.4345, val_top_5: 0.8274\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.4439, val_acc: 0.4358, val_top_1: 0.4358, val_top_5: 0.8252\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.4386, val_acc: 0.4358, val_top_1: 0.4358, val_top_5: 0.8274\n",
      "wt desc =  None\n",
      "Compression= 0.9938581820878062 Result after pruning and retraining is  {'val_loss': 1.424758791923523, 'val_acc': 0.4458984434604645, 'val_top_1': 0.4458984434604645, 'val_top_5': 0.82958984375}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.03 tensor(0.9958)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2765, val_acc: 0.1851, val_top_1: 0.1851, val_top_5: 0.5551\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.2352, val_acc: 0.1854, val_top_1: 0.1853, val_top_5: 0.6713\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.1722, val_acc: 0.2252, val_top_1: 0.2252, val_top_5: 0.7018\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.1042, val_acc: 0.2228, val_top_1: 0.2228, val_top_5: 0.7010\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.0538, val_acc: 0.2228, val_top_1: 0.2228, val_top_5: 0.7013\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.0103, val_acc: 0.2429, val_top_1: 0.2429, val_top_5: 0.7827\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.9736, val_acc: 0.2469, val_top_1: 0.2469, val_top_5: 0.7787\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.9383, val_acc: 0.2461, val_top_1: 0.2461, val_top_5: 0.7745\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.9005, val_acc: 0.2480, val_top_1: 0.2480, val_top_5: 0.7771\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.8499, val_acc: 0.3087, val_top_1: 0.3087, val_top_5: 0.7798\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.7765, val_acc: 0.3426, val_top_1: 0.3426, val_top_5: 0.8007\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.7099, val_acc: 0.3726, val_top_1: 0.3726, val_top_5: 0.8041\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.6629, val_acc: 0.3885, val_top_1: 0.3885, val_top_5: 0.8099\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.6291, val_acc: 0.3938, val_top_1: 0.3938, val_top_5: 0.8135\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.6032, val_acc: 0.4066, val_top_1: 0.4066, val_top_5: 0.8160\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.5811, val_acc: 0.4127, val_top_1: 0.4127, val_top_5: 0.8185\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.5623, val_acc: 0.4195, val_top_1: 0.4195, val_top_5: 0.8200\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.5455, val_acc: 0.4235, val_top_1: 0.4235, val_top_5: 0.8216\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.5310, val_acc: 0.4278, val_top_1: 0.4278, val_top_5: 0.8210\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.5179, val_acc: 0.4287, val_top_1: 0.4287, val_top_5: 0.8229\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.5064, val_acc: 0.4313, val_top_1: 0.4313, val_top_5: 0.8226\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4958, val_acc: 0.4325, val_top_1: 0.4325, val_top_5: 0.8763\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4867, val_acc: 0.4322, val_top_1: 0.4322, val_top_5: 0.8244\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4779, val_acc: 0.4321, val_top_1: 0.4321, val_top_5: 0.8252\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4699, val_acc: 0.4343, val_top_1: 0.4343, val_top_5: 0.8237\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4629, val_acc: 0.4350, val_top_1: 0.4350, val_top_5: 0.8245\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.4563, val_acc: 0.4346, val_top_1: 0.4346, val_top_5: 0.8262\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.4507, val_acc: 0.4371, val_top_1: 0.4371, val_top_5: 0.8225\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.4447, val_acc: 0.4370, val_top_1: 0.4370, val_top_5: 0.8761\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.4396, val_acc: 0.4373, val_top_1: 0.4373, val_top_5: 0.8765\n",
      "wt desc =  None\n",
      "Compression= 0.9939652828696419 Result after pruning and retraining is  {'val_loss': 1.4248692989349365, 'val_acc': 0.447265625, 'val_top_1': 0.447265625, 'val_top_5': 0.87890625}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.05 tensor(0.9959)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2776, val_acc: 0.1847, val_top_1: 0.1847, val_top_5: 0.5488\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.2373, val_acc: 0.1865, val_top_1: 0.1865, val_top_5: 0.6698\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.1760, val_acc: 0.2254, val_top_1: 0.2254, val_top_5: 0.7021\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.1080, val_acc: 0.2219, val_top_1: 0.2219, val_top_5: 0.7007\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.0574, val_acc: 0.2222, val_top_1: 0.2222, val_top_5: 0.7008\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.0143, val_acc: 0.2412, val_top_1: 0.2412, val_top_5: 0.7812\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.9779, val_acc: 0.2449, val_top_1: 0.2449, val_top_5: 0.7706\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.9430, val_acc: 0.2417, val_top_1: 0.2417, val_top_5: 0.7720\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.9060, val_acc: 0.2439, val_top_1: 0.2439, val_top_5: 0.7684\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.8573, val_acc: 0.3041, val_top_1: 0.3041, val_top_5: 0.7780\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.7845, val_acc: 0.3448, val_top_1: 0.3448, val_top_5: 0.7964\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.7147, val_acc: 0.3735, val_top_1: 0.3735, val_top_5: 0.8270\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.6654, val_acc: 0.3830, val_top_1: 0.3830, val_top_5: 0.8112\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.6310, val_acc: 0.3961, val_top_1: 0.3961, val_top_5: 0.8136\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.6040, val_acc: 0.4062, val_top_1: 0.4062, val_top_5: 0.8165\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.5823, val_acc: 0.4180, val_top_1: 0.4180, val_top_5: 0.8170\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.5629, val_acc: 0.4220, val_top_1: 0.4220, val_top_5: 0.8195\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.5459, val_acc: 0.4233, val_top_1: 0.4233, val_top_5: 0.8215\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.5311, val_acc: 0.4255, val_top_1: 0.4255, val_top_5: 0.8224\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.5180, val_acc: 0.4288, val_top_1: 0.4288, val_top_5: 0.8230\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.5071, val_acc: 0.4313, val_top_1: 0.4313, val_top_5: 0.8205\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4960, val_acc: 0.4319, val_top_1: 0.4319, val_top_5: 0.8238\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4870, val_acc: 0.4332, val_top_1: 0.4332, val_top_5: 0.8214\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4782, val_acc: 0.4327, val_top_1: 0.4327, val_top_5: 0.8250\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4704, val_acc: 0.4330, val_top_1: 0.4330, val_top_5: 0.8253\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4631, val_acc: 0.4344, val_top_1: 0.4344, val_top_5: 0.8256\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.4571, val_acc: 0.4337, val_top_1: 0.4337, val_top_5: 0.8282\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.4506, val_acc: 0.4367, val_top_1: 0.4367, val_top_5: 0.8233\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.4452, val_acc: 0.4360, val_top_1: 0.4360, val_top_5: 0.8261\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.4405, val_acc: 0.4379, val_top_1: 0.4379, val_top_5: 0.8221\n",
      "wt desc =  None\n",
      "Compression= 0.9940476680864385 Result after pruning and retraining is  {'val_loss': 1.4255492687225342, 'val_acc': 0.4471679627895355, 'val_top_1': 0.4471679627895355, 'val_top_5': 0.826464831829071}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.07 tensor(0.9960)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2788, val_acc: 0.1896, val_top_1: 0.1896, val_top_5: 0.5435\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.2392, val_acc: 0.1885, val_top_1: 0.1885, val_top_5: 0.6681\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.1785, val_acc: 0.2246, val_top_1: 0.2246, val_top_5: 0.7025\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.1094, val_acc: 0.2208, val_top_1: 0.2208, val_top_5: 0.7001\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.0585, val_acc: 0.2218, val_top_1: 0.2218, val_top_5: 0.7008\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4967, val_acc: 0.4326, val_top_1: 0.4326, val_top_5: 0.8224\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22], val_loss: 1.4876, val_acc: 0.4323, val_top_1: 0.4323, val_top_5: 0.8245\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4790, val_acc: 0.4331, val_top_1: 0.4331, val_top_5: 0.8238\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4714, val_acc: 0.4347, val_top_1: 0.4347, val_top_5: 0.8729\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4640, val_acc: 0.4353, val_top_1: 0.4353, val_top_5: 0.8235\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.4579, val_acc: 0.4332, val_top_1: 0.4332, val_top_5: 0.8261\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.4516, val_acc: 0.4354, val_top_1: 0.4354, val_top_5: 0.8256\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.4461, val_acc: 0.4380, val_top_1: 0.4380, val_top_5: 0.8237\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.4426, val_acc: 0.4385, val_top_1: 0.4385, val_top_5: 0.8207\n",
      "wt desc =  None\n",
      "Compression= 0.9941424110857547 Result after pruning and retraining is  {'val_loss': 1.4269108772277832, 'val_acc': 0.44775390625, 'val_top_1': 0.44775390625, 'val_top_5': 0.8246093988418579}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.1 tensor(0.9962)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.2796, val_acc: 0.1918, val_top_1: 0.1918, val_top_5: 0.5397\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.2412, val_acc: 0.1891, val_top_1: 0.1891, val_top_5: 0.6636\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.1829, val_acc: 0.2249, val_top_1: 0.2249, val_top_5: 0.7020\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.1149, val_acc: 0.2223, val_top_1: 0.2223, val_top_5: 0.7003\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.0645, val_acc: 0.2228, val_top_1: 0.2228, val_top_5: 0.7024\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.0255, val_acc: 0.2240, val_top_1: 0.2240, val_top_5: 0.7120\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.9891, val_acc: 0.2427, val_top_1: 0.2427, val_top_5: 0.7774\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.9570, val_acc: 0.2363, val_top_1: 0.2363, val_top_5: 0.7699\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.9262, val_acc: 0.2334, val_top_1: 0.2334, val_top_5: 0.7719\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.8943, val_acc: 0.2935, val_top_1: 0.2935, val_top_5: 0.7732\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.8552, val_acc: 0.3087, val_top_1: 0.3087, val_top_5: 0.7832\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.7930, val_acc: 0.3368, val_top_1: 0.3368, val_top_5: 0.7959\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.7205, val_acc: 0.3688, val_top_1: 0.3688, val_top_5: 0.8060\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.6650, val_acc: 0.3800, val_top_1: 0.3800, val_top_5: 0.8124\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.6280, val_acc: 0.3909, val_top_1: 0.3909, val_top_5: 0.8150\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.5996, val_acc: 0.4062, val_top_1: 0.4062, val_top_5: 0.8172\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.5769, val_acc: 0.4180, val_top_1: 0.4180, val_top_5: 0.8184\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.5575, val_acc: 0.4209, val_top_1: 0.4209, val_top_5: 0.8193\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.5414, val_acc: 0.4236, val_top_1: 0.4236, val_top_5: 0.8213\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.5276, val_acc: 0.4287, val_top_1: 0.4287, val_top_5: 0.8198\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.5152, val_acc: 0.4268, val_top_1: 0.4268, val_top_5: 0.8231\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.5039, val_acc: 0.4282, val_top_1: 0.4282, val_top_5: 0.8239\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4938, val_acc: 0.4300, val_top_1: 0.4300, val_top_5: 0.8239\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4852, val_acc: 0.4317, val_top_1: 0.4317, val_top_5: 0.8214\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4771, val_acc: 0.4332, val_top_1: 0.4332, val_top_5: 0.8216\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4700, val_acc: 0.4350, val_top_1: 0.4350, val_top_5: 0.8212\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.4630, val_acc: 0.4340, val_top_1: 0.4340, val_top_5: 0.8236\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.4567, val_acc: 0.4339, val_top_1: 0.4339, val_top_5: 0.8247\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.4516, val_acc: 0.4336, val_top_1: 0.4336, val_top_5: 0.8254\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.4474, val_acc: 0.4318, val_top_1: 0.4318, val_top_5: 0.8310\n",
      "wt desc =  None\n",
      "Compression= 0.9943071815193482 Result after pruning and retraining is  {'val_loss': 1.4319038391113281, 'val_acc': 0.4385742247104645, 'val_top_1': 0.4385742247104645, 'val_top_5': 0.8345702886581421}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.3 tensor(0.9970)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.3081, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.2924, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5286\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.2799, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.6302\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.2660, val_acc: 0.1437, val_top_1: 0.1437, val_top_5: 0.6382\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.2470, val_acc: 0.2141, val_top_1: 0.2141, val_top_5: 0.6357\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.2213, val_acc: 0.1950, val_top_1: 0.1950, val_top_5: 0.6335\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 2.1913, val_acc: 0.1926, val_top_1: 0.1926, val_top_5: 0.6702\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 2.1615, val_acc: 0.1953, val_top_1: 0.1953, val_top_5: 0.6736\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 2.1339, val_acc: 0.1977, val_top_1: 0.1977, val_top_5: 0.6769\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 2.1081, val_acc: 0.2006, val_top_1: 0.2006, val_top_5: 0.6533\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 2.0840, val_acc: 0.2072, val_top_1: 0.2072, val_top_5: 0.6281\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 2.0618, val_acc: 0.2824, val_top_1: 0.2824, val_top_5: 0.6638\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 2.0422, val_acc: 0.2861, val_top_1: 0.2861, val_top_5: 0.6662\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 2.0254, val_acc: 0.2869, val_top_1: 0.2869, val_top_5: 0.6666\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 2.0113, val_acc: 0.2875, val_top_1: 0.2875, val_top_5: 0.6668\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.9996, val_acc: 0.2888, val_top_1: 0.2888, val_top_5: 0.6508\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.9899, val_acc: 0.2891, val_top_1: 0.2891, val_top_5: 0.6677\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.9816, val_acc: 0.2882, val_top_1: 0.2882, val_top_5: 0.6520\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.9751, val_acc: 0.2893, val_top_1: 0.2893, val_top_5: 0.6557\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.9683, val_acc: 0.2895, val_top_1: 0.2895, val_top_5: 0.6186\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.9626, val_acc: 0.2871, val_top_1: 0.2871, val_top_5: 0.6522\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.9575, val_acc: 0.2862, val_top_1: 0.2862, val_top_5: 0.6180\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.9522, val_acc: 0.2861, val_top_1: 0.2861, val_top_5: 0.6688\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.9473, val_acc: 0.2875, val_top_1: 0.2875, val_top_5: 0.6693\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.9429, val_acc: 0.2869, val_top_1: 0.2869, val_top_5: 0.6691\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.9389, val_acc: 0.2867, val_top_1: 0.2867, val_top_5: 0.6689\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.9350, val_acc: 0.2869, val_top_1: 0.2869, val_top_5: 0.6289\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.9315, val_acc: 0.2869, val_top_1: 0.2869, val_top_5: 0.6320\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.9278, val_acc: 0.2879, val_top_1: 0.2879, val_top_5: 0.6557\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.9247, val_acc: 0.2874, val_top_1: 0.2874, val_top_5: 0.6351\n",
      "wt desc =  None\n",
      "Compression= 0.9951021988614362 Result after pruning and retraining is  {'val_loss': 1.9292113780975342, 'val_acc': 0.28291016817092896, 'val_top_1': 0.28291016817092896, 'val_top_5': 0.6474609375}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.5 tensor(0.9977)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.3147, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.3029, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5263\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], val_loss: 2.2964, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5464\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.2919, val_acc: 0.1107, val_top_1: 0.1107, val_top_5: 0.5522\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.2876, val_acc: 0.1498, val_top_1: 0.1498, val_top_5: 0.5578\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.2829, val_acc: 0.1656, val_top_1: 0.1656, val_top_5: 0.5634\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 2.2771, val_acc: 0.1757, val_top_1: 0.1757, val_top_5: 0.5795\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 2.2701, val_acc: 0.1792, val_top_1: 0.1792, val_top_5: 0.5804\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 2.2618, val_acc: 0.1797, val_top_1: 0.1797, val_top_5: 0.5809\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 2.2526, val_acc: 0.1807, val_top_1: 0.1807, val_top_5: 0.5691\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 2.2428, val_acc: 0.1831, val_top_1: 0.1831, val_top_5: 0.5748\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 2.2330, val_acc: 0.1687, val_top_1: 0.1687, val_top_5: 0.5756\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 2.2238, val_acc: 0.1724, val_top_1: 0.1724, val_top_5: 0.5759\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 2.2156, val_acc: 0.1750, val_top_1: 0.1750, val_top_5: 0.5762\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 2.2083, val_acc: 0.1771, val_top_1: 0.1771, val_top_5: 0.5713\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 2.2021, val_acc: 0.1786, val_top_1: 0.1786, val_top_5: 0.5716\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 2.1965, val_acc: 0.1807, val_top_1: 0.1807, val_top_5: 0.5718\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 2.1918, val_acc: 0.1815, val_top_1: 0.1815, val_top_5: 0.5722\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 2.1879, val_acc: 0.1821, val_top_1: 0.1821, val_top_5: 0.5721\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 2.1843, val_acc: 0.1829, val_top_1: 0.1829, val_top_5: 0.5250\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 2.1811, val_acc: 0.1842, val_top_1: 0.1842, val_top_5: 0.5790\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 2.1784, val_acc: 0.1844, val_top_1: 0.1844, val_top_5: 0.5729\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 2.1757, val_acc: 0.1850, val_top_1: 0.1850, val_top_5: 0.5788\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 2.1745, val_acc: 0.1839, val_top_1: 0.1839, val_top_5: 0.5726\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 2.1716, val_acc: 0.1860, val_top_1: 0.1860, val_top_5: 0.5797\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 2.1708, val_acc: 0.1845, val_top_1: 0.1845, val_top_5: 0.5764\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 2.1674, val_acc: 0.1859, val_top_1: 0.1859, val_top_5: 0.5785\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 2.1649, val_acc: 0.1862, val_top_1: 0.1862, val_top_5: 0.5779\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 2.1622, val_acc: 0.1866, val_top_1: 0.1866, val_top_5: 0.5738\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 2.1599, val_acc: 0.1865, val_top_1: 0.1865, val_top_5: 0.5781\n",
      "wt desc =  None\n",
      "Compression= 0.9958189502475676 Result after pruning and retraining is  {'val_loss': 2.1616125106811523, 'val_acc': 0.1875, 'val_top_1': 0.1875, 'val_top_5': 0.59423828125}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.7 tensor(0.9982)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.3167, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.3058, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.3003, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5285\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.2968, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5421\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.2940, val_acc: 0.1208, val_top_1: 0.1208, val_top_5: 0.5460\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.2911, val_acc: 0.1529, val_top_1: 0.1529, val_top_5: 0.5497\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 2.2876, val_acc: 0.1668, val_top_1: 0.1668, val_top_5: 0.5708\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 2.2833, val_acc: 0.1751, val_top_1: 0.1751, val_top_5: 0.5729\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 2.2777, val_acc: 0.1758, val_top_1: 0.1758, val_top_5: 0.5737\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 2.2708, val_acc: 0.1768, val_top_1: 0.1768, val_top_5: 0.5747\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 2.2627, val_acc: 0.1782, val_top_1: 0.1782, val_top_5: 0.5706\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 2.2535, val_acc: 0.1691, val_top_1: 0.1691, val_top_5: 0.5713\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 2.2440, val_acc: 0.1730, val_top_1: 0.1730, val_top_5: 0.5730\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 2.2345, val_acc: 0.1755, val_top_1: 0.1755, val_top_5: 0.5706\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 2.2255, val_acc: 0.1777, val_top_1: 0.1777, val_top_5: 0.5761\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 2.2177, val_acc: 0.1786, val_top_1: 0.1786, val_top_5: 0.5759\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 2.2106, val_acc: 0.1369, val_top_1: 0.1369, val_top_5: 0.5704\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 2.2044, val_acc: 0.1811, val_top_1: 0.1811, val_top_5: 0.5704\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 2.1992, val_acc: 0.1818, val_top_1: 0.1818, val_top_5: 0.5707\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 2.1944, val_acc: 0.1827, val_top_1: 0.1827, val_top_5: 0.5708\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 2.1901, val_acc: 0.1828, val_top_1: 0.1828, val_top_5: 0.5703\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 2.1861, val_acc: 0.1832, val_top_1: 0.1832, val_top_5: 0.5781\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 2.1826, val_acc: 0.1833, val_top_1: 0.1833, val_top_5: 0.5781\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 2.1795, val_acc: 0.1841, val_top_1: 0.1841, val_top_5: 0.5788\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 2.1767, val_acc: 0.1840, val_top_1: 0.1840, val_top_5: 0.5786\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 2.1745, val_acc: 0.1840, val_top_1: 0.1840, val_top_5: 0.5782\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 2.1724, val_acc: 0.1853, val_top_1: 0.1853, val_top_5: 0.5796\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 2.1707, val_acc: 0.1848, val_top_1: 0.1848, val_top_5: 0.5789\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 2.1691, val_acc: 0.1850, val_top_1: 0.1850, val_top_5: 0.5727\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 2.1677, val_acc: 0.1849, val_top_1: 0.1849, val_top_5: 0.5794\n",
      "wt desc =  None\n",
      "Compression= 0.9963173808091876 Result after pruning and retraining is  {'val_loss': 2.171868085861206, 'val_acc': 0.18437500298023224, 'val_top_1': 0.18437500298023224, 'val_top_5': 0.59423828125}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.9 tensor(0.9986)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.3180, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.3073, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.3024, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5118\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.2997, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5251\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.2980, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5304\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.2964, val_acc: 0.1277, val_top_1: 0.1277, val_top_5: 0.5342\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 2.2946, val_acc: 0.1442, val_top_1: 0.1442, val_top_5: 0.5591\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 2.2925, val_acc: 0.1616, val_top_1: 0.1616, val_top_5: 0.5626\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 2.2900, val_acc: 0.1696, val_top_1: 0.1696, val_top_5: 0.5645\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 2.2868, val_acc: 0.1738, val_top_1: 0.1738, val_top_5: 0.5746\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 2.2827, val_acc: 0.1743, val_top_1: 0.1743, val_top_5: 0.5687\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 2.2776, val_acc: 0.1569, val_top_1: 0.1569, val_top_5: 0.5693\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 2.2715, val_acc: 0.1578, val_top_1: 0.1578, val_top_5: 0.5713\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 2.2642, val_acc: 0.1623, val_top_1: 0.1623, val_top_5: 0.5678\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 2.2561, val_acc: 0.1652, val_top_1: 0.1652, val_top_5: 0.5682\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 2.2474, val_acc: 0.1690, val_top_1: 0.1690, val_top_5: 0.5688\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 2.2385, val_acc: 0.1715, val_top_1: 0.1715, val_top_5: 0.5692\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], val_loss: 2.2299, val_acc: 0.1739, val_top_1: 0.1739, val_top_5: 0.5694\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 2.2218, val_acc: 0.1762, val_top_1: 0.1762, val_top_5: 0.5695\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 2.2145, val_acc: 0.1780, val_top_1: 0.1780, val_top_5: 0.5748\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 2.2079, val_acc: 0.1799, val_top_1: 0.1799, val_top_5: 0.5702\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 2.2022, val_acc: 0.1807, val_top_1: 0.1807, val_top_5: 0.5748\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 2.1967, val_acc: 0.1830, val_top_1: 0.1830, val_top_5: 0.5774\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 2.1921, val_acc: 0.1842, val_top_1: 0.1842, val_top_5: 0.5271\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 2.1875, val_acc: 0.1847, val_top_1: 0.1847, val_top_5: 0.5788\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 2.1833, val_acc: 0.1849, val_top_1: 0.1849, val_top_5: 0.5786\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 2.1796, val_acc: 0.1849, val_top_1: 0.1849, val_top_5: 0.5786\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 2.1763, val_acc: 0.1847, val_top_1: 0.1847, val_top_5: 0.5784\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 2.1734, val_acc: 0.1850, val_top_1: 0.1850, val_top_5: 0.5785\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 2.1710, val_acc: 0.1855, val_top_1: 0.1855, val_top_5: 0.5793\n",
      "wt desc =  None\n",
      "Compression= 0.9967210683714914 Result after pruning and retraining is  {'val_loss': 2.176867961883545, 'val_acc': 0.18496093153953552, 'val_top_1': 0.18496093153953552, 'val_top_5': 0.593945324420929}\n",
      "Test result is  {'val_loss': 0.35806748270988464, 'val_acc': 0.871874988079071, 'val_top_1': 0.871874988079071, 'val_top_5': 0.9970703125}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  5.9 tensor(0.9986)\n",
      "At train\n",
      "Epoch [0], val_loss: 2.3179, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.3073, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.4998\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 2.3024, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5129\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 2.2997, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5242\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 2.2980, val_acc: 0.0958, val_top_1: 0.0958, val_top_5: 0.5302\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 2.2964, val_acc: 0.1308, val_top_1: 0.1308, val_top_5: 0.5560\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 2.2946, val_acc: 0.1507, val_top_1: 0.1507, val_top_5: 0.5385\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 2.2925, val_acc: 0.1609, val_top_1: 0.1609, val_top_5: 0.5625\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 2.2900, val_acc: 0.1692, val_top_1: 0.1692, val_top_5: 0.5645\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 2.2868, val_acc: 0.1715, val_top_1: 0.1715, val_top_5: 0.5435\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 2.2827, val_acc: 0.1759, val_top_1: 0.1759, val_top_5: 0.5694\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 2.2776, val_acc: 0.1569, val_top_1: 0.1569, val_top_5: 0.5697\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 2.2714, val_acc: 0.1605, val_top_1: 0.1605, val_top_5: 0.5693\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 2.2642, val_acc: 0.1635, val_top_1: 0.1635, val_top_5: 0.5695\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 2.2561, val_acc: 0.1656, val_top_1: 0.1656, val_top_5: 0.5710\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 2.2473, val_acc: 0.1679, val_top_1: 0.1679, val_top_5: 0.5694\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 2.2384, val_acc: 0.1712, val_top_1: 0.1712, val_top_5: 0.5729\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 2.2298, val_acc: 0.1742, val_top_1: 0.1742, val_top_5: 0.5730\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 2.2217, val_acc: 0.1762, val_top_1: 0.1762, val_top_5: 0.5745\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 2.2146, val_acc: 0.1786, val_top_1: 0.1786, val_top_5: 0.5743\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 2.2079, val_acc: 0.1800, val_top_1: 0.1800, val_top_5: 0.5751\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 2.2020, val_acc: 0.1819, val_top_1: 0.1819, val_top_5: 0.5771\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 2.1967, val_acc: 0.1832, val_top_1: 0.1832, val_top_5: 0.5773\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 2.1919, val_acc: 0.1838, val_top_1: 0.1838, val_top_5: 0.5776\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 2.1874, val_acc: 0.1848, val_top_1: 0.1848, val_top_5: 0.5788\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 2.1831, val_acc: 0.1848, val_top_1: 0.1848, val_top_5: 0.5784\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 2.1795, val_acc: 0.1850, val_top_1: 0.1850, val_top_5: 0.5788\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 2.1763, val_acc: 0.1847, val_top_1: 0.1847, val_top_5: 0.5782\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 2.1734, val_acc: 0.1850, val_top_1: 0.1850, val_top_5: 0.5736\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 2.1710, val_acc: 0.1854, val_top_1: 0.1854, val_top_5: 0.5792\n",
      "wt desc =  None\n",
      "Compression= 0.9967210683714914 Result after pruning and retraining is  {'val_loss': 2.1768219470977783, 'val_acc': 0.18486328423023224, 'val_top_1': 0.18486328423023224, 'val_top_5': 0.593945324420929}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics={}\n",
    "metrics[\"prune_rate\"]=[]\n",
    "metrics[\"compression\"]=[]\n",
    "metrics[\"epochs\"]=[]\n",
    "metrics[\"top_5\"]=[]\n",
    "metrics[\"top_1\"]=[]\n",
    "\n",
    "df_base=pd.read_csv(\"results_sheet/song_hn.csv\")\n",
    "done_pruning_list=list(df_base[\"prune_rate\"])\n",
    "\n",
    "\n",
    "# prune_rate_range=[0.1,0.3,0.4,0.8,1.1,1.3,1.5,1.7,\n",
    "#                   1.8,2.1,2.4,2.9,3.1,3.15,3.12,3.22,3.29,3.35,3.4,3.45,3.5,3.55,\n",
    "#                   3.6,3.65,3.7,3.75,3.8,3.85,3.9,3.95,4,4.25,4.5,5,6]\n",
    "\n",
    "prune_rate_range=[4.51,4.53,4.55,4.57,5.01,5.03,5.05,5.07,5.1,5.3,5.5,5.7,5.9,5.9]\n",
    "\n",
    "for prune_rate in prune_rate_range:\n",
    "    if prune_rate in done_pruning_list:\n",
    "        continue\n",
    "\n",
    "    model_state_path=\"model_state/mod.pt\"\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(model_state_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_state_path,map_location=torch.device('cpu')))\n",
    "    \n",
    "\n",
    "\n",
    "    result = evaluate(model, test_loader)\n",
    "    print(\"Test result is \",result)\n",
    "\n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"Original Compression=\",compression)\n",
    "\n",
    "    \n",
    "    mask_whole_model=prune_model_get_mask(model,prune_rate)\n",
    "\n",
    "    print(\"Mask compression = \",prune_rate,get_mask_compression(mask_whole_model))\n",
    "\n",
    "\n",
    "    apply_mask_model(model,mask_whole_model)\n",
    "\n",
    "\n",
    "    epochs=30\n",
    "    history_prune,_ = fit(epochs, lr, model, train_loader, val_loader,\n",
    "                          mask_whole_model=mask_whole_model)\n",
    "\n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"Compression=\",compression,\"Result after pruning and retraining is \",res)\n",
    "    metrics[\"prune_rate\"].append(prune_rate)\n",
    "    metrics[\"compression\"].append(compression)\n",
    "    metrics[\"epochs\"].append(epochs)\n",
    "    metrics[\"top_5\"].append(res['val_top_5'])\n",
    "    metrics[\"top_1\"].append(res['val_top_1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prune_rate  compression  epochs     top_5     top_1\n",
      "0         0.1     0.037283      30  0.996582  0.879395\n",
      "1         0.3     0.110376      30  0.996973  0.880957\n",
      "2         0.4     0.147058      30  0.996680  0.861621\n",
      "3         0.8     0.292117      30  0.997266  0.871094\n",
      "4         1.1     0.400215      30  0.996777  0.871582\n"
     ]
    }
   ],
   "source": [
    "dataframe_results=pd.DataFrame(metrics)\n",
    "dataframe_results=pd.concat([df_base, dataframe_results], ignore_index=True)\n",
    "\n",
    "print(dataframe_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(\"results_sheet\",\"song_hn.csv\")):\n",
    "    df_base=pd.read_csv(os.path.join(\"results_sheet\",\"song_hn.csv\"))\n",
    "    df_concat=pd.concat([df_base,dataframe_results])\n",
    "    dataframe_results=df_concat\n",
    "\n",
    "dataframe_results.to_csv(os.path.join(\"results_sheet\",\"song_hn.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[0.55, 0.6,0.65, 0.7,0.75,0.8,0.85,0.9,0.95,\n",
    "                 1.1, 1.4, 1.7, 2, 2.3,2.6,3,3.2,3.6,3.9,\n",
    "                  4.3,4.6,4.9\n",
    "                 ]\n",
    "l2=[0.02, 0.025,0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065,0.07, 0.09,0.1,\n",
    "                 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.29, 0.3, 0.5,1,5,9,12,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.extend(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=sorted(list(set(l1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=pd.read_csv(\"results_sheet/song_hn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prune_rate</th>\n",
       "      <th>compression</th>\n",
       "      <th>epochs</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.037283</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996582</td>\n",
       "      <td>0.879395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.110376</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996973</td>\n",
       "      <td>0.880957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.147058</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.861621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.292117</td>\n",
       "      <td>30</td>\n",
       "      <td>0.997266</td>\n",
       "      <td>0.871094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.10</td>\n",
       "      <td>0.400215</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996777</td>\n",
       "      <td>0.871582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0.471433</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996484</td>\n",
       "      <td>0.879883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0.609729</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996484</td>\n",
       "      <td>0.877930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0.642984</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.872461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.737723</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996191</td>\n",
       "      <td>0.876758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.40</td>\n",
       "      <td>0.823288</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996191</td>\n",
       "      <td>0.872363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.90</td>\n",
       "      <td>0.917499</td>\n",
       "      <td>30</td>\n",
       "      <td>0.995898</td>\n",
       "      <td>0.870605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.10</td>\n",
       "      <td>0.937824</td>\n",
       "      <td>30</td>\n",
       "      <td>0.995508</td>\n",
       "      <td>0.868945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.941890</td>\n",
       "      <td>30</td>\n",
       "      <td>0.993555</td>\n",
       "      <td>0.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.12</td>\n",
       "      <td>0.939476</td>\n",
       "      <td>30</td>\n",
       "      <td>0.994922</td>\n",
       "      <td>0.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.22</td>\n",
       "      <td>0.947356</td>\n",
       "      <td>30</td>\n",
       "      <td>0.993066</td>\n",
       "      <td>0.862988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.29</td>\n",
       "      <td>0.952241</td>\n",
       "      <td>30</td>\n",
       "      <td>0.992969</td>\n",
       "      <td>0.861621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.955961</td>\n",
       "      <td>30</td>\n",
       "      <td>0.992578</td>\n",
       "      <td>0.860645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.958972</td>\n",
       "      <td>30</td>\n",
       "      <td>0.992383</td>\n",
       "      <td>0.860156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>30</td>\n",
       "      <td>0.991699</td>\n",
       "      <td>0.843555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.964393</td>\n",
       "      <td>30</td>\n",
       "      <td>0.991406</td>\n",
       "      <td>0.839453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.966696</td>\n",
       "      <td>30</td>\n",
       "      <td>0.991602</td>\n",
       "      <td>0.839551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.968891</td>\n",
       "      <td>30</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.837793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.65</td>\n",
       "      <td>0.970922</td>\n",
       "      <td>30</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>0.770020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.972809</td>\n",
       "      <td>30</td>\n",
       "      <td>0.989551</td>\n",
       "      <td>0.765332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.974683</td>\n",
       "      <td>30</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>0.763672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.976314</td>\n",
       "      <td>30</td>\n",
       "      <td>0.990723</td>\n",
       "      <td>0.760742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.85</td>\n",
       "      <td>0.977867</td>\n",
       "      <td>30</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.759961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.90</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>30</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.756738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.95</td>\n",
       "      <td>0.980672</td>\n",
       "      <td>30</td>\n",
       "      <td>0.990039</td>\n",
       "      <td>0.756934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.981851</td>\n",
       "      <td>30</td>\n",
       "      <td>0.989453</td>\n",
       "      <td>0.755469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.25</td>\n",
       "      <td>0.986757</td>\n",
       "      <td>30</td>\n",
       "      <td>0.985059</td>\n",
       "      <td>0.606934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0.989772</td>\n",
       "      <td>30</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>0.431445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.993801</td>\n",
       "      <td>30</td>\n",
       "      <td>0.880078</td>\n",
       "      <td>0.434863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.00</td>\n",
       "      <td>0.996952</td>\n",
       "      <td>30</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.51</td>\n",
       "      <td>0.989879</td>\n",
       "      <td>30</td>\n",
       "      <td>0.881738</td>\n",
       "      <td>0.503223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.53</td>\n",
       "      <td>0.990068</td>\n",
       "      <td>30</td>\n",
       "      <td>0.882520</td>\n",
       "      <td>0.502930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.990270</td>\n",
       "      <td>30</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.499121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.57</td>\n",
       "      <td>0.990501</td>\n",
       "      <td>30</td>\n",
       "      <td>0.884668</td>\n",
       "      <td>0.499512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.01</td>\n",
       "      <td>0.993858</td>\n",
       "      <td>30</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.445898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.03</td>\n",
       "      <td>0.993965</td>\n",
       "      <td>30</td>\n",
       "      <td>0.878906</td>\n",
       "      <td>0.447266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.05</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>30</td>\n",
       "      <td>0.826465</td>\n",
       "      <td>0.447168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.07</td>\n",
       "      <td>0.994142</td>\n",
       "      <td>30</td>\n",
       "      <td>0.824609</td>\n",
       "      <td>0.447754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0.994307</td>\n",
       "      <td>30</td>\n",
       "      <td>0.834570</td>\n",
       "      <td>0.438574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.30</td>\n",
       "      <td>0.995102</td>\n",
       "      <td>30</td>\n",
       "      <td>0.647461</td>\n",
       "      <td>0.282910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.50</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>30</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.70</td>\n",
       "      <td>0.996317</td>\n",
       "      <td>30</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.184375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.90</td>\n",
       "      <td>0.996721</td>\n",
       "      <td>30</td>\n",
       "      <td>0.593945</td>\n",
       "      <td>0.184961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.90</td>\n",
       "      <td>0.996721</td>\n",
       "      <td>30</td>\n",
       "      <td>0.593945</td>\n",
       "      <td>0.184863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    prune_rate  compression  epochs     top_5     top_1\n",
       "0         0.10     0.037283      30  0.996582  0.879395\n",
       "1         0.30     0.110376      30  0.996973  0.880957\n",
       "2         0.40     0.147058      30  0.996680  0.861621\n",
       "3         0.80     0.292117      30  0.997266  0.871094\n",
       "4         1.10     0.400215      30  0.996777  0.871582\n",
       "5         1.30     0.471433      30  0.996680  0.875000\n",
       "6         1.50     0.541600      30  0.996484  0.879883\n",
       "7         1.70     0.609729      30  0.996484  0.877930\n",
       "8         1.80     0.642984      30  0.996680  0.872461\n",
       "9         2.10     0.737723      30  0.996191  0.876758\n",
       "10        2.40     0.823288      30  0.996191  0.872363\n",
       "11        2.90     0.917499      30  0.995898  0.870605\n",
       "12        3.10     0.937824      30  0.995508  0.868945\n",
       "13        3.15     0.941890      30  0.993555  0.865234\n",
       "14        3.12     0.939476      30  0.994922  0.865234\n",
       "15        3.22     0.947356      30  0.993066  0.862988\n",
       "16        3.29     0.952241      30  0.992969  0.861621\n",
       "17        3.35     0.955961      30  0.992578  0.860645\n",
       "18        3.40     0.958972      30  0.992383  0.860156\n",
       "19        3.45     0.961905      30  0.991699  0.843555\n",
       "20        3.50     0.964393      30  0.991406  0.839453\n",
       "21        3.55     0.966696      30  0.991602  0.839551\n",
       "22        3.60     0.968891      30  0.991504  0.837793\n",
       "23        3.65     0.970922      30  0.989746  0.770020\n",
       "24        3.70     0.972809      30  0.989551  0.765332\n",
       "25        3.75     0.974683      30  0.989746  0.763672\n",
       "26        3.80     0.976314      30  0.990723  0.760742\n",
       "27        3.85     0.977867      30  0.989941  0.759961\n",
       "28        3.90     0.979354      30  0.989941  0.756738\n",
       "29        3.95     0.980672      30  0.990039  0.756934\n",
       "30        4.00     0.981851      30  0.989453  0.755469\n",
       "31        4.25     0.986757      30  0.985059  0.606934\n",
       "32        4.50     0.989772      30  0.936523  0.431445\n",
       "33        5.00     0.993801      30  0.880078  0.434863\n",
       "34        6.00     0.996952      30  0.500000  0.097656\n",
       "35        4.51     0.989879      30  0.881738  0.503223\n",
       "36        4.53     0.990068      30  0.882520  0.502930\n",
       "37        4.55     0.990270      30  0.880566  0.499121\n",
       "38        4.57     0.990501      30  0.884668  0.499512\n",
       "39        5.01     0.993858      30  0.829590  0.445898\n",
       "40        5.03     0.993965      30  0.878906  0.447266\n",
       "41        5.05     0.994048      30  0.826465  0.447168\n",
       "42        5.07     0.994142      30  0.824609  0.447754\n",
       "43        5.10     0.994307      30  0.834570  0.438574\n",
       "44        5.30     0.995102      30  0.647461  0.282910\n",
       "45        5.50     0.995819      30  0.594238  0.187500\n",
       "46        5.70     0.996317      30  0.594238  0.184375\n",
       "47        5.90     0.996721      30  0.593945  0.184961\n",
       "48        5.90     0.996721      30  0.593945  0.184863"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
