{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song Han cifar10\n",
      "Torch cuda  False\n",
      "device  cpu\n",
      "Files already downloaded and verified\n",
      "initial result [{'val_loss': 2.3054258823394775, 'val_acc': 0.10078124701976776, 'val_top_1': 0.10078124701976776, 'val_top_5': 0.49736326932907104}]\n",
      "Prune rate 2.2\n",
      "Test result is  {'val_loss': 1.215560793876648, 'val_acc': 0.5616210699081421, 'val_top_1': 0.5616210699081421, 'val_top_5': 0.9493163824081421}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  2.2 tensor(0.7892)\n",
      "After pruning,with 2.2  Compression= 0.7862303648034061 \n",
      "Result after pruning is  {'val_loss': 2.2403454780578613, 'val_acc': 0.15214844048023224, 'val_top_1': 0.15214844048023224, 'val_top_5': 0.630078136920929}\n",
      "At train\n",
      "Epoch [0], val_loss: 1.6416, val_acc: 0.4073, val_top_1: 0.4073, val_top_5: 0.9026\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 1.5296, val_acc: 0.4502, val_top_1: 0.4502, val_top_5: 0.9209\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.4748, val_acc: 0.4685, val_top_1: 0.4685, val_top_5: 0.9250\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.4346, val_acc: 0.4860, val_top_1: 0.4860, val_top_5: 0.9294\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.4123, val_acc: 0.4967, val_top_1: 0.4967, val_top_5: 0.9319\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.3940, val_acc: 0.5039, val_top_1: 0.5039, val_top_5: 0.9336\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.3809, val_acc: 0.5030, val_top_1: 0.5030, val_top_5: 0.9336\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.3658, val_acc: 0.5136, val_top_1: 0.5136, val_top_5: 0.9357\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.3464, val_acc: 0.5188, val_top_1: 0.5188, val_top_5: 0.9368\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.3499, val_acc: 0.5198, val_top_1: 0.5198, val_top_5: 0.9384\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.3251, val_acc: 0.5282, val_top_1: 0.5282, val_top_5: 0.9422\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.3275, val_acc: 0.5312, val_top_1: 0.5312, val_top_5: 0.9399\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.3051, val_acc: 0.5369, val_top_1: 0.5369, val_top_5: 0.9426\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.2946, val_acc: 0.5375, val_top_1: 0.5375, val_top_5: 0.9459\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.2815, val_acc: 0.5451, val_top_1: 0.5451, val_top_5: 0.9446\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.2793, val_acc: 0.5442, val_top_1: 0.5442, val_top_5: 0.9472\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.2807, val_acc: 0.5452, val_top_1: 0.5452, val_top_5: 0.9465\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.2656, val_acc: 0.5493, val_top_1: 0.5493, val_top_5: 0.9469\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.2583, val_acc: 0.5526, val_top_1: 0.5526, val_top_5: 0.9473\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.2540, val_acc: 0.5561, val_top_1: 0.5561, val_top_5: 0.9479\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.2653, val_acc: 0.5485, val_top_1: 0.5485, val_top_5: 0.9479\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.2739, val_acc: 0.5488, val_top_1: 0.5488, val_top_5: 0.9452\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.2614, val_acc: 0.5567, val_top_1: 0.5567, val_top_5: 0.9443\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.2295, val_acc: 0.5685, val_top_1: 0.5685, val_top_5: 0.9507\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.2658, val_acc: 0.5528, val_top_1: 0.5528, val_top_5: 0.9482\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.2316, val_acc: 0.5677, val_top_1: 0.5677, val_top_5: 0.9498\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.2522, val_acc: 0.5602, val_top_1: 0.5602, val_top_5: 0.9472\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.2288, val_acc: 0.5624, val_top_1: 0.5624, val_top_5: 0.9489\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.2104, val_acc: 0.5691, val_top_1: 0.5691, val_top_5: 0.9515\n",
      "wt desc =  None\n",
      "Compression= 0.7862303648034061 Result after pruning is  {'val_loss': 1.2382203340530396, 'val_acc': 0.5609375238418579, 'val_top_1': 0.5609375238418579, 'val_top_5': 0.951464831829071}\n",
      "********************\n",
      "Prune rate 2.4\n",
      "Test result is  {'val_loss': 1.215560793876648, 'val_acc': 0.5616210699081421, 'val_top_1': 0.5616210699081421, 'val_top_5': 0.9493163824081421}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  2.4 tensor(0.8402)\n",
      "After pruning,with 2.4  Compression= 0.8369673902525562 \n",
      "Result after pruning is  {'val_loss': 2.259248733520508, 'val_acc': 0.13408203423023224, 'val_top_1': 0.13408203423023224, 'val_top_5': 0.6036132574081421}\n",
      "At train\n",
      "Epoch [0], val_loss: 1.7648, val_acc: 0.3595, val_top_1: 0.3595, val_top_5: 0.8871\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 1.6374, val_acc: 0.4134, val_top_1: 0.4134, val_top_5: 0.9056\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.5727, val_acc: 0.4311, val_top_1: 0.4311, val_top_5: 0.9126\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.5437, val_acc: 0.4421, val_top_1: 0.4421, val_top_5: 0.9169\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.5079, val_acc: 0.4530, val_top_1: 0.4530, val_top_5: 0.9203\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.4846, val_acc: 0.4656, val_top_1: 0.4656, val_top_5: 0.9225\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.4730, val_acc: 0.4714, val_top_1: 0.4714, val_top_5: 0.9232\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.4545, val_acc: 0.4789, val_top_1: 0.4789, val_top_5: 0.9258\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.4425, val_acc: 0.4867, val_top_1: 0.4867, val_top_5: 0.9280\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.4383, val_acc: 0.4858, val_top_1: 0.4858, val_top_5: 0.9278\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.4207, val_acc: 0.4940, val_top_1: 0.4940, val_top_5: 0.9311\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.4163, val_acc: 0.4908, val_top_1: 0.4908, val_top_5: 0.9320\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.4224, val_acc: 0.4944, val_top_1: 0.4944, val_top_5: 0.9311\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.4172, val_acc: 0.4969, val_top_1: 0.4969, val_top_5: 0.9313\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.3886, val_acc: 0.5065, val_top_1: 0.5065, val_top_5: 0.9338\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.3994, val_acc: 0.4960, val_top_1: 0.4960, val_top_5: 0.9333\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.3817, val_acc: 0.5064, val_top_1: 0.5064, val_top_5: 0.9337\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.3747, val_acc: 0.5144, val_top_1: 0.5144, val_top_5: 0.9378\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.3706, val_acc: 0.5122, val_top_1: 0.5122, val_top_5: 0.9368\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.3793, val_acc: 0.5134, val_top_1: 0.5134, val_top_5: 0.9363\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.3680, val_acc: 0.5144, val_top_1: 0.5144, val_top_5: 0.9363\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.3780, val_acc: 0.5027, val_top_1: 0.5027, val_top_5: 0.9366\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.3603, val_acc: 0.5201, val_top_1: 0.5201, val_top_5: 0.9356\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.3498, val_acc: 0.5221, val_top_1: 0.5221, val_top_5: 0.9401\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.3490, val_acc: 0.5172, val_top_1: 0.5172, val_top_5: 0.9418\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.3342, val_acc: 0.5234, val_top_1: 0.5234, val_top_5: 0.9406\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.3376, val_acc: 0.5229, val_top_1: 0.5229, val_top_5: 0.9390\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.3329, val_acc: 0.5320, val_top_1: 0.5320, val_top_5: 0.9376\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.3346, val_acc: 0.5248, val_top_1: 0.5248, val_top_5: 0.9401\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.3231, val_acc: 0.5286, val_top_1: 0.5286, val_top_5: 0.9420\n",
      "wt desc =  None\n",
      "Compression= 0.8369673902525562 Result after pruning is  {'val_loss': 1.331545352935791, 'val_acc': 0.5238281488418579, 'val_top_1': 0.5238281488418579, 'val_top_5': 0.9364258050918579}\n",
      "********************\n",
      "Prune rate 2.6\n",
      "Test result is  {'val_loss': 1.215560793876648, 'val_acc': 0.5616210699081421, 'val_top_1': 0.5616210699081421, 'val_top_5': 0.9493163824081421}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  2.6 tensor(0.8777)\n",
      "After pruning,with 2.6  Compression= 0.8743831242137857 \n",
      "Result after pruning is  {'val_loss': 2.240614891052246, 'val_acc': 0.13457031548023224, 'val_top_1': 0.13457031548023224, 'val_top_5': 0.6376953125}\n",
      "At train\n",
      "Epoch [0], val_loss: 1.9115, val_acc: 0.3271, val_top_1: 0.3271, val_top_5: 0.8151\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 1.7671, val_acc: 0.3618, val_top_1: 0.3618, val_top_5: 0.8760\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.6816, val_acc: 0.3804, val_top_1: 0.3804, val_top_5: 0.8984\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.6293, val_acc: 0.4031, val_top_1: 0.4031, val_top_5: 0.9067\n",
      "wt desc =  None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], val_loss: 1.5877, val_acc: 0.4224, val_top_1: 0.4224, val_top_5: 0.9130\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.5598, val_acc: 0.4341, val_top_1: 0.4341, val_top_5: 0.9144\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.5416, val_acc: 0.4401, val_top_1: 0.4401, val_top_5: 0.9164\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.5228, val_acc: 0.4457, val_top_1: 0.4457, val_top_5: 0.9197\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.5056, val_acc: 0.4499, val_top_1: 0.4499, val_top_5: 0.9200\n",
      "wt desc =  None\n",
      "Epoch [9], val_loss: 1.4934, val_acc: 0.4576, val_top_1: 0.4576, val_top_5: 0.9224\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.4853, val_acc: 0.4605, val_top_1: 0.4605, val_top_5: 0.9213\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.4747, val_acc: 0.4626, val_top_1: 0.4626, val_top_5: 0.9225\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.4683, val_acc: 0.4631, val_top_1: 0.4631, val_top_5: 0.9236\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.4566, val_acc: 0.4725, val_top_1: 0.4725, val_top_5: 0.9255\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.4486, val_acc: 0.4748, val_top_1: 0.4748, val_top_5: 0.9258\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.4457, val_acc: 0.4771, val_top_1: 0.4771, val_top_5: 0.9289\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.4373, val_acc: 0.4814, val_top_1: 0.4814, val_top_5: 0.9291\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.4232, val_acc: 0.4834, val_top_1: 0.4834, val_top_5: 0.9280\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.4340, val_acc: 0.4853, val_top_1: 0.4853, val_top_5: 0.9273\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.4178, val_acc: 0.4890, val_top_1: 0.4890, val_top_5: 0.9306\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.4277, val_acc: 0.4832, val_top_1: 0.4832, val_top_5: 0.9307\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4065, val_acc: 0.4931, val_top_1: 0.4931, val_top_5: 0.9314\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4096, val_acc: 0.4917, val_top_1: 0.4917, val_top_5: 0.9316\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4049, val_acc: 0.4965, val_top_1: 0.4965, val_top_5: 0.9326\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4086, val_acc: 0.4928, val_top_1: 0.4928, val_top_5: 0.9326\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.3840, val_acc: 0.5006, val_top_1: 0.5006, val_top_5: 0.9336\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.3847, val_acc: 0.5030, val_top_1: 0.5030, val_top_5: 0.9374\n",
      "wt desc =  None\n",
      "Epoch [27], val_loss: 1.4037, val_acc: 0.4973, val_top_1: 0.4973, val_top_5: 0.9330\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.3956, val_acc: 0.4973, val_top_1: 0.4973, val_top_5: 0.9337\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.3731, val_acc: 0.5118, val_top_1: 0.5118, val_top_5: 0.9376\n",
      "wt desc =  None\n",
      "Compression= 0.8743831242137857 Result after pruning is  {'val_loss': 1.3695659637451172, 'val_acc': 0.507128894329071, 'val_top_1': 0.507128894329071, 'val_top_5': 0.934863269329071}\n",
      "********************\n",
      "Prune rate 2.8\n",
      "Test result is  {'val_loss': 1.215560793876648, 'val_acc': 0.5616210699081421, 'val_top_1': 0.5616210699081421, 'val_top_5': 0.9493163824081421}\n",
      "Original Compression= 0.0\n",
      "Mask compression =  2.8 tensor(0.9056)\n",
      "After pruning,with 2.8  Compression= 0.9021062477824726 \n",
      "Result after pruning is  {'val_loss': 2.3327584266662598, 'val_acc': 0.099609375, 'val_top_1': 0.099609375, 'val_top_5': 0.5328124761581421}\n",
      "At train\n",
      "Epoch [0], val_loss: 2.1303, val_acc: 0.2385, val_top_1: 0.2385, val_top_5: 0.7438\n",
      "wt desc =  None\n",
      "Epoch [1], val_loss: 2.0008, val_acc: 0.2840, val_top_1: 0.2840, val_top_5: 0.7837\n",
      "wt desc =  None\n",
      "Epoch [2], val_loss: 1.8818, val_acc: 0.3155, val_top_1: 0.3155, val_top_5: 0.8226\n",
      "wt desc =  None\n",
      "Epoch [3], val_loss: 1.8096, val_acc: 0.3345, val_top_1: 0.3345, val_top_5: 0.8495\n",
      "wt desc =  None\n",
      "Epoch [4], val_loss: 1.7484, val_acc: 0.3603, val_top_1: 0.3603, val_top_5: 0.8756\n",
      "wt desc =  None\n",
      "Epoch [5], val_loss: 1.7097, val_acc: 0.3717, val_top_1: 0.3717, val_top_5: 0.8851\n",
      "wt desc =  None\n",
      "Epoch [6], val_loss: 1.6832, val_acc: 0.3855, val_top_1: 0.3855, val_top_5: 0.8927\n",
      "wt desc =  None\n",
      "Epoch [7], val_loss: 1.6591, val_acc: 0.3937, val_top_1: 0.3937, val_top_5: 0.8940\n",
      "wt desc =  None\n",
      "Epoch [8], val_loss: 1.6289, val_acc: 0.4005, val_top_1: 0.4005, val_top_5: 0.9016\n",
      "wt desc =  None\n",
      "Epoch [10], val_loss: 1.6046, val_acc: 0.4156, val_top_1: 0.4156, val_top_5: 0.9036\n",
      "wt desc =  None\n",
      "Epoch [11], val_loss: 1.5840, val_acc: 0.4240, val_top_1: 0.4240, val_top_5: 0.9087\n",
      "wt desc =  None\n",
      "Epoch [12], val_loss: 1.5684, val_acc: 0.4300, val_top_1: 0.4300, val_top_5: 0.9131\n",
      "wt desc =  None\n",
      "Epoch [13], val_loss: 1.5571, val_acc: 0.4335, val_top_1: 0.4335, val_top_5: 0.9151\n",
      "wt desc =  None\n",
      "Epoch [14], val_loss: 1.5487, val_acc: 0.4362, val_top_1: 0.4362, val_top_5: 0.9175\n",
      "wt desc =  None\n",
      "Epoch [15], val_loss: 1.5329, val_acc: 0.4419, val_top_1: 0.4419, val_top_5: 0.9183\n",
      "wt desc =  None\n",
      "Epoch [16], val_loss: 1.5300, val_acc: 0.4420, val_top_1: 0.4420, val_top_5: 0.9185\n",
      "wt desc =  None\n",
      "Epoch [17], val_loss: 1.5191, val_acc: 0.4507, val_top_1: 0.4507, val_top_5: 0.9190\n",
      "wt desc =  None\n",
      "Epoch [18], val_loss: 1.5071, val_acc: 0.4547, val_top_1: 0.4547, val_top_5: 0.9214\n",
      "wt desc =  None\n",
      "Epoch [19], val_loss: 1.5022, val_acc: 0.4605, val_top_1: 0.4605, val_top_5: 0.9220\n",
      "wt desc =  None\n",
      "Epoch [20], val_loss: 1.5209, val_acc: 0.4452, val_top_1: 0.4452, val_top_5: 0.9197\n",
      "wt desc =  None\n",
      "Epoch [21], val_loss: 1.4916, val_acc: 0.4610, val_top_1: 0.4610, val_top_5: 0.9226\n",
      "wt desc =  None\n",
      "Epoch [22], val_loss: 1.4828, val_acc: 0.4635, val_top_1: 0.4635, val_top_5: 0.9216\n",
      "wt desc =  None\n",
      "Epoch [23], val_loss: 1.4787, val_acc: 0.4671, val_top_1: 0.4671, val_top_5: 0.9229\n",
      "wt desc =  None\n",
      "Epoch [24], val_loss: 1.4785, val_acc: 0.4647, val_top_1: 0.4647, val_top_5: 0.9223\n",
      "wt desc =  None\n",
      "Epoch [25], val_loss: 1.4925, val_acc: 0.4565, val_top_1: 0.4565, val_top_5: 0.9227\n",
      "wt desc =  None\n",
      "Epoch [26], val_loss: 1.4639, val_acc: 0.4709, val_top_1: 0.4709, val_top_5: 0.9244\n",
      "wt desc =  None\n",
      "Epoch [28], val_loss: 1.4580, val_acc: 0.4757, val_top_1: 0.4757, val_top_5: 0.9240\n",
      "wt desc =  None\n",
      "Epoch [29], val_loss: 1.4514, val_acc: 0.4824, val_top_1: 0.4824, val_top_5: 0.9244\n",
      "wt desc =  None\n",
      "Compression= 0.9021062477824726 Result after pruning is  {'val_loss': 1.4477386474609375, 'val_acc': 0.47333985567092896, 'val_top_1': 0.47333985567092896, 'val_top_5': 0.92626953125}\n",
      "********************\n",
      "Prune rate 3.5\n",
      "Already done\n",
      "Prune rate 4\n",
      "Already done\n",
      "Prune rate 4.5\n",
      "Already done\n",
      "Prune rate 5\n",
      "Already done\n",
      "   prune_rate  compression  epochs     top_5     top_1\n",
      "0        0.15     0.060897      30  0.954199  0.602344\n",
      "1        0.44     0.174886      30  0.956250  0.606445\n",
      "2        0.90     0.350789      30  0.959473  0.604102\n",
      "3        1.35     0.517708      30  0.953906  0.593066\n",
      "4        1.94     0.711270      30  0.951172  0.576074\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "    \n",
    "\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def countDiffMasks(mask1,mask2):\n",
    "    total_diff=0\n",
    "    for i in range(len(mask1)):\n",
    "        m_1=mask1[i].flatten()\n",
    "        m_2=mask2[i].flatten()\n",
    "        count_same=(m_1 == m_2).sum()\n",
    "        count_different=m_1.flatten().shape[0]-count_same\n",
    "        total_diff+=count_different\n",
    "    return total_diff\n",
    "\n",
    "\n",
    "def get_mask_compression(mask_whole_model):\n",
    "    num_total=0\n",
    "    num_non_zeros=0\n",
    "    for mask_each_layer in mask_whole_model:\n",
    "        num_total+=torch.numel(mask_each_layer)\n",
    "        num_non_zeros+=torch.count_nonzero(mask_each_layer)\n",
    "        \n",
    "    return (num_total-num_non_zeros)/num_total\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def prune_model_get_mask(model,prune_rate):\n",
    "    '''\n",
    "    works purely on the model to get\n",
    "    mask\n",
    "    '''\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.std(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "            threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    " \n",
    "    \n",
    "def get_thresholds_each_layer(model,prune_rate):\n",
    "    thresholds_per_layer=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "            abs_std=torch.std(torch.abs(params.data))\n",
    "            threshold=abs_std*prune_rate\n",
    "            thresholds_per_layer.append(threshold)\n",
    "    return thresholds_per_layer\n",
    "    \n",
    "                \n",
    "def apply_mask_model(model,list_mask_whole_model):\n",
    "    mask_layer_count=0\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=list_mask_whole_model[mask_layer_count]\n",
    "            with torch.no_grad():\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#                 print(\"Devices are \",params.device,mask_layer.device)\n",
    "                mask_layer=mask_layer.to(device)\n",
    "    \n",
    "                params.data=params.data*mask_layer            \n",
    "            mask_layer_count+=1\n",
    "    \n",
    "\n",
    "def store_weights_in_dic(weight_description,model):\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            if nm not in weight_description:\n",
    "                weight_description[nm]={}\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                if index_component not in weight_description[nm]:\n",
    "                    weight_description[nm][index_component]={}\n",
    "                values=params[index_component]\n",
    "                flat_values=values.flatten()\n",
    "                for index_wt in range(flat_values.shape[0]):\n",
    "                    if index_wt not in weight_description[nm][index_component]:\n",
    "                        weight_description[nm][index_component][index_wt]=[]\n",
    "                    weight_description[nm][index_component][index_wt].append(flat_values[index_wt].detach().item())\n",
    "    return weight_description\n",
    "\n",
    "\n",
    "def get_boolean_dict_weight_dict(weight_description,prune_rate,thresholds_per_layer):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of 1s and 0s to show\n",
    "    how many times weight is more than threshold\n",
    "    per layer\n",
    "    '''\n",
    "    boolean_weight_description={}\n",
    "    count=0\n",
    "    for layer in weight_description.keys():  \n",
    "#         print(\"Count = \",count)\n",
    "        threshold_this_layer=thresholds_per_layer[count]\n",
    "#         print(\"Threshold for layer \",count,layer,\"is \",threshold_this_layer)\n",
    "        if layer not in boolean_weight_description:\n",
    "            boolean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in boolean_weight_description[layer]:\n",
    "                boolean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in boolean_weight_description[layer][index_component]:\n",
    "                    boolean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt]\n",
    "                all_wts_boolean=[]\n",
    "                for wt in all_wts:\n",
    "                    if abs(wt)>threshold_this_layer:\n",
    "                        all_wts_boolean.append(1)\n",
    "                    else:\n",
    "                        all_wts_boolean.append(0)\n",
    "                boolean_weight_description[layer][index_component][index_wt]=all_wts_boolean                    \n",
    "        count+=1\n",
    "        \n",
    "    return boolean_weight_description\n",
    "    \n",
    "# create mask from boolean weight dictionary\n",
    "def create_mask_from_boolean_wt(model,boolean_wt_dict):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.var(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "#             threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten() \n",
    "                mask_vals=[]\n",
    "                for val_index in range(re_shaped_values.shape[0]):\n",
    "                    boolean_vals=boolean_wt_dict[nm][index_component][val_index]\n",
    "                    m = stats.mode(boolean_vals)\n",
    "#                     print(\"Verdict for this weight is \",m[0][0])\n",
    "                    mask_vals.append(m[0][0])\n",
    "#                 mask_vals = (torch.abs(re_shaped_values)>threshold).float()                \n",
    "                mask_vals=np.asarray(mask_vals)\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=torch.from_numpy(mask_vals)\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "         \n",
    "\n",
    "    \n",
    "# this part copied from shrinkbench\n",
    "\n",
    "def nonzero(tensor):\n",
    "    \"\"\"Returns absolute number of values different from 0\n",
    "\n",
    "    Arguments:\n",
    "        tensor {numpy.ndarray} -- Array to compute over\n",
    "\n",
    "    Returns:\n",
    "        int -- Number of nonzero elements\n",
    "    \"\"\"\n",
    "    return np.sum(tensor != 0.0)\n",
    "\n",
    "\n",
    "def model_size(model, as_bits=False):\n",
    "    \"\"\"Returns absolute and nonzero model size\n",
    "\n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Network to compute model size over\n",
    "\n",
    "    Keyword Arguments:\n",
    "        as_bits {bool} -- Whether to account for the size of dtype\n",
    "\n",
    "    Returns:\n",
    "        int -- Total number of weight & bias params\n",
    "        int -- Out total_params exactly how many are nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for tensor in model.parameters():\n",
    "        t = np.prod(tensor.shape)\n",
    "        nz = nonzero(tensor.detach().cpu().numpy())\n",
    "        if as_bits:\n",
    "            bits = dtype2bits[tensor.dtype]\n",
    "            t *= bits\n",
    "            nz *= bits\n",
    "        total_params += t\n",
    "        nonzero_params += nz\n",
    "    return int(total_params), int(nonzero_params)\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))        \n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes how many correct outputs with respect to targets\n",
    "\n",
    "    Does NOT compute accuracy but just a raw amount of correct\n",
    "    outputs given target labels. This is done for each value in\n",
    "    topk. A value is considered correct if target is in the topk\n",
    "    highest values of output.\n",
    "    The values returned are upperbounded by the given batch size\n",
    "\n",
    "    [description]\n",
    "\n",
    "    Arguments:\n",
    "        output {torch.Tensor} -- Output prediction of the model\n",
    "        target {torch.Tensor} -- Target labels from data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "    Returns:\n",
    "        List(int) -- Number of correct values for each topk\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        # Only need to do topk for highest k, reuse for the rest\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(torch.tensor(correct_k.item()))\n",
    "        return res\n",
    "\n",
    "\n",
    "# below copied from shrinkbench, to be used later\n",
    "# def accuracy(model, dataloader, topk=(1,)):\n",
    "#     \"\"\"Compute accuracy of a model over a dataloader for various topk\n",
    "\n",
    "#     Arguments:\n",
    "#         model {torch.nn.Module} -- Network to evaluate\n",
    "#         dataloader {torch.utils.data.DataLoader} -- Data to iterate over\n",
    "\n",
    "#     Keyword Arguments:\n",
    "#         topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "#     Returns:\n",
    "#         List(float) -- List of accuracies for each topk\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Use same device as model\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     accs = np.zeros(len(topk))\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for i, (input, target) in enumerate(dataloader):\n",
    "#             input = input.to(device)\n",
    "#             target = target.to(device)\n",
    "#             output = model(input)\n",
    "\n",
    "#             accs += np.array(correct(output, target, topk))\n",
    "\n",
    "#     # Normalize over data length\n",
    "#     accs /= len(dataloader.dataset)\n",
    "\n",
    "#     return accs\n",
    "\n",
    "\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc3.is_classifier = True\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        top_1, top_5 = correct(out, labels,topk=(1,5))\n",
    "#         print(\"Batch is \",batch[1].shape)\n",
    "        \n",
    "        top_1=top_1/batch[1].shape[0]\n",
    "        top_5=top_5/batch[1].shape[0]\n",
    "\n",
    "#         print(\"corr\",top_1,top_5)\n",
    "#         return {'val_loss': loss, 'val_acc': acc}\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'top_1': top_1, 'top_5': top_5}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        \n",
    "        batch_top_1s = [x['top_1'] for x in outputs]\n",
    "#         print(batch_top_1s)\n",
    "        epoch_top_1 = torch.stack(batch_top_1s).mean()      # Combine top_1\n",
    "        \n",
    "        batch_top_5s = [x['top_5'] for x in outputs]\n",
    "        epoch_top_5 = torch.stack(batch_top_5s).mean()      # Combine top_5\n",
    "        \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),\n",
    "               'val_top_1': epoch_top_1.item(), 'val_top_5': epoch_top_5.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_top_1: {:.4f}, val_top_5: {:.4f}\".format(\n",
    "                                epoch, result['val_loss'], result['val_acc'], \n",
    "                                result['val_top_1'], result['val_top_5']))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     print(\"outputs are \",outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD,\n",
    "        weight_description=None,mask_whole_model=None):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    print(\"At train\")\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if mask_whole_model:\n",
    "#                 print(\"Applying mask\")\n",
    "                apply_mask_model(model,mask_whole_model)\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        print(\"wt desc = \",weight_description)\n",
    "        if weight_description!=None:\n",
    "            print(\"going for weight\")\n",
    "            weight_description=store_weights_in_dic(weight_description,model)\n",
    "    return history, weight_description\n",
    "\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Song Han cifar10\")\n",
    "\n",
    "print(\"Torch cuda \",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(\"device \",device)\n",
    "\n",
    "# data_transforms=torchvision.transforms.Compose(\n",
    "#     [torchvision.transforms.Resize((28,28)),torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "data_transforms=torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "dataset = CIFAR10(root='data/', download=True, transform=data_transforms)\n",
    "\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = CIFAR10(root='data/', train=False,transform=data_transforms)\n",
    "\n",
    "\n",
    "\n",
    "val_size = 10000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "# test_loader=val_loader\n",
    "\n",
    "shape=dataset[0][0].shape\n",
    "input_size=1\n",
    "for s in shape:\n",
    "    input_size*=s\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "targets=train_ds.dataset.targets\n",
    "training_data=torch.tensor(train_ds.dataset.data)\n",
    "\n",
    "training_data = training_data.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "model=CIFAR10CNN()\n",
    "if torch.cuda.is_available():\n",
    "    model=model.cuda()\n",
    "    \n",
    "history = [evaluate(model, val_loader)]\n",
    "print(\"initial result\",history)\n",
    "lr=0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics={}\n",
    "metrics[\"prune_rate\"]=[]\n",
    "metrics[\"compression\"]=[]\n",
    "metrics[\"epochs\"]=[]\n",
    "metrics[\"top_5\"]=[]\n",
    "metrics[\"top_1\"]=[]\n",
    "\n",
    "df_base=pd.read_csv(\"results_sheet/song_hn.csv\")\n",
    "done_pruning_list=list(df_base[\"prune_rate\"])\n",
    "\n",
    "\n",
    "# prune_rate_range=[0.1,0.3,0.4,0.6,0.8,1.1,1.3,1.5,1.7,\n",
    "#                   1.8,2.1,2.4,2.9,3.1,3.15,3.12,3.22,3.29,3.35,3.4,3.45,3.5,3.55,\n",
    "#                   3.6,3.65,3.7,3.75,3.8,3.85,3.9,3.95,4,4.25,4.5,5,6]\n",
    "\n",
    "prune_rate_range=[2.2,2.4,2.6,2.8,3.5,4,4.5,5]\n",
    "\n",
    "for prune_rate in prune_rate_range:\n",
    "    print(\"Prune rate\",prune_rate)\n",
    "    if prune_rate in done_pruning_list:\n",
    "        print(\"Already done\")\n",
    "        continue\n",
    "\n",
    "    model_state_path=\"model_state/mod_CNN.pt\"\n",
    "    \n",
    "     \n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(model_state_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_state_path,map_location=torch.device('cpu')))\n",
    "\n",
    "    result = evaluate(model, test_loader)\n",
    "    print(\"Test result is \",result)\n",
    "\n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"Original Compression=\",compression)\n",
    "\n",
    "    \n",
    "    mask_whole_model=prune_model_get_mask(model,prune_rate)\n",
    "\n",
    "    print(\"Mask compression = \",prune_rate,get_mask_compression(mask_whole_model))\n",
    "\n",
    "\n",
    "    apply_mask_model(model,mask_whole_model)\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"After pruning,with\",prune_rate,\" Compression=\",compression,\"\\nResult after pruning is \",res)\n",
    "\n",
    "\n",
    "    epochs=30\n",
    "    history_prune,_ = fit(epochs, lr, model, train_loader, val_loader,\n",
    "                          mask_whole_model=mask_whole_model)\n",
    "\n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"Compression=\",compression,\"Result after pruning is \",res)\n",
    "    metrics[\"prune_rate\"].append(prune_rate)\n",
    "    metrics[\"compression\"].append(compression)\n",
    "    metrics[\"epochs\"].append(epochs)\n",
    "    metrics[\"top_5\"].append(res['val_top_5'])\n",
    "    metrics[\"top_1\"].append(res['val_top_1'])\n",
    "    print(\"*\"*20)\n",
    "\n",
    "dataframe_results=pd.DataFrame(metrics)\n",
    "dataframe_results=pd.concat([df_base, dataframe_results], ignore_index=True)\n",
    "\n",
    "print(dataframe_results.head())\n",
    "dataframe_results.to_csv(\"results_sheet/song_hn.csv\",\n",
    "                         index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
