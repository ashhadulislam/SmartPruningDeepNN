{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program To apply mean wt on CIFAR10\n",
      "Torch cuda  False\n",
      "device  cpu\n",
      "Files already downloaded and verified\n",
      "initial result [{'val_loss': 2.3033668994903564, 'val_acc': 0.09980468451976776, 'val_top_1': 0.09980468451976776, 'val_top_5': 0.4844726622104645}]\n",
      "Test result is  {'val_loss': 1.2410106658935547, 'val_acc': 0.585742175579071, 'val_top_1': 0.585742175579071, 'val_top_5': 0.9462890625}\n",
      "Compression= 0.0\n",
      "Last few =  95\n",
      "Prune rate is  1.2\n",
      "Mask compression =  1.2 tensor(0.7013)\n",
      "After masking, Compression= 0.6986581943682869 \n",
      "Result after pruning is  {'val_loss': 2.0348517894744873, 'val_acc': 0.2796874940395355, 'val_top_1': 0.2796874940395355, 'val_top_5': 0.783398449420929}\n",
      "Prune rate is  1.22\n",
      "Mask compression =  1.22 tensor(0.7115)\n",
      "After masking, Compression= 0.7087539915492049 \n",
      "Result after pruning is  {'val_loss': 2.036360263824463, 'val_acc': 0.2738281190395355, 'val_top_1': 0.2738281190395355, 'val_top_5': 0.7833007574081421}\n",
      "Prune rate is  1.24\n",
      "Mask compression =  1.24 tensor(0.7211)\n",
      "After masking, Compression= 0.7183337096410025 \n",
      "Result after pruning is  {'val_loss': 1.9943466186523438, 'val_acc': 0.2906250059604645, 'val_top_1': 0.2906250059604645, 'val_top_5': 0.804003894329071}\n",
      "Prune rate is  1.26\n",
      "Mask compression =  1.26 tensor(0.7307)\n",
      "After masking, Compression= 0.7279134277328001 \n",
      "Result after pruning is  {'val_loss': 1.9950215816497803, 'val_acc': 0.28271484375, 'val_top_1': 0.28271484375, 'val_top_5': 0.802929699420929}\n",
      "Prune rate is  1.28\n",
      "Mask compression =  1.28 tensor(0.7398)\n",
      "After masking, Compression= 0.7370093216785473 \n",
      "Result after pruning is  {'val_loss': 2.0815978050231934, 'val_acc': 0.24531249701976776, 'val_top_1': 0.24531249701976776, 'val_top_5': 0.7759765386581421}\n",
      "Prune rate is  1.3\n",
      "Mask compression =  1.3 tensor(0.7497)\n",
      "After masking, Compression= 0.746814824371835 \n",
      "Result after pruning is  {'val_loss': 2.059932231903076, 'val_acc': 0.24287109076976776, 'val_top_1': 0.24287109076976776, 'val_top_5': 0.774609386920929}\n",
      "At train\n",
      "Epoch [0], val_loss: 1.1982, val_acc: 0.5797, val_top_1: 0.5797, val_top_5: 0.9490\n",
      "Epoch [1], val_loss: 1.1565, val_acc: 0.5952, val_top_1: 0.5952, val_top_5: 0.9485\n",
      "Epoch [2], val_loss: 1.1244, val_acc: 0.6074, val_top_1: 0.6074, val_top_5: 0.9526\n",
      "Epoch [3], val_loss: 1.1358, val_acc: 0.5976, val_top_1: 0.5976, val_top_5: 0.9518\n",
      "Epoch [4], val_loss: 1.1201, val_acc: 0.6071, val_top_1: 0.6071, val_top_5: 0.9535\n",
      "Epoch [5], val_loss: 1.1046, val_acc: 0.6155, val_top_1: 0.6155, val_top_5: 0.9537\n",
      "Epoch [6], val_loss: 1.0891, val_acc: 0.6208, val_top_1: 0.6208, val_top_5: 0.9565\n",
      "Epoch [7], val_loss: 1.0865, val_acc: 0.6172, val_top_1: 0.6172, val_top_5: 0.9556\n",
      "Epoch [8], val_loss: 1.0901, val_acc: 0.6137, val_top_1: 0.6137, val_top_5: 0.9559\n",
      "Epoch [9], val_loss: 1.1274, val_acc: 0.6106, val_top_1: 0.6106, val_top_5: 0.9555\n",
      "Epoch [10], val_loss: 1.1111, val_acc: 0.6149, val_top_1: 0.6149, val_top_5: 0.9554\n",
      "Epoch [11], val_loss: 1.0739, val_acc: 0.6263, val_top_1: 0.6263, val_top_5: 0.9559\n",
      "Epoch [12], val_loss: 1.0893, val_acc: 0.6243, val_top_1: 0.6243, val_top_5: 0.9576\n",
      "Epoch [13], val_loss: 1.0890, val_acc: 0.6216, val_top_1: 0.6216, val_top_5: 0.9549\n",
      "Epoch [14], val_loss: 1.0857, val_acc: 0.6228, val_top_1: 0.6228, val_top_5: 0.9562\n",
      "Epoch [15], val_loss: 1.0863, val_acc: 0.6216, val_top_1: 0.6216, val_top_5: 0.9563\n",
      "Epoch [16], val_loss: 1.0971, val_acc: 0.6162, val_top_1: 0.6162, val_top_5: 0.9554\n",
      "Epoch [17], val_loss: 1.0870, val_acc: 0.6294, val_top_1: 0.6294, val_top_5: 0.9568\n",
      "Epoch [18], val_loss: 1.0873, val_acc: 0.6238, val_top_1: 0.6238, val_top_5: 0.9569\n",
      "Epoch [19], val_loss: 1.0835, val_acc: 0.6268, val_top_1: 0.6268, val_top_5: 0.9582\n",
      "Epoch [20], val_loss: 1.0876, val_acc: 0.6249, val_top_1: 0.6249, val_top_5: 0.9574\n",
      "Epoch [21], val_loss: 1.1178, val_acc: 0.6110, val_top_1: 0.6110, val_top_5: 0.9564\n",
      "Epoch [22], val_loss: 1.0791, val_acc: 0.6265, val_top_1: 0.6265, val_top_5: 0.9570\n",
      "Epoch [23], val_loss: 1.0871, val_acc: 0.6234, val_top_1: 0.6234, val_top_5: 0.9570\n",
      "Epoch [24], val_loss: 1.0805, val_acc: 0.6294, val_top_1: 0.6294, val_top_5: 0.9579\n",
      "Epoch [25], val_loss: 1.0759, val_acc: 0.6279, val_top_1: 0.6279, val_top_5: 0.9586\n",
      "Epoch [26], val_loss: 1.0835, val_acc: 0.6289, val_top_1: 0.6289, val_top_5: 0.9582\n",
      "Epoch [27], val_loss: 1.1117, val_acc: 0.6157, val_top_1: 0.6157, val_top_5: 0.9581\n",
      "Epoch [28], val_loss: 1.0894, val_acc: 0.6263, val_top_1: 0.6263, val_top_5: 0.9574\n",
      "Epoch [29], val_loss: 1.0913, val_acc: 0.6262, val_top_1: 0.6262, val_top_5: 0.9585\n",
      "Pruning+Re training the weighted threshold way, complete\n",
      "Retrain+Pruning, Compression= 0.746814824371835 \n",
      "Result after pruning is  {'val_loss': 1.1961549520492554, 'val_acc': 0.5918945074081421, 'val_top_1': 0.5918945074081421, 'val_top_5': 0.9496093988418579}\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from scipy import stats\n",
    "    \n",
    "\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def countDiffMasks(mask1,mask2):\n",
    "    total_diff=0\n",
    "    for i in range(len(mask1)):\n",
    "        m_1=mask1[i].flatten()\n",
    "        m_2=mask2[i].flatten()\n",
    "        count_same=(m_1 == m_2).sum()\n",
    "        count_different=m_1.flatten().shape[0]-count_same\n",
    "        total_diff+=count_different\n",
    "    return total_diff\n",
    "\n",
    "\n",
    "def get_mask_compression(mask_whole_model):\n",
    "    num_total=0\n",
    "    num_non_zeros=0\n",
    "    for mask_each_layer in mask_whole_model:\n",
    "        num_total+=torch.numel(mask_each_layer)\n",
    "        num_non_zeros+=torch.count_nonzero(mask_each_layer)\n",
    "        \n",
    "    return (num_total-num_non_zeros)/num_total\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def prune_model_get_mask(model,prune_rate):\n",
    "    '''\n",
    "    works purely on the model to get\n",
    "    mask\n",
    "    '''\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.std(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "            threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    " \n",
    "    \n",
    "def get_thresholds_each_layer(model,prune_rate):\n",
    "    thresholds_per_layer=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "            abs_std=torch.std(torch.abs(params.data))\n",
    "            threshold=abs_std*prune_rate\n",
    "            thresholds_per_layer.append(threshold)\n",
    "    return thresholds_per_layer\n",
    "    \n",
    "                \n",
    "def apply_mask_model(model,list_mask_whole_model):\n",
    "    mask_layer_count=0\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=list_mask_whole_model[mask_layer_count]\n",
    "            with torch.no_grad():\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#                 print(\"Devices are \",params.device,mask_layer.device)\n",
    "                mask_layer=mask_layer.to(device)\n",
    "    \n",
    "                params.data=params.data*mask_layer            \n",
    "            mask_layer_count+=1\n",
    "    \n",
    "\n",
    "def store_weights_in_dic(weight_description,model):\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            if nm not in weight_description:\n",
    "                weight_description[nm]={}\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                if index_component not in weight_description[nm]:\n",
    "                    weight_description[nm][index_component]={}\n",
    "                values=params[index_component]\n",
    "                flat_values=values.flatten()\n",
    "                for index_wt in range(flat_values.shape[0]):\n",
    "                    if index_wt not in weight_description[nm][index_component]:\n",
    "                        weight_description[nm][index_component][index_wt]=[]\n",
    "                    weight_description[nm][index_component][index_wt].append(flat_values[index_wt].detach().item())\n",
    "    return weight_description\n",
    "\n",
    "\n",
    "def get_boolean_dict_weight_dict(weight_description,prune_rate,thresholds_per_layer,last_few):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of 1s and 0s to show\n",
    "    how many times weight is more than threshold\n",
    "    per layer\n",
    "    '''\n",
    "    boolean_weight_description={}\n",
    "    count=0\n",
    "    for layer in weight_description.keys():  \n",
    "#         print(\"Count = \",count)\n",
    "        threshold_this_layer=thresholds_per_layer[count]\n",
    "#         print(\"Threshold for layer \",count,layer,\"is \",threshold_this_layer)\n",
    "        if layer not in boolean_weight_description:\n",
    "            boolean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in boolean_weight_description[layer]:\n",
    "                boolean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in boolean_weight_description[layer][index_component]:\n",
    "                    boolean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt][:-last_few]\n",
    "                all_wts_boolean=[]\n",
    "                for wt in all_wts:\n",
    "                    if abs(wt)>threshold_this_layer:\n",
    "                        all_wts_boolean.append(1)\n",
    "                    else:\n",
    "                        all_wts_boolean.append(0)\n",
    "                boolean_weight_description[layer][index_component][index_wt]=all_wts_boolean                    \n",
    "        count+=1\n",
    "        \n",
    "    return boolean_weight_description\n",
    "\n",
    "\n",
    "\n",
    "def get_mean_dict_weight_dict(weight_description,last_few):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of mean of weights simply\n",
    "    for the last few epochs\n",
    "    '''\n",
    "    mean_weight_description={}\n",
    "\n",
    "    for layer in weight_description.keys():  \n",
    "        if layer not in mean_weight_description:\n",
    "            mean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in mean_weight_description[layer]:\n",
    "                mean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in mean_weight_description[layer][index_component]:\n",
    "                    mean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt][-last_few:]                \n",
    "                all_wts_mean=np.mean(all_wts)                \n",
    "                mean_weight_description[layer][index_component][index_wt]=all_wts_mean                    \n",
    "        \n",
    "    return mean_weight_description\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# create mask from boolean weight dictionary\n",
    "def create_mask_from_boolean_wt(model,boolean_wt_dict):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)\n",
    "#             print(nm,params.shape)\n",
    "            abs_var=torch.var(torch.abs(params.data))\n",
    "#             print(abs_var)\n",
    "#             print(params)\n",
    "#             threshold=abs_var*prune_rate\n",
    "            num_components=params.shape[0]\n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten() \n",
    "                mask_vals=[]\n",
    "                for val_index in range(re_shaped_values.shape[0]):\n",
    "                    boolean_vals=boolean_wt_dict[nm][index_component][val_index]\n",
    "                    m = stats.mode(boolean_vals)\n",
    "#                     print(\"Verdict for this weight is \",m[0][0])\n",
    "                    mask_vals.append(m[0][0])\n",
    "#                 mask_vals = (torch.abs(re_shaped_values)>threshold).float()                \n",
    "                mask_vals=np.asarray(mask_vals)\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=torch.from_numpy(mask_vals)\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "\n",
    "\n",
    "         \n",
    "def create_mask_from_mean_wt(model,mean_weight_description,prune_rate):\n",
    "    mask_whole_model=[]\n",
    "    for nm, params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            mask_layer=torch.ones(params.shape)    \n",
    "            # get all mean weights for this layer\n",
    "            mean_wt_layer=mean_weight_description[nm]\n",
    "            wts_this_layer=[]\n",
    "            for neuron_index in list(mean_wt_layer.keys()):\n",
    "                all_wts_this_neu=mean_wt_layer[neuron_index]\n",
    "\n",
    "                for weight_index in list(mean_wt_layer[neuron_index].keys()):\n",
    "                    wts_this_layer.append(mean_wt_layer[neuron_index][weight_index])\n",
    "            # end of all neurons\n",
    "#             print(\"first 5 Mean weights this layer are \",wts_this_layer[:5])\n",
    "            abs_var=torch.std(torch.FloatTensor(wts_this_layer))\n",
    "#             print(\"standard dev is \",abs_var)\n",
    "            threshold=abs_var*prune_rate\n",
    "#             print(\"Threshold is \",threshold)\n",
    "            num_components=params.shape[0]            \n",
    "            for index_component in range(num_components):\n",
    "                values=params[index_component]            \n",
    "                re_shaped_values=values.flatten()                \n",
    "                mask_vals = (torch.abs(re_shaped_values)>threshold).float()\n",
    "                mask_vals=mask_vals.reshape(values.shape)\n",
    "#                 print(mask_vals.shape)\n",
    "                mask_layer[index_component]=mask_vals\n",
    "            mask_whole_model.append(mask_layer)\n",
    "    return mask_whole_model\n",
    "            \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "def get_weighted_mean_dict_weight_dict(weight_description,last_few):\n",
    "    '''\n",
    "    works on the dictionary of weights\n",
    "    to create a dict of mean of weights     \n",
    "    for the last few epochs\n",
    "    and gives a weightage to each weight value\n",
    "    depending on the epoch\n",
    "    last epoch highest weight\n",
    "    '''\n",
    "    weighted_mean_weight_description={}\n",
    "\n",
    "    for layer in weight_description.keys():  \n",
    "        if layer not in weighted_mean_weight_description:\n",
    "            weighted_mean_weight_description[layer]={}\n",
    "        for index_component in weight_description[layer].keys():\n",
    "            if index_component not in weighted_mean_weight_description[layer]:\n",
    "                weighted_mean_weight_description[layer][index_component]={}\n",
    "            for index_wt in weight_description[layer][index_component].keys():\n",
    "                if index_wt not in weighted_mean_weight_description[layer][index_component]:\n",
    "                    weighted_mean_weight_description[layer][index_component][index_wt]=[]\n",
    "                all_wts=weight_description[layer][index_component][index_wt][-last_few:]\n",
    "                i_weights=[math.sqrt(i) for i in range(1,last_few+1)]\n",
    "#                 print(all_wts,i_weights)\n",
    "                all_wts_mean=np.average(all_wts,weights=i_weights)                \n",
    "                weighted_mean_weight_description[layer][index_component][index_wt]=all_wts_mean                    \n",
    "        \n",
    "    return weighted_mean_weight_description    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this part copied from shrinkbench\n",
    "\n",
    "def nonzero(tensor):\n",
    "    \"\"\"Returns absolute number of values different from 0\n",
    "\n",
    "    Arguments:\n",
    "        tensor {numpy.ndarray} -- Array to compute over\n",
    "\n",
    "    Returns:\n",
    "        int -- Number of nonzero elements\n",
    "    \"\"\"\n",
    "    return np.sum(tensor != 0.0)\n",
    "\n",
    "\n",
    "def model_size(model, as_bits=False):\n",
    "    \"\"\"Returns absolute and nonzero model size\n",
    "\n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Network to compute model size over\n",
    "\n",
    "    Keyword Arguments:\n",
    "        as_bits {bool} -- Whether to account for the size of dtype\n",
    "\n",
    "    Returns:\n",
    "        int -- Total number of weight & bias params\n",
    "        int -- Out total_params exactly how many are nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for tensor in model.parameters():\n",
    "        t = np.prod(tensor.shape)\n",
    "        nz = nonzero(tensor.detach().cpu().numpy())\n",
    "        if as_bits:\n",
    "            bits = dtype2bits[tensor.dtype]\n",
    "            t *= bits\n",
    "            nz *= bits\n",
    "        total_params += t\n",
    "        nonzero_params += nz\n",
    "    return int(total_params), int(nonzero_params)\n",
    "\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))        \n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes how many correct outputs with respect to targets\n",
    "\n",
    "    Does NOT compute accuracy but just a raw amount of correct\n",
    "    outputs given target labels. This is done for each value in\n",
    "    topk. A value is considered correct if target is in the topk\n",
    "    highest values of output.\n",
    "    The values returned are upperbounded by the given batch size\n",
    "\n",
    "    [description]\n",
    "\n",
    "    Arguments:\n",
    "        output {torch.Tensor} -- Output prediction of the model\n",
    "        target {torch.Tensor} -- Target labels from data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "    Returns:\n",
    "        List(int) -- Number of correct values for each topk\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        # Only need to do topk for highest k, reuse for the rest\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(torch.tensor(correct_k.item()))\n",
    "        return res\n",
    "\n",
    "\n",
    "# below copied from shrinkbench, to be used later\n",
    "# def accuracy(model, dataloader, topk=(1,)):\n",
    "#     \"\"\"Compute accuracy of a model over a dataloader for various topk\n",
    "\n",
    "#     Arguments:\n",
    "#         model {torch.nn.Module} -- Network to evaluate\n",
    "#         dataloader {torch.utils.data.DataLoader} -- Data to iterate over\n",
    "\n",
    "#     Keyword Arguments:\n",
    "#         topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "#     Returns:\n",
    "#         List(float) -- List of accuracies for each topk\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Use same device as model\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     accs = np.zeros(len(topk))\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for i, (input, target) in enumerate(dataloader):\n",
    "#             input = input.to(device)\n",
    "#             target = target.to(device)\n",
    "#             output = model(input)\n",
    "\n",
    "#             accs += np.array(correct(output, target, topk))\n",
    "\n",
    "#     # Normalize over data length\n",
    "#     accs /= len(dataloader.dataset)\n",
    "\n",
    "#     return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     print(\"outputs are \",outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD,\n",
    "        weight_description=None,mask_whole_model=None,\n",
    "       model_state_path=None):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    print(\"At train\")\n",
    "    history = []\n",
    "    best_so_far=-999        \n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if mask_whole_model:\n",
    "#                 print(\"Applying mask\")\n",
    "                apply_mask_model(model,mask_whole_model)\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        if best_so_far<result[\"val_top_1\"]:\n",
    "            best_so_far=result[\"val_top_1\"]\n",
    "            if model_state_path:\n",
    "                torch.save(model.state_dict(), model_state_path)\n",
    "        \n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "#         print(\"wt desc = \",weight_description)\n",
    "        if weight_description!=None:\n",
    "            print(\"going for weight\")\n",
    "            weight_description=store_weights_in_dic(weight_description,model)\n",
    "    return history, weight_description\n",
    "\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc3.is_classifier = True\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        top_1, top_5 = correct(out, labels,topk=(1,5))\n",
    "#         print(\"Batch is \",batch[1].shape)\n",
    "        \n",
    "        top_1=top_1/batch[1].shape[0]\n",
    "        top_5=top_5/batch[1].shape[0]\n",
    "\n",
    "#         print(\"corr\",top_1,top_5)\n",
    "#         return {'val_loss': loss, 'val_acc': acc}\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'top_1': top_1, 'top_5': top_5}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        \n",
    "        batch_top_1s = [x['top_1'] for x in outputs]\n",
    "#         print(batch_top_1s)\n",
    "        epoch_top_1 = torch.stack(batch_top_1s).mean()      # Combine top_1\n",
    "        \n",
    "        batch_top_5s = [x['top_5'] for x in outputs]\n",
    "        epoch_top_5 = torch.stack(batch_top_5s).mean()      # Combine top_5\n",
    "        \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),\n",
    "               'val_top_1': epoch_top_1.item(), 'val_top_5': epoch_top_5.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_top_1: {:.4f}, val_top_5: {:.4f}\".format(\n",
    "                                epoch, result['val_loss'], result['val_acc'], \n",
    "                                result['val_top_1'], result['val_top_5']))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(\"Program To apply mean wt on CIFAR10\")\n",
    "\n",
    "print(\"Torch cuda \",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(\"device \",device)\n",
    "\n",
    "# data_transforms=torchvision.transforms.Compose(\n",
    "#     [torchvision.transforms.Resize((28,28)),torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "data_transforms=torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "dataset = CIFAR10(root='data/', download=True, transform=data_transforms)\n",
    "\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = CIFAR10(root='data/', train=False,transform=data_transforms)\n",
    "\n",
    "\n",
    "\n",
    "val_size = 10000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "# test_loader=val_loader\n",
    "\n",
    "shape=dataset[0][0].shape\n",
    "input_size=1\n",
    "for s in shape:\n",
    "    input_size*=s\n",
    "    \n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "targets=train_ds.dataset.targets\n",
    "training_data=torch.tensor(train_ds.dataset.data)\n",
    "\n",
    "training_data = training_data.to(device=device)\n",
    "\n",
    "\n",
    "model=CIFAR10CNN()\n",
    "# model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model=model.cuda()\n",
    "history = [evaluate(model, val_loader)]\n",
    "print(\"initial result\",history)\n",
    "weight_description={}\n",
    "\n",
    "lr=0.01\n",
    "\n",
    "\n",
    "model_state_path=\"model_state/mod_CNN_05_1_100.pt\"\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(model_state_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_state_path,map_location=torch.device('cpu')))\n",
    "# model=model.to_device()\n",
    "\n",
    "weight_description= pickle.load( open( \"pickles/weight_description_CNN_05_1_100.p\", \"rb\" ) )\n",
    "\n",
    "result = evaluate(model, test_loader)\n",
    "print(\"Test result is \",result)\n",
    "\n",
    "\n",
    "total_size,nz_size=model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "print(\"Compression=\",compression)\n",
    "\n",
    "\n",
    "\n",
    "last_few=95\n",
    "mean_weight_description_weighted=get_weighted_mean_dict_weight_dict(weight_description,last_few)\n",
    "print(\"Last few = \",last_few)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prune_rate_range=[1.2,1.22,1.24,1.26,1.28,1.3,1.32,1.34,1.36,1.38]\n",
    "\n",
    "target_compression=0.746\n",
    "\n",
    "\n",
    "for prune_rate in prune_rate_range:\n",
    "    print(\"Prune rate is \",prune_rate)\n",
    "\n",
    "    \n",
    "    model_state_path=\"model_state/mod_CNN_05_1_100.pt\"\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(model_state_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_state_path,map_location=torch.device('cpu')))\n",
    "\n",
    "    mask_whole_model=create_mask_from_mean_wt(model,mean_weight_description_weighted,prune_rate)\n",
    "    print(\"Mask compression = \",prune_rate,get_mask_compression(mask_whole_model))\n",
    "\n",
    "    \n",
    "    apply_mask_model(model,mask_whole_model)\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "    print(\"After masking, Compression=\",compression,\"\\nResult after pruning is \",res)\n",
    "    if compression<(target_compression-0.005) or compression>(target_compression+0.005):\n",
    "        continue\n",
    "\n",
    "\n",
    "    epochs=30\n",
    "    pruned_model_state_path=\"model_state/threshold_mean_wts.pt\"\n",
    "    history_prune,_ = fit(epochs, lr, model, train_loader, val_loader,\n",
    "                          mask_whole_model=mask_whole_model,\n",
    "                         model_state_path=pruned_model_state_path)\n",
    "\n",
    "    print(\"Pruning+Re training the weighted threshold way, complete\")\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(pruned_model_state_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(pruned_model_state_path,map_location=torch.device('cpu')))\n",
    "        \n",
    "\n",
    "    total_size,nz_size=model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    res = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"Retrain+Pruning, Compression=\",compression,\"\\nResult after pruning is \",res)    \n",
    "    \n",
    "\n",
    "    print(\"*\"*30)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746814824371835 {'val_loss': 1.1961549520492554, 'val_acc': 0.5918945074081421, 'val_top_1': 0.5918945074081421, 'val_top_5': 0.9496093988418579}\n"
     ]
    }
   ],
   "source": [
    "total_size,nz_size=model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "res = evaluate(model, test_loader)\n",
    "print(compression,res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gaussian noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdevs=np.linspace(0,1,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'val_loss': 1.1961549520492554, 'val_acc': 0.5918945074081421, 'val_top_1': 0.5918945074081421, 'val_top_5': 0.9496093988418579}\n",
      "0.034482758620689655 {'val_loss': 1.1983468532562256, 'val_acc': 0.590039074420929, 'val_top_1': 0.590039074420929, 'val_top_5': 0.950488269329071}\n",
      "0.06896551724137931 {'val_loss': 1.2098954916000366, 'val_acc': 0.5879882574081421, 'val_top_1': 0.5879882574081421, 'val_top_5': 0.949999988079071}\n",
      "0.10344827586206896 {'val_loss': 1.2493252754211426, 'val_acc': 0.5693359375, 'val_top_1': 0.5693359375, 'val_top_5': 0.945507824420929}\n",
      "0.13793103448275862 {'val_loss': 1.320732831954956, 'val_acc': 0.5459960699081421, 'val_top_1': 0.5459960699081421, 'val_top_5': 0.939453125}\n",
      "0.1724137931034483 {'val_loss': 1.4372519254684448, 'val_acc': 0.521289050579071, 'val_top_1': 0.521289050579071, 'val_top_5': 0.9332031011581421}\n",
      "0.20689655172413793 {'val_loss': 1.5642175674438477, 'val_acc': 0.49042969942092896, 'val_top_1': 0.49042969942092896, 'val_top_5': 0.922070324420929}\n",
      "0.24137931034482757 {'val_loss': 1.7421905994415283, 'val_acc': 0.4541992247104645, 'val_top_1': 0.4541992247104645, 'val_top_5': 0.9110351800918579}\n",
      "0.27586206896551724 {'val_loss': 1.9589134454727173, 'val_acc': 0.4305664002895355, 'val_top_1': 0.4305664002895355, 'val_top_5': 0.8915039300918579}\n",
      "0.3103448275862069 {'val_loss': 2.234466075897217, 'val_acc': 0.3946289122104645, 'val_top_1': 0.3946289122104645, 'val_top_5': 0.8780273199081421}\n",
      "0.3448275862068966 {'val_loss': 2.497805595397949, 'val_acc': 0.36699217557907104, 'val_top_1': 0.36699217557907104, 'val_top_5': 0.8570312261581421}\n",
      "0.3793103448275862 {'val_loss': 2.8136048316955566, 'val_acc': 0.34794920682907104, 'val_top_1': 0.34794920682907104, 'val_top_5': 0.839062511920929}\n",
      "0.41379310344827586 {'val_loss': 3.104313373565674, 'val_acc': 0.3311523497104645, 'val_top_1': 0.3311523497104645, 'val_top_5': 0.820996105670929}\n",
      "0.4482758620689655 {'val_loss': 3.458219051361084, 'val_acc': 0.3104492127895355, 'val_top_1': 0.3104492127895355, 'val_top_5': 0.8011718988418579}\n",
      "0.48275862068965514 {'val_loss': 3.7882869243621826, 'val_acc': 0.2894531190395355, 'val_top_1': 0.2894531190395355, 'val_top_5': 0.7796875238418579}\n",
      "0.5172413793103449 {'val_loss': 4.1780219078063965, 'val_acc': 0.2734375, 'val_top_1': 0.2734375, 'val_top_5': 0.7582031488418579}\n",
      "0.5517241379310345 {'val_loss': 4.489023208618164, 'val_acc': 0.26513671875, 'val_top_1': 0.26513671875, 'val_top_5': 0.748339831829071}\n",
      "0.5862068965517241 {'val_loss': 4.857316493988037, 'val_acc': 0.2557617127895355, 'val_top_1': 0.2557617127895355, 'val_top_5': 0.737011730670929}\n",
      "0.6206896551724138 {'val_loss': 5.180564880371094, 'val_acc': 0.24736328423023224, 'val_top_1': 0.24736328423023224, 'val_top_5': 0.713671863079071}\n",
      "0.6551724137931034 {'val_loss': 5.558099746704102, 'val_acc': 0.23857422173023224, 'val_top_1': 0.23857422173023224, 'val_top_5': 0.7017577886581421}\n",
      "0.6896551724137931 {'val_loss': 5.895544528961182, 'val_acc': 0.22587890923023224, 'val_top_1': 0.22587890923023224, 'val_top_5': 0.692089855670929}\n",
      "0.7241379310344828 {'val_loss': 6.299381732940674, 'val_acc': 0.2197265625, 'val_top_1': 0.2197265625, 'val_top_5': 0.67236328125}\n",
      "0.7586206896551724 {'val_loss': 6.6225786209106445, 'val_acc': 0.21308593451976776, 'val_top_1': 0.21308593451976776, 'val_top_5': 0.66796875}\n",
      "0.7931034482758621 {'val_loss': 7.0481133460998535, 'val_acc': 0.20322266221046448, 'val_top_1': 0.20322266221046448, 'val_top_5': 0.6571289300918579}\n",
      "0.8275862068965517 {'val_loss': 7.388627052307129, 'val_acc': 0.20556640625, 'val_top_1': 0.20556640625, 'val_top_5': 0.640820324420929}\n",
      "0.8620689655172413 {'val_loss': 7.783586025238037, 'val_acc': 0.19121094048023224, 'val_top_1': 0.19121094048023224, 'val_top_5': 0.640917956829071}\n",
      "0.896551724137931 {'val_loss': 8.127923965454102, 'val_acc': 0.19023437798023224, 'val_top_1': 0.19023437798023224, 'val_top_5': 0.626953125}\n",
      "0.9310344827586207 {'val_loss': 8.427300453186035, 'val_acc': 0.18671874701976776, 'val_top_1': 0.18671874701976776, 'val_top_5': 0.6224609613418579}\n",
      "0.9655172413793103 {'val_loss': 8.879655838012695, 'val_acc': 0.17929688096046448, 'val_top_1': 0.17929688096046448, 'val_top_5': 0.618359386920929}\n",
      "1.0 {'val_loss': 9.192976951599121, 'val_acc': 0.1796875, 'val_top_1': 0.1796875, 'val_top_5': 0.6109374761581421}\n"
     ]
    }
   ],
   "source": [
    "results={}\n",
    "results[\"stdev\"]=[]\n",
    "results[\"val_1\"]=[]\n",
    "for stdev in stdevs:\n",
    "    transform_noisy=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        AddGaussianNoise(0., stdev)\n",
    "    ])\n",
    "    # Define test dataset\n",
    "    test_dataset = CIFAR10(root='data/', train=False,transform=transform_noisy)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "    test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "    res = evaluate(model, test_loader)\n",
    "    \n",
    "    print(stdev,res)\n",
    "    results[\"stdev\"].append(stdev)\n",
    "    results[\"val_1\"].append(res[\"val_acc\"])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stdev</th>\n",
       "      <th>val_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.590039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.587988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.569336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.545996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.521289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.490430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.454199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.430566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.394629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.366992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.347949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.331152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.310449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.289453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.273438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.265137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.255762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.247363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.238574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.225879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.219727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.213086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.203223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.205566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.191211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.190234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.186719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.179297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.179688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stdev     val_1\n",
       "0   0.000000  0.591895\n",
       "1   0.034483  0.590039\n",
       "2   0.068966  0.587988\n",
       "3   0.103448  0.569336\n",
       "4   0.137931  0.545996\n",
       "5   0.172414  0.521289\n",
       "6   0.206897  0.490430\n",
       "7   0.241379  0.454199\n",
       "8   0.275862  0.430566\n",
       "9   0.310345  0.394629\n",
       "10  0.344828  0.366992\n",
       "11  0.379310  0.347949\n",
       "12  0.413793  0.331152\n",
       "13  0.448276  0.310449\n",
       "14  0.482759  0.289453\n",
       "15  0.517241  0.273438\n",
       "16  0.551724  0.265137\n",
       "17  0.586207  0.255762\n",
       "18  0.620690  0.247363\n",
       "19  0.655172  0.238574\n",
       "20  0.689655  0.225879\n",
       "21  0.724138  0.219727\n",
       "22  0.758621  0.213086\n",
       "23  0.793103  0.203223\n",
       "24  0.827586  0.205566\n",
       "25  0.862069  0.191211\n",
       "26  0.896552  0.190234\n",
       "27  0.931034  0.186719\n",
       "28  0.965517  0.179297\n",
       "29  1.000000  0.179688"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv(\"results_sheet/noisy/02thres_mean.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
